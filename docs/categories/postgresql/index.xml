<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Postgresql on Dimitri Fontaine, Expert PostgreSQL</title>
    <link>http://tapoueh.org/categories/postgresql/</link>
    <description>Recent content in Postgresql on Dimitri Fontaine, Expert PostgreSQL</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 16 Nov 2015 14:18:00 +0200</lastBuildDate>
    
	<atom:link href="http://tapoueh.org/categories/postgresql/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>All Your Base</title>
      <link>http://tapoueh.org/manual-post/2015/11/16-allyourbase/</link>
      <pubDate>Mon, 16 Nov 2015 14:18:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/manual-post/2015/11/16-allyourbase/</guid>
      <description>&lt;p&gt;I had the pleasure to be invited to speak
at
&lt;a href=&#34;http://allyourbaseconf.com/2015/speakers#dimitri-fontaine&#34;&gt;All Your Base Conference 2015&lt;/a&gt; about
&lt;a href=&#34;http://www.postgresql.org&#34;&gt;PostgreSQL&lt;/a&gt; (of course).&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Meetup PostgreSQL à Paris</title>
      <link>http://tapoueh.org/blog/2014/10/meetup-postgresql-%C3%A0-paris/</link>
      <pubDate>Thu, 02 Oct 2014 11:49:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2014/10/meetup-postgresql-%C3%A0-paris/</guid>
      <description>Mercredi 8 octobre se tiendra le prochain Meetup PostgreSQL à Paris dans les locaux de Mozilla Europe, dont la capacité est de 90 personnes ! Venez nombreux !
Le programme de cette édition est un peu particulier puisque nous avons reçu quatre propositions enthousiastes de présentations. Chacun aura donc 15 à 20 minutes (questions incluses) pour présenter son sujet. Voici le programme :
 Aggrégation temporelle sous contrainte d&amp;rsquo;iops, par Jean-Gérard Pailloncy</description>
    </item>
    
    <item>
      <title>PHP Tour, La Video</title>
      <link>http://tapoueh.org/manual-post/2014/09/php-tour/</link>
      <pubDate>Wed, 10 Sep 2014 11:59:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/manual-post/2014/09/php-tour/</guid>
      <description>&lt;p&gt;En juin dernier se tenait
le &lt;a href=&#34;http://afup.org/pages/phptourlyon2014/&#34;&gt;PHP Tour 2014&lt;/a&gt; à Lyon, où j&amp;rsquo;ai
eu le plaisir de présenter une conférence
sur
&lt;a href=&#34;http://tapoueh.org/confs/2014/06/23-PHPTour-Lyon-2014&#34;&gt;PostgreSQL en 2014&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Going to Chicago, Postgres Open</title>
      <link>http://tapoueh.org/manual-post/2014/08/postgres-open/</link>
      <pubDate>Fri, 29 Aug 2014 14:26:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/manual-post/2014/08/postgres-open/</guid>
      <description>&lt;p&gt;Next month, &lt;a href=&#34;https://postgresopen.org/2014/&#34;&gt;Postgres Open 2014&lt;/a&gt; is
happening in Chicago, and I&amp;rsquo;ll have the pleasure to host a tutorial about
PostgreSQL
Extensions
&lt;a href=&#34;https://postgresopen.org/events/schedule/pgopen2014/session/77-4-writing-using-postgres-extensions/index.html/&#34;&gt;Writing &amp;amp; Using Postgres Extensions&lt;/a&gt;,
and a talk aimed at developers wanting to make the best out of
PostgreSQL,
&lt;a href=&#34;https://postgresopen.org/events/schedule/pgopen2014/session/56-postgresql-for-developers/&#34;&gt;PostgreSQL for developers&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>PostgreSQL, Aggregates and Histograms</title>
      <link>http://tapoueh.org/blog/2014/02/postgresql-aggregates-and-histograms/</link>
      <pubDate>Fri, 21 Feb 2014 13:25:00 +0100</pubDate>
      
      <guid>http://tapoueh.org/blog/2014/02/postgresql-aggregates-and-histograms/</guid>
      <description>In our previous article Aggregating NBA data, PostgreSQL vs MongoDB we spent time comparing the pretty new MongoDB Aggregation Framework with the decades old SQL aggregates. Today, let&amp;rsquo;s showcase more of those SQL aggregates, producing a nice histogram right from our SQL console.
PostgreSQL and Mathematics The other day while giving a Practical SQL training my attention drifted to the width_bucket function available as part of the Mathematical Functions and Operators PostgreSQL is offering to its fearless SQL users.</description>
    </item>
    
    <item>
      <title>PostgreSQL, Aggregates and Histograms</title>
      <link>http://tapoueh.org/manual-post/2014/02/postgresql-histogram/</link>
      <pubDate>Fri, 21 Feb 2014 13:25:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/manual-post/2014/02/postgresql-histogram/</guid>
      <description>&lt;p&gt;In our previous
article
&lt;a href=&#34;http://tapoueh.org/blog/2014/02/17-aggregating-nba-data-PostgreSQL-vs-MongoDB&#34;&gt;Aggregating NBA data, PostgreSQL vs MongoDB&lt;/a&gt; we
spent time comparing the pretty new &lt;em&gt;MongoDB Aggregation Framework&lt;/em&gt; with the
decades old SQL aggregates. Today, let&amp;rsquo;s showcase more of those SQL
aggregates, producing a nice &lt;em&gt;histogram&lt;/em&gt; right from our SQL console.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Aggregating NBA data, PostgreSQL vs MongoDB</title>
      <link>http://tapoueh.org/blog/2014/02/aggregating-nba-data-postgresql-vs-mongodb/</link>
      <pubDate>Mon, 17 Feb 2014 23:40:00 +0100</pubDate>
      
      <guid>http://tapoueh.org/blog/2014/02/aggregating-nba-data-postgresql-vs-mongodb/</guid>
      <description>When reading the article Crunching 30 Years of NBA Data with MongoDB Aggregation I coulnd&amp;rsquo;t help but think that we&amp;rsquo;ve been enjoying aggregates in SQL for 3 or 4 decades already. When using PostgreSQL it&amp;rsquo;s even easy to actually add your own aggregates given the SQL command create aggregate.
Photo Credit: Copyright All rights reserved by Segward Graupner
The next step after thinking how obvious the queries written in the mentionned article would be to express in SQL was to actually load the data into PostgreSQL and write the aggregate queries, of course.</description>
    </item>
    
    <item>
      <title>Aggregating NBA data, PostgreSQL vs MongoDB</title>
      <link>http://tapoueh.org/manual-post/2014/02/aggregating-nba-data-postgresql-vs-mongodb/</link>
      <pubDate>Mon, 17 Feb 2014 23:40:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/manual-post/2014/02/aggregating-nba-data-postgresql-vs-mongodb/</guid>
      <description>&lt;p&gt;When reading the
article
&lt;a href=&#34;http://thecodebarbarian.wordpress.com/2014/02/14/crunching-30-years-of-nba-data-with-mongodb-aggregation/&#34;&gt;Crunching 30 Years of NBA Data with MongoDB Aggregation&lt;/a&gt; I
coulnd&amp;rsquo;t help but think that we&amp;rsquo;ve been
enjoying
&lt;a href=&#34;http://www.postgresql.org/docs/current/static/tutorial-agg.html&#34;&gt;aggregates&lt;/a&gt; in
SQL for 3 or 4 decades already. When
using &lt;a href=&#34;http://www.postgresql.org/&#34;&gt;PostgreSQL&lt;/a&gt; it&amp;rsquo;s even easy to actually
add your own aggregates given the SQL
command
&lt;a href=&#34;http://www.postgresql.org/docs/current/static/sql-createaggregate.html&#34;&gt;create aggregate&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Denormalizing Tags</title>
      <link>http://tapoueh.org/blog/2013/10/denormalizing-tags/</link>
      <pubDate>Thu, 24 Oct 2013 13:40:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2013/10/denormalizing-tags/</guid>
      <description>In our Tour of Extensions today&amp;rsquo;s article is about advanced tag indexing. We have a great data collection to play with and our goal today is to be able to quickly find data matching a complex set of tags. So, let&amp;rsquo;s find out those lastfm tracks that are tagged as blues and rhythm and blues, for instance.
In this article, we&amp;rsquo;re going to play with music related tags
We&amp;rsquo;re going to use the Last.</description>
    </item>
    
    <item>
      <title>An Interview about MariaDB and PostgreSQL</title>
      <link>http://tapoueh.org/blog/2013/10/an-interview-about-mariadb-and-postgresql/</link>
      <pubDate>Wed, 16 Oct 2013 21:07:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2013/10/an-interview-about-mariadb-and-postgresql/</guid>
      <description>At the Open World Forum two weeks ago I had the pleasure to meet with Colin Charles. We had a nice talk about the current state of both MariaDB and PostgreSQL, and even were both interviewed by the Open World Forum Team. The interview is now available online. Dear French readers, it&amp;rsquo;s in English.
Allo Mum? Yeah, I&amp;rsquo;m on TV. Well, Actually, Internet TV.
Here&amp;rsquo;s the video:
 Executive Summary: MariaDB is a drop-in fully Open Source replacement for MySQL and sees quite some progress and innovation being made, and PostgreSQL is YeSQL!</description>
    </item>
    
    <item>
      <title>PostgreSQL Autonomous Transaction</title>
      <link>http://tapoueh.org/blog/2013/10/postgresql-autonomous-transaction/</link>
      <pubDate>Mon, 14 Oct 2013 11:25:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2013/10/postgresql-autonomous-transaction/</guid>
      <description>PostgreSQL is an all round impressive Relational DataBase Management System which implements the SQL standard (see the very useful reference page Comparison of different SQL implementations for details). PostgreSQL also provides with unique solutions in the database market and has been leading innovation for some years now. Still, there&amp;rsquo;s no support for Autonomous Transactions within the server itself. Let&amp;rsquo;s have a look at how to easily implement them with PL/Proxy.</description>
    </item>
    
    <item>
      <title>Geolocation with PostgreSQL</title>
      <link>http://tapoueh.org/blog/2013/10/geolocation-with-postgresql/</link>
      <pubDate>Wed, 09 Oct 2013 17:42:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2013/10/geolocation-with-postgresql/</guid>
      <description>Let&amp;rsquo;s get back to our Tour of Extensions that had to be kept aside for awhile with other concerns such as last chance PostgreSQL data recovery. Now that we have a data loading tool up to the task (read about it in the Loading Geolocation Data article) we&amp;rsquo;re going to be able to play with the awesome ip4r extension from RhodiumToad.
The name of the game is to put IP adresses on a map</description>
    </item>
    
    <item>
      <title>PostgreSQL data recovery</title>
      <link>http://tapoueh.org/blog/2013/09/postgresql-data-recovery/</link>
      <pubDate>Tue, 17 Sep 2013 10:39:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2013/09/postgresql-data-recovery/</guid>
      <description>The following story is only interesting to read if you like it when bad things happen, or if you don&amp;rsquo;t have a trustworthy backup policy in place. By trustworthy I mean that each backup you take must be tested with a test recovery job. Only tested backups will prove useful when you need them. So go read our Backup and Restore documentation chapter then learn how to setup Barman for handling physical backups and Point In Time Recovery.</description>
    </item>
    
    <item>
      <title>Using trigrams against typos</title>
      <link>http://tapoueh.org/blog/2013/09/using-trigrams-against-typos/</link>
      <pubDate>Fri, 06 Sep 2013 16:15:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2013/09/using-trigrams-against-typos/</guid>
      <description>In our ongoing Tour of Extensions we played with earth distance in How far is the nearest pub? then with hstore in a series about trigger, first to generalize Trigger Parameters then to enable us to Auditing Changes with Hstore. Today we are going to work with pg_trgm which is the trigrams PostgreSQL extension: its usage got seriously enhanced in recent PostgreSQL releases and it&amp;rsquo;s now a poor&amp;rsquo;s man Full Text Search engine.</description>
    </item>
    
    <item>
      <title>Auditing Changes with Hstore</title>
      <link>http://tapoueh.org/blog/2013/08/auditing-changes-with-hstore/</link>
      <pubDate>Tue, 27 Aug 2013 17:35:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2013/08/auditing-changes-with-hstore/</guid>
      <description>In a previous article about Trigger Parameters we have been using the extension hstore in order to compute some extra field in our records, where the fields used both for the computation and for storing the results were passed in as dynamic parameters. Today we&amp;rsquo;re going to see another trigger use case for hstore: we are going to record changes made to our tuples.
Comparing hstores One of the operators that hstore propose is the hstore - hstore operator whose documentation says that it will delete matching pairs from left operand.</description>
    </item>
    
    <item>
      <title>Trigger Parameters</title>
      <link>http://tapoueh.org/blog/2013/08/trigger-parameters/</link>
      <pubDate>Fri, 23 Aug 2013 12:08:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2013/08/trigger-parameters/</guid>
      <description>Sometimes you want to compute values automatically at INSERT time, like for example a duration column out of a start and an end column, both timestamptz. It&amp;rsquo;s easy enough to do with a BEFORE TRIGGER on your table. What&amp;rsquo;s more complex is to come up with a parametrized spelling of the trigger, where you can attach the same stored procedure to any table even when the column names are different from one another.</description>
    </item>
    
    <item>
      <title>Understanding Window Functions</title>
      <link>http://tapoueh.org/manual-post/2013/08/window-functions/</link>
      <pubDate>Tue, 20 Aug 2013 12:04:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/manual-post/2013/08/window-functions/</guid>
      <description>&lt;p&gt;There was SQL
before
&lt;a href=&#34;http://www.postgresql.org/docs/current/static/tutorial-window.html&#34;&gt;window functions&lt;/a&gt; and
SQL after &lt;em&gt;window functions&lt;/em&gt;: that&amp;rsquo;s how powerful this tool is. Being that
of a deal breaker unfortunately means that it can be quite hard to grasp the
feature. This article aims at making it crystal clear so that you can begin
using it today and are able to reason about it and recognize cases where you
want to be using &lt;em&gt;window functions&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Understanding Window Functions</title>
      <link>http://tapoueh.org/blog/2013/08/understanding-window-functions/</link>
      <pubDate>Tue, 20 Aug 2013 12:04:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2013/08/understanding-window-functions/</guid>
      <description>&lt;p&gt;There was SQL before
&lt;a href=&#34;http://www.postgresql.org/docs/current/static/tutorial-window.html&#34;&gt;window functions&lt;/a&gt; and SQL after
&lt;em&gt;window functions&lt;/em&gt;: that&amp;rsquo;s
how powerful this tool is. Being that of a deal breaker unfortunately means
that it can be quite hard to grasp the feature. This article aims at making
it crystal clear so that you can begin using it today and are able to reason
about it and recognize cases where you want to be using
&lt;em&gt;window functions&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;


 
  
  
  
  
    
      
    
  
    
  
    
      
    
  

&lt;div class=&#34;figure fig50 dim-margin&#34; &gt;
  
    &lt;a class=&#34;fancybox&#34; href=&#34;http://tapoueh.org/img/old/moving_window.gif&#34; data-fancybox-group=&#34;&#34;&gt;
  
    &lt;img class=&#34;fig-img&#34; src=&#34;http://tapoueh.org/img/old/moving_window.gif&#34; &gt;
  
    &lt;/a&gt;
  
  
&lt;/div&gt;

&lt;/center&gt;
&lt;center&gt;&lt;em&gt;We see a part of the data as if through a little window&lt;/em&gt;&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>How far is the nearest pub?</title>
      <link>http://tapoueh.org/manual-post/2013/08/earthdistance/</link>
      <pubDate>Mon, 05 Aug 2013 08:11:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/manual-post/2013/08/earthdistance/</guid>
      <description>&lt;p&gt;In our recent article
about &lt;a href=&#34;http://tapoueh.org/blog/2013/08/02-pub-names-knn&#34;&gt;The Most Popular Pub Names&lt;/a&gt; we did
have a look at how to find the pubs nearby, but didn&amp;rsquo;t compute the
&lt;strong&gt;distance&lt;/strong&gt; in between that pub and us. That&amp;rsquo;s because how to compute a
distance given a position on the earth expressed as &lt;em&gt;longitude&lt;/em&gt; and
&lt;em&gt;latitude&lt;/em&gt; is not that easy. Today, we are going to solve that problem
nonetheless, thanks
to
&lt;a href=&#34;http://www.postgresql.org/docs/9.2/interactive/extend-extensions.html&#34;&gt;PostgreSQL Extensions&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>How far is the nearest pub?</title>
      <link>http://tapoueh.org/blog/2013/08/how-far-is-the-nearest-pub/</link>
      <pubDate>Mon, 05 Aug 2013 08:11:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2013/08/how-far-is-the-nearest-pub/</guid>
      <description>In our recent article about The Most Popular Pub Names we did have a look at how to find the pubs nearby, but didn&amp;rsquo;t compute the distance in between that pub and us. That&amp;rsquo;s because how to compute a distance given a position on the earth expressed as longitude and latitude is not that easy. Today, we are going to solve that problem nonetheless, thanks to PostgreSQL Extensions.
Some math are required to go from (long, lat) to distance on earth</description>
    </item>
    
    <item>
      <title>The Most Popular Pub Names</title>
      <link>http://tapoueh.org/manual-post/2013/08/pub-names-knn/</link>
      <pubDate>Fri, 02 Aug 2013 10:19:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/manual-post/2013/08/pub-names-knn/</guid>
      <description>&lt;p&gt;In his article
titled
&lt;a href=&#34;http://blog.mongodb.org/post/56876800071/the-most-popular-pub-names?utm_content=buffer4922c&amp;amp;utm_source=buffer&amp;amp;utm_medium=facebook&amp;amp;utm_campaign=Buffer&#34;&gt;The Most Popular Pub Names&lt;/a&gt; &lt;em&gt;Ross
Lawley&lt;/em&gt; did show us how to perform some quite interesting &lt;em&gt;geographic
queries&lt;/em&gt; against &lt;a href=&#34;http://www.mongodb.org/&#34;&gt;MongoDB&lt;/a&gt;, using some nice &lt;em&gt;Open
Data&lt;/em&gt; found at the &lt;a href=&#34;http://www.openstreetmap.org/&#34;&gt;Open Street Map&lt;/a&gt; project.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>The Most Popular Pub Names</title>
      <link>http://tapoueh.org/blog/2013/08/the-most-popular-pub-names/</link>
      <pubDate>Fri, 02 Aug 2013 10:19:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2013/08/the-most-popular-pub-names/</guid>
      <description>In his article titled The Most Popular Pub Names Ross Lawley did show us how to perform some quite interesting geographic queries against MongoDB, using some nice Open Data found at the Open Street Map project.
   
The Open Street Map project publishes a lot of information!
I found the idea behind that article really neat: using easily accessible data produced by an Open Source project to show off some nice queries with real data is what we should do more often.</description>
    </item>
    
    <item>
      <title>Archiving data as fast as possible</title>
      <link>http://tapoueh.org/blog/2013/07/archiving-data-as-fast-as-possible/</link>
      <pubDate>Fri, 05 Jul 2013 15:30:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2013/07/archiving-data-as-fast-as-possible/</guid>
      <description>In a recent article here we&amp;rsquo;ve been talking about how do do Batch Updates in a very efficient way, using the Writable CTE features available in PostgreSQL 9.1. I sometime read how Common Table Expressions changed the life of fellow DBAs and developers, and would say that Writable CTE are at least the same boost again.
Writable CTEs allow to easily implement data processing pipelines
In the case of archiving data into side tables the pipeline we&amp;rsquo;re talking about aims to move data out of a table (that&amp;rsquo;s a DELETE) then store it on the destination ( archiving) table, and that&amp;rsquo;s an INSERT:</description>
    </item>
    
    <item>
      <title>Simple Case for Pivoting in SQL</title>
      <link>http://tapoueh.org/blog/2013/07/simple-case-for-pivoting-in-sql/</link>
      <pubDate>Thu, 04 Jul 2013 15:55:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2013/07/simple-case-for-pivoting-in-sql/</guid>
      <description>In a recent article Craig Kerstiens from Heroku did demo the really useful crosstab extension. That function allows you to pivot a table so that you can see the data from different categories in separate columns in the same row rather than in separate rows. The article from Craig is Pivoting in Postgres.
Pivoting a matrix, also known as a matrix transposition
Let&amp;rsquo;s do the same setup as he did, with a table containing some randomly generated data about hypothetical visits to a web page, say, by date then by operating system.</description>
    </item>
    
    <item>
      <title>Nearest Big City</title>
      <link>http://tapoueh.org/blog/2013/05/nearest-big-city/</link>
      <pubDate>Thu, 02 May 2013 11:34:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2013/05/nearest-big-city/</guid>
      <description>In this article, we want to find the town with the greatest number of inhabitants near a given location.
A very localized example We first need to find and import some data, and I found at the following place a CSV listing of french cities with coordinates and population and some numbers of interest for the exercise here.
To import the data set, we first need a table, then a COPY command:</description>
    </item>
    
    <item>
      <title>The Need For Speed</title>
      <link>http://tapoueh.org/blog/2013/03/the-need-for-speed/</link>
      <pubDate>Fri, 29 Mar 2013 09:49:00 +0100</pubDate>
      
      <guid>http://tapoueh.org/blog/2013/03/the-need-for-speed/</guid>
      <description>Hier se tenait la cinquième édition de la conférence organisée par dalibo, où des intervenants extérieurs sont régulièrement invités. Le thème hier était à la fois clair et très vaste : la performance.
   
J&amp;rsquo;ai eu le plaisir de réaliser une présentation intitulée « The Need for Speed » dans laquelle on replace l&amp;rsquo;effort d&amp;rsquo;optimisation dans son contexte métier, afin de faire une étude des coûts et bénéfices et de savoir non seulement à quoi s&amp;rsquo;attendre mais aussi quand s&amp;rsquo;arrêter.</description>
    </item>
    
    <item>
      <title>Bulk Replication</title>
      <link>http://tapoueh.org/blog/2013/03/bulk-replication/</link>
      <pubDate>Mon, 18 Mar 2013 14:54:00 +0100</pubDate>
      
      <guid>http://tapoueh.org/blog/2013/03/bulk-replication/</guid>
      <description>In the previous article here we talked about how to properly update more than one row at a time, under the title Batch Update. We did consider performances, including network round trips, and did look at the behavior of our results when used concurrently.
A case where we want to apply the previous article approach is when replicating data with a trigger based solution, such as SkyTools and londiste. Well, maybe not in all cases, we need to have a amount of UPDATE trafic worthy of setting up the solution.</description>
    </item>
    
    <item>
      <title>Batch Update</title>
      <link>http://tapoueh.org/blog/2013/03/batch-update/</link>
      <pubDate>Fri, 15 Mar 2013 10:47:00 +0100</pubDate>
      
      <guid>http://tapoueh.org/blog/2013/03/batch-update/</guid>
      <description>Performance consulting involves some tricks that you have to teach over and over again. One of them is that SQL tends to be so much better at dealing with plenty of rows in a single statement when compared to running as many statements, each one against a single row.
Another kind of Batch to update
So when you need to UPDATE a bunch of rows from a given source, remember that you can actually use a JOIN in the update statement.</description>
    </item>
    
    <item>
      <title>HyperLogLog Unions</title>
      <link>http://tapoueh.org/blog/2013/02/hyperloglog-unions/</link>
      <pubDate>Tue, 26 Feb 2013 12:44:00 +0100</pubDate>
      
      <guid>http://tapoueh.org/blog/2013/02/hyperloglog-unions/</guid>
      <description>In the article from yesterday we talked about PostgreSQL HyperLogLog with some details. The real magic of that extension has been skimmed over though, and needs another very small article all by itself, in case you missed it.
Which Set Operation do you want for counting unique values?
The first query here has the default level of magic in it, really. What happens is that each time we do an update of the HyperLogLog hash value, we update some data which are allowing us to compute its cardinality.</description>
    </item>
    
    <item>
      <title>PostgreSQL HyperLogLog</title>
      <link>http://tapoueh.org/blog/2013/02/postgresql-hyperloglog/</link>
      <pubDate>Mon, 25 Feb 2013 10:23:00 +0100</pubDate>
      
      <guid>http://tapoueh.org/blog/2013/02/postgresql-hyperloglog/</guid>
      <description>If you&amp;rsquo;ve been following along at home the newer statistics developments, you might have heard about this new State of The Art Cardinality Estimation Algorithm called HyperLogLog. This technique is now available for PostgreSQL in the extension postgresql-hll available at https://github.com/aggregateknowledge/postgresql-hll and soon to be in debian.
How to Compute Cardinality?
Installing postgresql-hll It&amp;rsquo;s as simple as CREATE EXTENSION hll; really, even if to get there you must have installed the package on your system.</description>
    </item>
    
    <item>
      <title>Extensions Templates</title>
      <link>http://tapoueh.org/blog/2013/01/extensions-templates/</link>
      <pubDate>Tue, 08 Jan 2013 17:53:00 +0100</pubDate>
      
      <guid>http://tapoueh.org/blog/2013/01/extensions-templates/</guid>
      <description>In a recent article titled Inline Extensions we detailed the problem of how to distribute an extension&amp;rsquo;s package to a remote server without having access to its file system at all. The solution to that problem is non trivial, let&amp;rsquo;s say. But thanks to the awesome PostgreSQL Community we finaly have some practical ideas on how to address the problem as discussed on pgsql-hackers, our development mailing list.
PostgreSQL is first an Awesome Community</description>
    </item>
    
    <item>
      <title>Inline Extensions</title>
      <link>http://tapoueh.org/blog/2012/12/inline-extensions/</link>
      <pubDate>Thu, 13 Dec 2012 11:34:00 +0100</pubDate>
      
      <guid>http://tapoueh.org/blog/2012/12/inline-extensions/</guid>
      <description>We&amp;rsquo;ve been having the CREATE EXTENSION feature in PostgreSQL for a couple of releases now, so let&amp;rsquo;s talk about how to go from here. The first goal of the extension facility has been to allow for a clean dump and restore process of contrib modules. As such it&amp;rsquo;s been tailored to the needs of deploying files on the file system because there&amp;rsquo;s no escaping from that when you have to ship binary and executable files, those infamous .</description>
    </item>
    
    <item>
      <title>Prefixes and Ranges</title>
      <link>http://tapoueh.org/blog/2012/10/prefixes-and-ranges/</link>
      <pubDate>Tue, 16 Oct 2012 10:47:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2012/10/prefixes-and-ranges/</guid>
      <description>It&amp;rsquo;s been a long time since I last had some time to spend on the prefix PostgreSQL extension and its prefix_range data type. With PostgreSQL 9.2 out, some users wanted me to update the extension for that release, and hinted me that it was high time that I fix that old bug for which I already had a patch.
prefix_range release 1.2.0 I&amp;rsquo;m sorry it took that long. It&amp;rsquo;s now done, you can have prefix 1.</description>
    </item>
    
    <item>
      <title>Reset Counter</title>
      <link>http://tapoueh.org/blog/2012/10/reset-counter/</link>
      <pubDate>Fri, 05 Oct 2012 09:44:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2012/10/reset-counter/</guid>
      <description>I&amp;rsquo;ve been given a nice puzzle that I think is a good blog article opportunity, as it involves some thinking and window functions.
What&amp;rsquo;s to solve Say we store in a table entries from a counter that only increases and the time stamp when we did the measurement. So that when you read 30 then later 40 in fact that means we counted 10 more the second reading when compared to the first, in other words the first 30 are counted again in the second counter value, 40.</description>
    </item>
    
    <item>
      <title>PostgreSQL 9.3</title>
      <link>http://tapoueh.org/blog/2012/09/postgresql-9.3/</link>
      <pubDate>Sat, 15 Sep 2012 18:43:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2012/09/postgresql-9.3/</guid>
      <description>PostgreSQL 9.2 is released! It&amp;rsquo;s an awesome new release that I urge you to consider trying and adopting, an upgrade from even 9.1 should be very well worth it, as your hardware could suddenly be able to process a much higher load. Indeed, better performances mean more work done on the same budget, that&amp;rsquo;s the name of the game!
As a PostgreSQL contributor though, the release of 9.2 mainly means to me that it&amp;rsquo;s time to fully concentrate on preparing 9.</description>
    </item>
    
    <item>
      <title>Clean PGQ Subconsumers</title>
      <link>http://tapoueh.org/blog/2012/04/clean-pgq-subconsumers/</link>
      <pubDate>Thu, 26 Apr 2012 15:05:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2012/04/clean-pgq-subconsumers/</guid>
      <description>Now that you&amp;rsquo;re all using the wonders of Cooperative Consumers to help you efficiently and reliably implement your business constraints and offload them from the main user transactions, you&amp;rsquo;re reaching a point where you have to clean up your development environment (because that&amp;rsquo;s what happens to development environments, right?), and you want a way to start again from a clean empty place.
Here we go. It used to be much more simple than that, so if you&amp;rsquo;re still using PGQ from Skytools2, just jump to the next step.</description>
    </item>
    
    <item>
      <title>PGQ Coop Consumers</title>
      <link>http://tapoueh.org/blog/2012/03/pgq-coop-consumers/</link>
      <pubDate>Mon, 12 Mar 2012 14:43:00 +0100</pubDate>
      
      <guid>http://tapoueh.org/blog/2012/03/pgq-coop-consumers/</guid>
      <description>While working a new PostgreSQL architecture for an high scale project that used to be in the top 10 of internet popular web sites (in terms of visitors), I needed to be able to off load some processing from the main path: that&amp;rsquo;s called a batch job. This needs to be transactional: don&amp;rsquo;t run the job if we did rollback; the transaction, process all events that were part of the same transaction in the same transaction, etc.</description>
    </item>
    
    <item>
      <title>Extension White Listing</title>
      <link>http://tapoueh.org/blog/2012/03/extension-white-listing/</link>
      <pubDate>Thu, 08 Mar 2012 14:25:00 +0100</pubDate>
      
      <guid>http://tapoueh.org/blog/2012/03/extension-white-listing/</guid>
      <description>PostgreSQL 9.1 includes proper extension support, as you might well know if you ever read this very blog here. Some hosting facilities are playing with PostgreSQL at big scale (hello Heroku!) and still meet with small caveats making their life uneasy.
To be specific, only superusers are allowed to install C coded stored procedures, and that impacts a lot of very useful PostgreSQL extension: all those shiped in the contrib package are coded in C.</description>
    </item>
    
    <item>
      <title>pgbouncer munin plugin</title>
      <link>http://tapoueh.org/blog/2011/11/pgbouncer-munin-plugin/</link>
      <pubDate>Wed, 16 Nov 2011 14:00:00 +0100</pubDate>
      
      <guid>http://tapoueh.org/blog/2011/11/pgbouncer-munin-plugin/</guid>
      <description>It seems that if you search for a munin plugin for pgbouncer it&amp;rsquo;s easy enough to reach an old page of mine with an old version of my plugin, and a broken link. Let&amp;rsquo;s remedy that by publishing here the newer version of the plugin. To be honest, I though it already made its way into the official munin 1.4 set of plugins, but I&amp;rsquo;ve not been following closely enough.</description>
    </item>
    
    <item>
      <title>Extensions en simple SQL</title>
      <link>http://tapoueh.org/blog/2011/10/extensions-en-simple-sql/</link>
      <pubDate>Mon, 31 Oct 2011 14:22:00 +0100</pubDate>
      
      <guid>http://tapoueh.org/blog/2011/10/extensions-en-simple-sql/</guid>
      <description>La conférence européenne à Amsterdam était un très bon évènement de la communauté, avec une organisation impeccable dans un hôtel accueillant. J&amp;rsquo;ai eu le plaisir d&amp;rsquo;y parler des extensions et de leur usage dans le cadre du développement applicatif « interne », sous le titre Extensions are good for business logic.
   
L&amp;rsquo;idée de ma présentation, que la plupart d&amp;rsquo;entre vous a loupé je suppose (en tout cas je n&amp;rsquo;avais qu&amp;rsquo;une petite poignée de français dans la salle, et j&amp;rsquo;espère avoir des lecteurs qui n&amp;rsquo;étaient pas à Amsterdam), l&amp;rsquo;idée est d&amp;rsquo;utiliser les mécanismes offerts par les extensions afin de maintenir le code PL que vous utilisez en production.</description>
    </item>
    
    <item>
      <title>Scaling Stored Procedures</title>
      <link>http://tapoueh.org/blog/2011/10/scaling-stored-procedures/</link>
      <pubDate>Thu, 06 Oct 2011 18:23:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2011/10/scaling-stored-procedures/</guid>
      <description>In the news recently stored procedures where used as an excuse for moving away logic from the database layer to application layer, and to migrate away from a powerful technology to a simpler one, now that there&amp;rsquo;s no logic anymore in the database.
It&amp;rsquo;s not the way I would typically approach scaling problems, and apparently I&amp;rsquo;m not alone on the Stored Procedures camp. Did you read this nice blog post Mythbusters: Stored Procedures Edition already?</description>
    </item>
    
    <item>
      <title>Skytools3: walmgr</title>
      <link>http://tapoueh.org/blog/2011/09/skytools3-walmgr/</link>
      <pubDate>Wed, 21 Sep 2011 17:21:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2011/09/skytools3-walmgr/</guid>
      <description>Let&amp;rsquo;s begin the Skytools 3 documentation effort, which is long overdue. The code is waiting for you over at github, and is stable and working. Why is it still in release candidate status, I hear you asking? Well because it&amp;rsquo;s missing updated documentation.
WalMgr is the Skytools component that manages WAL shipping for you, and archiving too. It knows how to prepare your master and standby setup, how to take a base backup and push it to the standby&amp;rsquo;s system, how to archive (at the satndby) master&amp;rsquo;s WAL files as they are produced and have the standby restore from this archive.</description>
    </item>
    
    <item>
      <title>PostgreSQL 9.1</title>
      <link>http://tapoueh.org/blog/2011/09/postgresql-9.1/</link>
      <pubDate>Wed, 14 Sep 2011 10:00:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2011/09/postgresql-9.1/</guid>
      <description>PostgreSQL 9.1 est dans les bacs ! Vous n&amp;rsquo;avez pas encore cette nouvelle version en production ? Pas encore évalué pourquoi vous devriez envisager de migrer à cette version ? Il existe beaucoup de bonnes raisons de passer à cette version, et peu de pièges.
Nous commençons à lire des articles qui reprennent la nouvelle dans la presse française, et j&amp;rsquo;ai le plaisir de mentionner celui de programmez.com qui annonce « un système d&amp;rsquo;extensions inégalé ».</description>
    </item>
    
    <item>
      <title>Éviter les injections SQL</title>
      <link>http://tapoueh.org/blog/2011/09/%C3%A9viter-les-injections-sql/</link>
      <pubDate>Wed, 07 Sep 2011 11:36:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2011/09/%C3%A9viter-les-injections-sql/</guid>
      <description>Nous avons parlé la dernière fois les règles d&amp;rsquo; échappement de chaînes avec PostgreSQL, et mentionné qu&amp;rsquo;utiliser ces techniques afin de protéger les données insérées dans les requêtes SQL n&amp;rsquo;était pas une bonne idée dans la mesure où PostgreSQL offre une fonctionnalité bien plus adaptée.
Nous faisons face ici à un problème de sécurité très bien décrit dans le billet humoristique de Little Boby Tables, dont je vous recommande la lecture.</description>
    </item>
    
    <item>
      <title>Skytools, version 3</title>
      <link>http://tapoueh.org/blog/2011/08/skytools-version-3/</link>
      <pubDate>Fri, 26 Aug 2011 21:30:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2011/08/skytools-version-3/</guid>
      <description>You can find skytools3 in debian experimental already, it&amp;rsquo;s in release candidate status. What&amp;rsquo;s missing is the documentation, so here&amp;rsquo;s an idea: I&amp;rsquo;m going to make a blog post series about skytools next features, how to use them, what they are good for, etc. This first article of the series will just list what are those new features.
Here are the slides from the CHAR(11) talk I made last month, about that very subject:</description>
    </item>
    
    <item>
      <title>Échappement de chaînes</title>
      <link>http://tapoueh.org/blog/2011/08/%C3%A9chappement-de-cha%C3%AEnes/</link>
      <pubDate>Thu, 18 Aug 2011 19:01:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2011/08/%C3%A9chappement-de-cha%C3%AEnes/</guid>
      <description>Parmis les nouveautés de la prochaine version de PostgreSQL, la fameuse 9.1, il faut signaler le changement de valeur par défaut de la variable standard_conforming_strings, qui passe à vraie.
En effet, l&amp;rsquo;utilisation d&amp;rsquo;échappements avec le caractère « anti-slash » n&amp;rsquo;est pas conforme au standard SQL. Le paramètre standard_conforming_strings permet de contrôler le comportement de PostgreSQL lorsqu&amp;rsquo;il lit une chaîne de caractère dans une requête SQL.
Voyons quelques exemples :
dimitri=# set standard_conforming_strings to true; SET dimitri=# select &#39;hop&#39;&#39;&#39;; ?</description>
    </item>
    
    <item>
      <title>See Tsung in action</title>
      <link>http://tapoueh.org/blog/2011/08/see-tsung-in-action/</link>
      <pubDate>Tue, 02 Aug 2011 10:30:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2011/08/see-tsung-in-action/</guid>
      <description>Tsung is an open-source multi-protocol distributed load testing tool and a mature project. It&amp;rsquo;s been available for about 10 years and is built with the Erlang system. It supports several protocols, including the PostgreSQL one.
When you want to benchmark your own application, to know how many more clients it can handle or how much gain you will see with some new shiny hardware, Tsung is the tool to use. It will allow you to record a number of sessions then replay them at high scale.</description>
    </item>
    
    <item>
      <title>Next month partitions</title>
      <link>http://tapoueh.org/blog/2011/07/next-month-partitions/</link>
      <pubDate>Wed, 27 Jul 2011 22:35:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2011/07/next-month-partitions/</guid>
      <description>When you do partition your tables monthly, then comes the question of when to create next partitions. I tend to create them just the week before next month and I have some nice nagios scripts to alert me in case I&amp;rsquo;ve forgotten to do so. How to check that by hand in the end of a month?
Here&amp;rsquo;s a catalog query to help you there:
=&amp;gt; select * -&amp;gt; from -&amp;gt; ( (&amp;gt; select &#39;previous parts&#39; as schemaname, count(*)::text as tablename (&amp;gt; from pg_tables (&amp;gt; where schemaname not in (&#39;pg_catalog&#39;,&#39;information_schema&#39;) (&amp;gt; and tablename like to_char(now(), &#39;%YYYYMM&#39;) (&amp;gt; (&amp;gt; union (&amp;gt; (&amp;gt; select schemaname, substring(tablename,1,length(tablename)-6) || &#39;201108&#39; (&amp;gt; from pg_tables (&amp;gt; where schemaname not in (&#39;pg_catalog&#39;,&#39;information_schema&#39;) (&amp;gt; and tablename like to_char(now(), &#39;%YYYYMM&#39;) (&amp;gt; (&amp;gt; except (&amp;gt; (&amp;gt; select schemaname, tablename (&amp;gt; from pg_tables (&amp;gt; where schemaname not in (&#39;pg_catalog&#39;,&#39;information_schema&#39;) (&amp;gt; and tablename like to_char(now() + interval &#39;1 month&#39;, &#39;%YYYYMM&#39;) (&amp;gt; ) as t -&amp;gt; order by schemaname &amp;lt;&amp;gt; &#39;previous parts&#39;, schemaname; schemaname | tablename ----------------+------------------------ previous parts | 1 central | stats_entrantes_201108 (2 rows)  As you see, our partitions are named _YYYYMM so that&amp;rsquo;s it&amp;rsquo;s easy to match them in our queries, but I guess about everyone does about the same here.</description>
    </item>
    
    <item>
      <title>Preparing for PGCON</title>
      <link>http://tapoueh.org/blog/2011/05/preparing-for-pgcon/</link>
      <pubDate>Thu, 12 May 2011 10:30:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2011/05/preparing-for-pgcon/</guid>
      <description>It&amp;rsquo;s this time of the year again, the main international PostgreSQL Conference is next week in Ottawa, Canada. If previous years are any indication, this will be great event where to meet with a lot of the members of your community. The core team will be there, developers will be there, and we will meet with users and their challenging use cases.
This is a very good time to review both what you did in the project those last 12 months, and what you plan to do next year.</description>
    </item>
    
    <item>
      <title>Tables and Views dependencies</title>
      <link>http://tapoueh.org/blog/2011/05/tables-and-views-dependencies/</link>
      <pubDate>Wed, 04 May 2011 11:45:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2011/05/tables-and-views-dependencies/</guid>
      <description>Let&amp;rsquo;s say you need to ALTER TABLE foo ALTER COLUMN bar TYPE bigint;, and PostgreSQL is helpfully telling you that no you can&amp;rsquo;t because such and such views depend on the column. The basic way to deal with that is to copy paste from the error message the names of the views involved, then prepare a script wherein you first DROP VIEW ...; then ALTER TABLE and finally CREATE VIEW again, all in the same transaction.</description>
    </item>
    
    <item>
      <title>Dynamic Triggers in PLpgSQL</title>
      <link>http://tapoueh.org/blog/2010/11/dynamic-triggers-in-plpgsql/</link>
      <pubDate>Wed, 24 Nov 2010 16:45:00 +0100</pubDate>
      
      <guid>http://tapoueh.org/blog/2010/11/dynamic-triggers-in-plpgsql/</guid>
      <description>You certainly know that implementing dynamic triggers in PLpgSQL is impossible. But I had a very bad night, being up from as soon as 3:30 am today, so that when a developer asked me about reusing the same trigger function code from more than one table and for a dynamic column name, I didn&amp;rsquo;t remember about it being impossible.
Here&amp;rsquo;s what happens in such cases, after a long time on the problem (yes, overall, that&amp;rsquo;s a slow day).</description>
    </item>
    
    <item>
      <title>pg_basebackup</title>
      <link>http://tapoueh.org/blog/2010/11/pg_basebackup/</link>
      <pubDate>Sun, 07 Nov 2010 13:45:00 +0100</pubDate>
      
      <guid>http://tapoueh.org/blog/2010/11/pg_basebackup/</guid>
      <description>Hannu just gave me a good idea in this email on -hackers, proposing that pg_basebackup should get the xlog files again and again in a loop for the whole duration of the base backup. That&amp;rsquo;s now done in the aforementioned tool, whose options got a little more useful now:
Usage: pg_basebackup.py [-v] [-f] [-j jobs] &amp;quot;dsn&amp;quot; dest Options: -h, --help show this help message and exit --version show version and quit -x, --pg_xlog backup the pg_xlog files -v, --verbose be verbose and about processing progress -d, --debug show debug information, including SQL queries -f, --force remove destination directory if it exists -j JOBS, --jobs=JOBS how many helper jobs to launch -D DELAY, --delay=DELAY pg_xlog subprocess loop delay, see -x -S, --slave auxilliary process --stdin get list of files to backup from stdin  Yeah, as implementing the xlog idea required having some kind of parallelism, I built on it and the script now has a --jobs option for you to setup how many processes to launch in parallel, all fetching some base backup files in its own standard ( libpq) PostgreSQL connection, in compressed chunks of 8 MB (so that&amp;rsquo;s not 8 MB chunks sent over).</description>
    </item>
    
    <item>
      <title>Extensions: writing a patch for PostgreSQL</title>
      <link>http://tapoueh.org/blog/2010/10/extensions-writing-a-patch-for-postgresql/</link>
      <pubDate>Fri, 15 Oct 2010 11:30:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2010/10/extensions-writing-a-patch-for-postgresql/</guid>
      <description>These days, thanks to my community oriented job, I&amp;rsquo;m working full time on a PostgreSQL patch to terminate basic support for extending SQL. First thing I want to share is that patching the backend code is not as hard as one would think. Second one is that git really is helping.
“Not as hard as one would think, are you kidding me?”, I hear some say. Well, that&amp;rsquo;s true. It&amp;rsquo;s C code in there, but with a very good layer of abstractions so that you&amp;rsquo;re not dealing with subtle problems that much.</description>
    </item>
    
    <item>
      <title>Date puzzle for starters</title>
      <link>http://tapoueh.org/blog/2010/10/date-puzzle-for-starters/</link>
      <pubDate>Fri, 08 Oct 2010 10:00:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2010/10/date-puzzle-for-starters/</guid>
      <description>The PostgreSQL IRC channel is a good place to be, for all the very good help you can get there, because people are always wanting to remain helpful, because of the off-topics discussions sometime, or to get to talk with community core members. And to start up your day too.
This morning&amp;rsquo;s question started simple : “how can I check if today is the &amp;ldquo;first sunday fo the month&amp;rdquo;. or &amp;ldquo;the second tuesday of the month&amp;rdquo; etc?</description>
    </item>
    
    <item>
      <title>Resuming work on Extensions, first little step</title>
      <link>http://tapoueh.org/blog/2010/10/resuming-work-on-extensions-first-little-step/</link>
      <pubDate>Thu, 07 Oct 2010 17:15:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2010/10/resuming-work-on-extensions-first-little-step/</guid>
      <description>Yeah I&amp;rsquo;m back on working on my part of the extension thing in PostgreSQL.
First step is a little one, but as it has public consequences, I figured I&amp;rsquo;d talk about it already. I&amp;rsquo;ve just refreshed my git repository to follow the new master one, and you can see that here http://git.postgresql.org/gitweb?p=postgresql-extension.git;a=commitdiff;h=9a88e9de246218e93c04b6b97e1ef61d97925430.
It&amp;rsquo;s been easier than I feared, mainly:
$ git --no-pager diff master..extension $ git --no-pager format-patch master..extension $ cp 0001-First-stab-at-writing-pg_execute_from_file-function.</description>
    </item>
    
    <item>
      <title>Window Functions example remix</title>
      <link>http://tapoueh.org/blog/2010/09/window-functions-example-remix/</link>
      <pubDate>Sun, 12 Sep 2010 21:35:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2010/09/window-functions-example-remix/</guid>
      <description>The drawback of hosting a static only website is, obviously, the lack of comments. What happens actually, though, is that I receive very few comments by direct mail. As I don&amp;rsquo;t get another spam source to cleanup, I&amp;rsquo;m left unconvinced that&amp;rsquo;s such a drawback. I still miss the low probability of seeing blog readers exchange directly, but I think a tapoueh.org mailing list would be my answer, here&amp;hellip;
Anyway, David Fetter took the time to send me a comment by mail with a cleaned up rewrite of the previous entry SQL, here&amp;rsquo;s it for your pleasure!</description>
    </item>
    
    <item>
      <title>Window Functions example</title>
      <link>http://tapoueh.org/blog/2010/09/window-functions-example/</link>
      <pubDate>Thu, 09 Sep 2010 16:35:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2010/09/window-functions-example/</guid>
      <description>So, when 8.4 came out there was all those comments about how getting window functions was an awesome addition. Now, it seems that a lot of people seeking for help in #postgresql just don&amp;rsquo;t know what kind of problem this feature helps solving. I&amp;rsquo;ve already been using them in some cases here in this blog, for getting some nice overview about Partitioning: relation size per “group”.
That&amp;rsquo;s another way to count change</description>
    </item>
    
    <item>
      <title>Synchronous Replication</title>
      <link>http://tapoueh.org/blog/2010/09/synchronous-replication/</link>
      <pubDate>Mon, 06 Sep 2010 18:05:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2010/09/synchronous-replication/</guid>
      <description>Although the new asynchronous replication facility that ships with 9.0 ain&amp;rsquo;t released to the wide public yet, our hackers hero are already working on the synchronous version of it. A part of the facility is rather easy to design, we want something comparable to DRBD flexibility, but specific to our database world. So synchronous would either mean recv, fsync or apply, depending on what you need the standby to have already done when the master acknowledges the COMMIT.</description>
    </item>
    
    <item>
      <title>Happy Numbers</title>
      <link>http://tapoueh.org/blog/2010/08/happy-numbers/</link>
      <pubDate>Mon, 30 Aug 2010 11:00:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2010/08/happy-numbers/</guid>
      <description>After discovering the excellent Gwene service, which allows you to subscribe to newsgroups to read RSS content ( blogs, planets, commits, etc), I came to read this nice article about Happy Numbers. That&amp;rsquo;s a little problem that fits well an interview style question, so I first solved it yesterday evening in Emacs Lisp as that&amp;rsquo;s the language I use the most those days.
 A happy number is defined by the following process.</description>
    </item>
    
    <item>
      <title>Playing with bit strings</title>
      <link>http://tapoueh.org/blog/2010/08/playing-with-bit-strings/</link>
      <pubDate>Thu, 26 Aug 2010 17:45:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2010/08/playing-with-bit-strings/</guid>
      <description>The idea of the day ain&amp;rsquo;t directly from me, I&amp;rsquo;m just helping with a very thin subpart of the problem. The problem, I can&amp;rsquo;t say much about, let&amp;rsquo;s just assume you want to reduce the storage of MD5 in your database, so you want to abuse bit strings. A solution to use them works fine, but the datatype is still missing some facilities, for example going from and to hexadecimal representation in text.</description>
    </item>
    
    <item>
      <title>Editing constants in constraints</title>
      <link>http://tapoueh.org/blog/2010/08/editing-constants-in-constraints/</link>
      <pubDate>Mon, 09 Aug 2010 14:45:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2010/08/editing-constants-in-constraints/</guid>
      <description>We&amp;rsquo;re using constants in some constraints here, for example in cases where several servers are replicating to the same federating one: each origin server has his own schema, and all is replicated nicely on the central host, thanks to Londiste, as you might have guessed already.
For bare-metal recovery scripts, I&amp;rsquo;m working on how to change those constants in the constraints, so that pg_dump -s plus some schema tweaking would kick-start a server.</description>
    </item>
    
    <item>
      <title>Querying the Catalog to plan an upgrade</title>
      <link>http://tapoueh.org/blog/2010/08/querying-the-catalog-to-plan-an-upgrade/</link>
      <pubDate>Thu, 05 Aug 2010 11:00:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2010/08/querying-the-catalog-to-plan-an-upgrade/</guid>
      <description>Some user on IRC was reading the releases notes in order to plan for a minor upgrade of his 8.3.3 installation, and was puzzled about potential needs for rebuilding GIST indexes. That&amp;rsquo;s from the 8.3.5 release notes, and from the 8.3.8 notes you see that you need to consider hash indexes on interval columns too. Now the question is, how to find out if any such beasts are in use in your database?</description>
    </item>
    
    <item>
      <title>Database Virtual Machines</title>
      <link>http://tapoueh.org/blog/2010/08/database-virtual-machines/</link>
      <pubDate>Tue, 03 Aug 2010 13:30:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2010/08/database-virtual-machines/</guid>
      <description>Today I&amp;rsquo;m being told once again about SQLite as an embedded database software. That one ain&amp;rsquo;t a database server but a software library that you can use straight into your main program. I&amp;rsquo;m yet to use it, but it looks like its SQL support is good enough for simple things — and that covers loads of things. I guess read-only cache and configuration storage would be the obvious ones, because it seems that SQLite use cases aren&amp;rsquo;t including mixed concurrency, that is workloads with concurrent readers and writers.</description>
    </item>
    
    <item>
      <title>Partitioning: relation size per “group”</title>
      <link>http://tapoueh.org/blog/2010/07/partitioning-relation-size-per-group/</link>
      <pubDate>Mon, 26 Jul 2010 17:00:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2010/07/partitioning-relation-size-per-group/</guid>
      <description>This time, we are trying to figure out where is the bulk of the data on disk. The trick is that we&amp;rsquo;re using DDL partitioning, but we want a “nice” view of size per partition set. Meaning that if you have for example a parent table foo with partitions foo_201006 and foo_201007, you would want to see a single category foo containing the accumulated size of all the partitions underneath foo.</description>
    </item>
    
    <item>
      <title>Background writers</title>
      <link>http://tapoueh.org/blog/2010/07/background-writers/</link>
      <pubDate>Mon, 19 Jul 2010 16:30:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2010/07/background-writers/</guid>
      <description>There&amp;rsquo;s currently a thread on hackers about bg worker: overview and a series of 6 patches. Thanks a lot Markus! This is all about generalizing a concept already in use in the autovacuum process, where you have an independent subsystem that require having an autonomous daemon running and able to start its own workers.
I&amp;rsquo;ve been advocating about generalizing this concept for awhile already, in order to have postmaster able to communicate to subsystems when to shut down and start and reload, etc.</description>
    </item>
    
    <item>
      <title>Logs analysis</title>
      <link>http://tapoueh.org/blog/2010/07/logs-analysis/</link>
      <pubDate>Tue, 13 Jul 2010 14:15:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2010/07/logs-analysis/</guid>
      <description>Nowadays to analyze logs and provide insights, the more common tool to use is pgfouine, which does an excellent job. But there has been some improvements in logs capabilities that we&amp;rsquo;re not benefiting from yet, and I&amp;rsquo;m thinking about the CSV log format.
So the idea would be to turn pgfouine into a set of SQL queries against the logs themselves once imported into the database. Wait. What about having our next PostgreSQL version, which is meant (I believe) to include CSV support in SQL/MED, to directly expose its logs as a system view?</description>
    </item>
    
    <item>
      <title>Using indexes as column store?</title>
      <link>http://tapoueh.org/blog/2010/07/using-indexes-as-column-store/</link>
      <pubDate>Thu, 08 Jul 2010 11:15:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2010/07/using-indexes-as-column-store/</guid>
      <description>There&amp;rsquo;s a big trend nowadays about using column storage as opposed to what PostgreSQL is doing, which would be row storage. The difference is that if you have the same column value in a lot of rows, you could get to a point where you have this value only once in the underlying storage file. That means high compression. Then you tweak the executor to be able to load this value only once, not once per row, and you win another huge source of data traffic (often enough, from disk).</description>
    </item>
    
    <item>
      <title>Back from PgCon2010</title>
      <link>http://tapoueh.org/blog/2010/05/back-from-pgcon2010/</link>
      <pubDate>Thu, 27 May 2010 14:26:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2010/05/back-from-pgcon2010/</guid>
      <description>This year&amp;rsquo;s edition has been the best pgcon ever for me. Granted, it&amp;rsquo;s only my third time, but still :) As Josh said the &amp;ldquo;Hall Track&amp;rdquo; in particular was very good, and the Dev Meeting has been very effective!
Extensions This time I prepared some slides to present the extension design and I tried hard to make it so that we get to agree on a plan, even recognizing it&amp;rsquo;s not solving all of our problems from the get go.</description>
    </item>
    
    <item>
      <title>Finding orphaned sequences</title>
      <link>http://tapoueh.org/blog/2010/03/finding-orphaned-sequences/</link>
      <pubDate>Wed, 17 Mar 2010 13:35:00 +0100</pubDate>
      
      <guid>http://tapoueh.org/blog/2010/03/finding-orphaned-sequences/</guid>
      <description>This time we&amp;rsquo;re having a database where sequences were used, but not systematically as a default value of a given column. It&amp;rsquo;s mainly an historic bad idea, but you know the usual excuse with bad ideas and bad code: the first 6 months it&amp;rsquo;s experimental, after that it&amp;rsquo;s historic.
Not talking about genome orphaned sequences here, though
Still, here&amp;rsquo;s a query for 8.4 that will allow you to list those sequences you have that are not used as a default value in any of your tables:</description>
    </item>
    
    <item>
      <title>Getting out of SQL_ASCII, part 2</title>
      <link>http://tapoueh.org/blog/2010/02/getting-out-of-sql_ascii-part-2/</link>
      <pubDate>Tue, 23 Feb 2010 17:30:00 +0100</pubDate>
      
      <guid>http://tapoueh.org/blog/2010/02/getting-out-of-sql_ascii-part-2/</guid>
      <description>So, if you followed the previous blog entry, now you have a new database containing all the static tables encoded in UTF-8 rather than SQL_ASCII. Because if it was not yet the case, you now severely distrust this non-encoding.
Now is the time to have a look at properly encoding the live data, those stored in tables that continue to receive write traffic. The idea is to use the UPDATE facilities of PostgreSQL to tweak the data, and too fix the applications so as not to continue inserting badly encoded strings in there.</description>
    </item>
    
    <item>
      <title>Getting out of SQL_ASCII, part 1</title>
      <link>http://tapoueh.org/blog/2010/02/getting-out-of-sql_ascii-part-1/</link>
      <pubDate>Thu, 18 Feb 2010 11:37:00 +0100</pubDate>
      
      <guid>http://tapoueh.org/blog/2010/02/getting-out-of-sql_ascii-part-1/</guid>
      <description>It happens that you have to manage databases designed by your predecessor, and it even happens that the team used to not have a DBA. Those histerical raisins can lead to having a SQL_ASCII database. The horror!
What SQL_ASCII means, if you&amp;rsquo;re not already familiar with the consequences of such a choice, is that all the text and varchar data that you put in the database is accepted as-is. No checks.</description>
    </item>
    
    <item>
      <title>Resetting sequences. All of them, please!</title>
      <link>http://tapoueh.org/blog/2010/02/resetting-sequences.-all-of-them-please/</link>
      <pubDate>Tue, 16 Feb 2010 16:23:00 +0100</pubDate>
      
      <guid>http://tapoueh.org/blog/2010/02/resetting-sequences.-all-of-them-please/</guid>
      <description>So, after restoring a production dump with intermediate filtering, none of our sequences were set to the right value. I could have tried to review the process of filtering the dump here, but it&amp;rsquo;s a one-shot action and you know what that sometimes mean. With some pressure you don&amp;rsquo;t script enough of it and you just crawl more and more.
Still, I think how I solved it is worthy of a blog entry.</description>
    </item>
    
    <item>
      <title>PgCon 2009</title>
      <link>http://tapoueh.org/blog/2009/05/pgcon-2009/</link>
      <pubDate>Wed, 27 May 2009 14:30:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2009/05/pgcon-2009/</guid>
      <description>I can&amp;rsquo;t really compare PgCon 2009 with previous years versions, last time I enjoyed the event it was in 2006, in Toronto. But still I found the experience to be a great one, and I hope I&amp;rsquo;ll be there next year too!
I&amp;rsquo;ve met a lot of known people in the community, some of them I already had the chance to run into at Toronto or Prato, but this was the first time I got to talk to many of them about interresting projects and ideas.</description>
    </item>
    
    <item>
      <title>Skytools 3.0 reaches alpha1</title>
      <link>http://tapoueh.org/blog/2009/04/skytools-3.0-reaches-alpha1/</link>
      <pubDate>Tue, 14 Apr 2009 00:00:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2009/04/skytools-3.0-reaches-alpha1/</guid>
      <description>It&amp;rsquo;s time for Skytools news again! First, we did improve documentation of current stable branch with hosting high level presentations and tutorials on the PostgreSQL wiki. Do check out the Londiste Tutorial, it seems that&amp;rsquo;s what people hesitating to try out londiste were missing the most.
The other things people miss out a lot in current stable Skytools (version 2.1.9 currently) are cascading replication (which allows for switchover and failover) and DDL support.</description>
    </item>
    
    <item>
      <title>Importing XML content from file</title>
      <link>http://tapoueh.org/blog/2009/02/importing-xml-content-from-file/</link>
      <pubDate>Thu, 05 Feb 2009 00:00:00 +0100</pubDate>
      
      <guid>http://tapoueh.org/blog/2009/02/importing-xml-content-from-file/</guid>
      <description>The problem was raised this week on IRC and this time again I felt it would be a good occasion for a blog entry: how to load an XML file content into a single field?
The usual tool used to import files is COPY, but it&amp;rsquo;ll want each line of the file to host a text representation of a database tuple, so it doesn&amp;rsquo;t apply to the case at hand. RhodiumToad was online and offered the following code to solve the problem:</description>
    </item>
    
    <item>
      <title>Skytools ticker daemon and londiste</title>
      <link>http://tapoueh.org/blog/2009/02/skytools-ticker-daemon-and-londiste/</link>
      <pubDate>Tue, 03 Feb 2009 00:00:00 +0100</pubDate>
      
      <guid>http://tapoueh.org/blog/2009/02/skytools-ticker-daemon-and-londiste/</guid>
      <description>One of the difficulties in getting to understand and configure londiste reside in the relation between the ticker and the replication. This question was raised once more on IRC yesterday, so I made a new FAQ entry about it: How do this ticker thing relates to londiste?</description>
    </item>
    
    <item>
      <title>Comparing Londiste and Slony</title>
      <link>http://tapoueh.org/blog/2009/01/comparing-londiste-and-slony/</link>
      <pubDate>Sat, 31 Jan 2009 00:00:00 +0100</pubDate>
      
      <guid>http://tapoueh.org/blog/2009/01/comparing-londiste-and-slony/</guid>
      <description>In the page about Skytools I&amp;rsquo;ve encouraged people to ask some more questions in order for me to be able to try and answer them. That just happened, as usual on the #postgresql IRC, and the question is What does londiste lack that slony has?</description>
    </item>
    
    <item>
      <title>Controling HOT usage in 8.3</title>
      <link>http://tapoueh.org/blog/2009/01/controling-hot-usage-in-8.3/</link>
      <pubDate>Wed, 28 Jan 2009 00:00:00 +0100</pubDate>
      
      <guid>http://tapoueh.org/blog/2009/01/controling-hot-usage-in-8.3/</guid>
      <description>As it happens, I&amp;rsquo;ve got some environments where I want to make sure HOT ( aka Heap Only Tuples) is in use. Because we&amp;rsquo;re doing so much updates a second that I want to get sure it&amp;rsquo;s not killing my database server. I not only wrote some checking view to see about it, but also made a quick article about it in the French PostgreSQL website. Handling around in #postgresql means that I&amp;rsquo;m now bound to write about it in English too!</description>
    </item>
    
    <item>
      <title>Londiste Trick</title>
      <link>http://tapoueh.org/blog/2009/01/londiste-trick/</link>
      <pubDate>Wed, 21 Jan 2009 00:00:00 +0100</pubDate>
      
      <guid>http://tapoueh.org/blog/2009/01/londiste-trick/</guid>
      <description>So, you&amp;rsquo;re using londiste and the ticker has not been running all night long, due to some restart glitch in your procedures, and the on call admin didn&amp;rsquo;t notice the restart failure. If you blindly restart the replication daemon, it will load in memory all those events produced during the night, at once, because you now have only one tick where to put them all.
The following query allows you to count how many events that represents, with the magic tick numbers coming from pgq.</description>
    </item>
    
    <item>
      <title>new site, using new software</title>
      <link>http://tapoueh.org/blog/2008/12/new-site-using-new-software/</link>
      <pubDate>Sat, 06 Dec 2008 00:00:00 +0100</pubDate>
      
      <guid>http://tapoueh.org/blog/2008/12/new-site-using-new-software/</guid>
      <description>Oh and check out the skytools page too. Emacs Muse is so great a project that instead of just working on how to publish a website with this tool, I found myself editing a rather large document about londite.py.</description>
    </item>
    
  </channel>
</rss>