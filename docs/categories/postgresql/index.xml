<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Postgresql on The Art of PostgreSQL</title>
    <link>https://tapoueh.org/categories/postgresql/</link>
    <description>Recent content in Postgresql on The Art of PostgreSQL</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 10 Nov 2018 15:40:01 +0100</lastBuildDate>
    
	<atom:link href="https://tapoueh.org/categories/postgresql/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Preventing SQL Injections</title>
      <link>https://tapoueh.org/blog/2018/11/preventing-sql-injections/</link>
      <pubDate>Sat, 10 Nov 2018 15:40:01 +0100</pubDate>
      
      <guid>https://tapoueh.org/blog/2018/11/preventing-sql-injections/</guid>
      <description>&lt;p&gt;An &lt;em&gt;SQL Injection&lt;/em&gt; is a security breach, one made famous by the &lt;a href=&#34;https://xkcd.com/327/&#34;&gt;Exploits of
a Mom&lt;/a&gt; &lt;code&gt;xkcd&lt;/code&gt; comic episode in which we read about
&lt;em&gt;little Bobby Tables&lt;/em&gt;:&lt;/p&gt;

&lt;figure class=&#34;center&#34;&gt;&lt;a href=&#34;https://xkcd.com/327/&#34;&gt;
    &lt;img src=&#34;https://tapoueh.org/img/exploits_of_a_mom.png&#34;/&gt; &lt;/a&gt;
&lt;/figure&gt;


&lt;p&gt;PostgreSQL implements a protocol level facility to send the static SQL query
text separately from its dynamic arguments. An SQL injection happens when
the database server is mistakenly led to consider a dynamic argument of a
query as part of the query text. Sending those parts as separate entities
over the protocol means that SQL injection is no longer possible.&lt;/p&gt;

&lt;!--toc--&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Geolocation with PostgreSQL</title>
      <link>https://tapoueh.org/blog/2018/08/geolocation-with-postgresql/</link>
      <pubDate>Fri, 24 Aug 2018 12:11:33 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2018/08/geolocation-with-postgresql/</guid>
      <description>&lt;p&gt;We have loaded Open Street Map points of interests in the article &lt;a href=&#34;https://tapoueh.org/blog/2013/08/the-most-popular-pub-names/&#34;&gt;The Most
Popular Pub Names&lt;/a&gt; — which
compares PostgreSQL with MongoDB for simple geographical queries, and is
part of our &lt;a href=&#34;https://tapoueh.org/tags/extensions/&#34;&gt;PostgreSQL Extensions&lt;/a&gt; article series. In
today&amp;rsquo;s article, look at how to geolocalize an IP address and locate the
nearest pub, all within a single SQL query!&lt;/p&gt;

&lt;p&gt;For that, we are going to use the awesome
&lt;a href=&#34;https://github.com/RhodiumToad/ip4r&#34;&gt;ip4r&lt;/a&gt; extension from
&lt;a href=&#34;http://blog.rhodiumtoad.org.uk/&#34;&gt;RhodiumToad&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>PostgreSQL Concurrency: an Article Series</title>
      <link>https://tapoueh.org/blog/2018/08/postgresql-concurrency-an-article-series/</link>
      <pubDate>Tue, 14 Aug 2018 11:49:02 +0300</pubDate>
      
      <guid>https://tapoueh.org/blog/2018/08/postgresql-concurrency-an-article-series/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.postgresql.org&#34;&gt;PostgreSQL&lt;/a&gt; is a relational database management
system. It&amp;rsquo;s even the world&amp;rsquo;s most advanced open source one of them. As
such, as its core, Postgres solves concurrent access to a set of data and
maintains consistency while allowing concurrent operations.&lt;/p&gt;

&lt;p&gt;In the &lt;a href=&#34;https://tapoueh.org/tags/concurrency/&#34;&gt;PostgreSQL Concurrency&lt;/a&gt; series of articles here
we did see several aspects of how to handle concurrent use cases of your
application design with PostgreSQL. The main thing to remember is that a
Database Management System first task is to handle concurrency access to the
data for you.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Scheduled Data Processing: How to use cron?</title>
      <link>https://tapoueh.org/blog/2018/08/scheduled-data-processing-how-to-use-cron/</link>
      <pubDate>Wed, 01 Aug 2018 11:24:25 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2018/08/scheduled-data-processing-how-to-use-cron/</guid>
      <description>&lt;p&gt;A previous article in the &lt;a href=&#34;https://tapoueh.org/tags/concurrency&#34;&gt;PostgreSQL Concurrency&lt;/a&gt; series
covered how to manage concurrent retweets in an efficient way: in &lt;a href=&#34;https://tapoueh.org/blog/2018/07/computing-and-caching/&#34;&gt;Computing
and Caching&lt;/a&gt;, we learnt how to
maintain a cache right in your PostgreSQL database, using MATERIALIZED
VIEWS. We also had a look at how to take care of &lt;a href=&#34;https://tapoueh.org/blog/2018/07/batch-updates-and-concurrency/&#34;&gt;Batch Updates and
Concurrency&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;While in the first case we are providing a solution to a technical problem
where we want to solve performance issues while keeping the same semantics,
in the second case we are actually implementing a part of the application&amp;rsquo;s
&lt;a href=&#34;https://tapoueh.org/blog/2017/06/sql-and-business-logic/&#34;&gt;Business Logic&lt;/a&gt; as a scheduled job.&lt;/p&gt;

&lt;p&gt;Today&amp;rsquo;s article shows a modern technique to handle the scheduling of those
business oriented activities that are not tied to any user activity. When
thinking about it this way, you certainly don&amp;rsquo;t want to implement the
backbone of your business logic in a &lt;em&gt;shell script&lt;/em&gt; that&amp;rsquo;s directly
maintained in the production environment, do you?&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Batch Updates and Concurrency</title>
      <link>https://tapoueh.org/blog/2018/07/batch-updates-and-concurrency/</link>
      <pubDate>Mon, 23 Jul 2018 22:45:43 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2018/07/batch-updates-and-concurrency/</guid>
      <description>&lt;p&gt;This article fits in the &lt;a href=&#34;https://tapoueh.org/tags/concurrency&#34;&gt;PostgreSQL Concurrency&lt;/a&gt; series,
where we installed a tweeter like application schema and had all the
characters from Shakespeare&amp;rsquo;s &lt;em&gt;A Midsummer Night&amp;rsquo;s Dream&lt;/em&gt; tweet their own
lines in our database in &lt;a href=&#34;https://tapoueh.org/blog/2018/06/PostgreSQL-DML.md&#34;&gt;PostgreSQL Concurrency: Data Modification
Language&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;A previous article in the series covered how to manage concurrent retweets
in an efficient way: &lt;a href=&#34;https://tapoueh.org/blog/2018/07/computing-and-caching/&#34;&gt;Computing and
Caching&lt;/a&gt;, where we learn how to
maintain a cache right in your PostgreSQL database, thanks for materialized
views. We even went as far as maintaining an &lt;em&gt;external&lt;/em&gt; cache in another
application layer using PostgreSQL
&lt;a href=&#34;https://www.postgresql.org/docs/current/static/sql-listen.html&#34;&gt;LISTEN&lt;/a&gt; and
&lt;a href=&#34;https://www.postgresql.org/docs/current/static/sql-notify.html&#34;&gt;NOTIFY&lt;/a&gt;
features and a Golang application.&lt;/p&gt;

&lt;p&gt;Today&amp;rsquo;s article is going to address concurrency in the context of updating
data in a batch. This activity is quite common, as soon as your system is
connected to other systems either internally or with external providers.
While it&amp;rsquo;s pretty easy to ingest new data, and easy enough to update data
from an external source when nothing happens in your database, doing the
operation safely with concurrent activity is more complex. Once more though,
PostgreSQL comes with all the tooling you need to handle that situation.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>PostgreSQL LISTEN/NOTIFY</title>
      <link>https://tapoueh.org/blog/2018/07/postgresql-listen-notify/</link>
      <pubDate>Thu, 19 Jul 2018 12:58:21 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2018/07/postgresql-listen-notify/</guid>
      <description>&lt;p&gt;This article fits in the &lt;a href=&#34;https://tapoueh.org/tags/concurrency&#34;&gt;PostgreSQL Concurrency&lt;/a&gt; series,
where we installed a tweeter like application schema and had all the
characters from Shakespeare&amp;rsquo;s &lt;em&gt;A Midsummer Night&amp;rsquo;s Dream&lt;/em&gt; tweet their own
lines in our database in &lt;a href=&#34;https://tapoueh.org/blog/2018/06/PostgreSQL-DML.md&#34;&gt;PostgreSQL Concurrency: Data Modification
Language&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;A previous article in the series covered how to manage concurrent retweets
in an efficient way: &lt;a href=&#34;https://tapoueh.org/blog/2018/07/computing-and-caching/&#34;&gt;Computing and
Caching&lt;/a&gt;, where we learn how to
maintain a cache right in your PostgreSQL database, thanks for materialized
views.&lt;/p&gt;

&lt;p&gt;Today&amp;rsquo;s article shows how to maintain an &lt;em&gt;external&lt;/em&gt; cache in another
application layer. In this article we are going to maintain an in-memory
cache in a Golang service, using PostgreSQL
&lt;a href=&#34;https://www.postgresql.org/docs/current/static/sql-listen.html&#34;&gt;LISTEN&lt;/a&gt; and
&lt;a href=&#34;https://www.postgresql.org/docs/current/static/sql-notify.html&#34;&gt;NOTIFY&lt;/a&gt;
features.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>PostgreSQL Event Based Processing</title>
      <link>https://tapoueh.org/blog/2018/07/postgresql-event-based-processing/</link>
      <pubDate>Mon, 16 Jul 2018 09:27:54 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2018/07/postgresql-event-based-processing/</guid>
      <description>&lt;p&gt;In the previous article of the series &lt;a href=&#34;https://tapoueh.org/blog/2018/07/modeling-for-concurrency/&#34;&gt;Modeling for
Concurrency&lt;/a&gt;, we saw how to model
your application for highly concurrent activity. It was a follow-up to the
article entitled &lt;a href=&#34;https://tapoueh.org/blog/2018/07/postgresql-concurrency-isolation-and-locking/&#34;&gt;PostgreSQL Concurrency: Isolation and
Locking&lt;/a&gt;, which
was a primer on PostgreSQL isolation and locking properties and behaviors.&lt;/p&gt;

&lt;p&gt;Today&amp;rsquo;s article takes us a step further and builds on what we did in the
previous articles in our series. After having had all the characters from
Shakespeare&amp;rsquo;s &lt;em&gt;A Midsummer Night&amp;rsquo;s Dream&lt;/em&gt; tweet their own lines in our
database in &lt;a href=&#34;https://tapoueh.org/blog/2018/06/PostgreSQL-DML.md&#34;&gt;PostgreSQL Concurrency: Data Modification
Language&lt;/a&gt;, and having had them like and
retweet a lot in &lt;a href=&#34;https://tapoueh.org/blog/2018/07/postgresql-concurrency-isolation-and-locking/&#34;&gt;PostgreSQL Concurrency: Isolation and
Locking&lt;/a&gt;, we
saw how to manage concurrent retweets in an efficient way in &lt;a href=&#34;https://tapoueh.org/blog/2018/07/computing-and-caching/&#34;&gt;Computing and
Caching&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;What we did implement in the previous article is a &lt;em&gt;cache&lt;/em&gt; system, all with
its necessary &lt;strong&gt;cache invalidation policy&lt;/strong&gt;. Sometimes though, the
processing of an &lt;em&gt;event&lt;/em&gt; needs to happen within the same transaction where
the event is registered in your system. PostgreSQL makes it possible to
maintain a summary table transactionally thanks to its
&lt;a href=&#34;https://www.postgresql.org/docs/current/static/sql-createtrigger.html&#34;&gt;trigger&lt;/a&gt;
support. Today, we&amp;rsquo;re going to dive in how to maintain a summary table with
triggers, and its impact on concurrency.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Computing and Caching</title>
      <link>https://tapoueh.org/blog/2018/07/computing-and-caching/</link>
      <pubDate>Fri, 13 Jul 2018 13:10:21 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2018/07/computing-and-caching/</guid>
      <description>&lt;p&gt;Let&amp;rsquo;s continue to dive in PostgreSQL Concurrency. In the previous article of
the series, &lt;a href=&#34;https://tapoueh.org/blog/2018/07/modeling-for-concurrency/&#34;&gt;Modeling for
Concurrency&lt;/a&gt;, we saw how to model
your application for highly concurrent activity. It was a follow-up to the
article entitled &lt;a href=&#34;https://tapoueh.org/blog/2018/07/postgresql-concurrency-isolation-and-locking/&#34;&gt;PostgreSQL Concurrency: Isolation and
Locking&lt;/a&gt;, which
was a primer on PostgreSQL isolation and locking properties and behaviors.&lt;/p&gt;

&lt;p&gt;Today&amp;rsquo;s article takes us a step further and builds on what we did in the
previous articles in our series. After having had all the characters from
Shakespeare&amp;rsquo;s &lt;em&gt;A Midsummer Night&amp;rsquo;s Dream&lt;/em&gt; tweet their own lines in our
database in &lt;a href=&#34;https://tapoueh.org/blog/2018/06/PostgreSQL-DML.md&#34;&gt;PostgreSQL Concurrency: Data Modification
Language&lt;/a&gt;, and having had them like a
retweet a lot in &lt;a href=&#34;https://tapoueh.org/blog/2018/07/postgresql-concurrency-isolation-and-locking/&#34;&gt;PostgreSQL Concurrency: Isolation and
Locking&lt;/a&gt;, it&amp;rsquo;s
time to think about how to display our counters in an efficient way.&lt;/p&gt;

&lt;p&gt;In this article, we&amp;rsquo;re going to think about when we should compute results
and when we should cache them for instant retrieval, all within the SQL
tooling. The SQL tooling for handling cache is a &lt;a href=&#34;https://www.postgresql.org/docs/current/static/sql-creatematerializedview.html&#34;&gt;MATERIALIZED
VIEW&lt;/a&gt;,
and it comes with &lt;strong&gt;cache invalidation&lt;/strong&gt; routines, of course.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Modeling for Concurrency</title>
      <link>https://tapoueh.org/blog/2018/07/modeling-for-concurrency/</link>
      <pubDate>Tue, 10 Jul 2018 10:26:47 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2018/07/modeling-for-concurrency/</guid>
      <description>&lt;p&gt;Let&amp;rsquo;s continue to dive in PostgreSQL Concurrency. Last week&amp;rsquo;s article
&lt;a href=&#34;https://tapoueh.org/blog/2018/07/postgresql-concurrency-isolation-and-locking/&#34;&gt;PostgreSQL Concurrency: Isolation and
Locking&lt;/a&gt; was a
primer on PostgreSQL isolation and locking properties and behaviors.&lt;/p&gt;

&lt;p&gt;Today&amp;rsquo;s article takes us a step further and builds on what we did last week,
in particular the database modeling for a &lt;em&gt;tweet&lt;/em&gt; like application. After
having had all the characters from Shakespeare&amp;rsquo;s &lt;em&gt;A Midsummer Night&amp;rsquo;s Dream&lt;/em&gt;
tweet their own lines in our database in &lt;a href=&#34;https://tapoueh.org/blog/2018/06/PostgreSQL-DML.md&#34;&gt;PostgreSQL Concurrency: Data
Modification Language&lt;/a&gt;, it&amp;rsquo;s time for them
to do some actions on the tweets: likes and retweet.&lt;/p&gt;

&lt;p&gt;Of course, we&amp;rsquo;re going to put concurrency to the test, so we&amp;rsquo;re going to
have to handle very very popular tweets from the play!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>PostgreSQL Concurrency: Isolation and Locking</title>
      <link>https://tapoueh.org/blog/2018/07/postgresql-concurrency-isolation-and-locking/</link>
      <pubDate>Tue, 03 Jul 2018 13:30:13 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2018/07/postgresql-concurrency-isolation-and-locking/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.postgresql.org&#34;&gt;PostgreSQL&lt;/a&gt; is a relational database management
system. It&amp;rsquo;s even the world&amp;rsquo;s most advanced open source one of them. As
such, as its core, Postgres solves concurrent access to a set of data and
maintains consistency while allowing concurrent operations.&lt;/p&gt;

&lt;p&gt;This article is a primer on PostgreSQL Isolation and Locking properties and
behaviors. You might be interested into the previous article in the series:
&lt;a href=&#34;https://tapoueh.org/blog/2018/06/postgresql-concurrency-data-modification-language/&#34;&gt;PostgreSQL Concurrency: Data Modification
Language&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>PostgreSQL Concurrency: Data Modification Language</title>
      <link>https://tapoueh.org/blog/2018/06/postgresql-concurrency-data-modification-language/</link>
      <pubDate>Mon, 25 Jun 2018 09:58:53 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2018/06/postgresql-concurrency-data-modification-language/</guid>
      <description>PostgreSQL is a relational database management system. It&amp;rsquo;s even the world&amp;rsquo;s most advanced open source one of them. As such, as its core, Postgres solves concurrent access to a set of data and maintains consistency while allowing concurrent operations.
Postgres exposes its concurrency APIs in the SQL language, in particular in the DML parts of it: you can read the Data Manipulation Language chapter of the PostgreSQL docs for all the details.</description>
    </item>
    
    <item>
      <title>PostgreSQL Data Types</title>
      <link>https://tapoueh.org/blog/2018/05/postgresql-data-types/</link>
      <pubDate>Thu, 24 May 2018 14:47:05 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2018/05/postgresql-data-types/</guid>
      <description>&lt;p&gt;Today it&amp;rsquo;s time to conclude our series of &lt;a href=&#34;https://tapoueh.org/tags/data-types/&#34;&gt;PostgreSQL Data
Types&lt;/a&gt; articles with a recap. The series cover lots of
core PostgreSQL data types and shows how to benefit from the PostgreSQL
concept of a data type: more than input validation, a PostgreSQL data type
also implements expected behaviors and processing functions.&lt;/p&gt;

&lt;p&gt;This allows an application developer to rely on PostgreSQL for more complex
queries, having the processing happen where the data is, for instance when
implementing advanced JOIN operations, then retrieving only the data set
that is interesting for the application.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>PostgreSQL Data Types: Point</title>
      <link>https://tapoueh.org/blog/2018/05/postgresql-data-types-point/</link>
      <pubDate>Mon, 07 May 2018 10:46:17 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2018/05/postgresql-data-types-point/</guid>
      <description>&lt;p&gt;Continuing our series of &lt;a href=&#34;https://tapoueh.org/tags/data-types/&#34;&gt;PostgreSQL Data Types&lt;/a&gt; today
we&amp;rsquo;re going to introduce the PostgreSQL Point type.&lt;/p&gt;

&lt;p&gt;In order to put the Point datatype in a context where it makes sense, we&amp;rsquo;re
going to download a complete geolocation data set and normalize it, thus
making good use of both the normalization good practice and those other
PostgreSQL data types we&amp;rsquo;ve been learning about in the previous articles of
this series.&lt;/p&gt;

&lt;p&gt;Buckle-up, this is a long article with a lot of SQL inside.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>PostgreSQL Data Types: ENUM</title>
      <link>https://tapoueh.org/blog/2018/05/postgresql-data-types-enum/</link>
      <pubDate>Wed, 02 May 2018 11:00:26 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2018/05/postgresql-data-types-enum/</guid>
      <description>&lt;p&gt;Continuing our series of &lt;a href=&#34;https://tapoueh.org/tags/data-types/&#34;&gt;PostgreSQL Data Types&lt;/a&gt; today
we&amp;rsquo;re going to introduce the PostgreSQL ENUM type.&lt;/p&gt;

&lt;p&gt;This data type has been added to PostgreSQL in order to make it easier to
support migrations from MySQL. Proper relational design would use a
reference table and a foreign key instead.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>PostgreSQL Data Types: JSON</title>
      <link>https://tapoueh.org/blog/2018/04/postgresql-data-types-json/</link>
      <pubDate>Mon, 30 Apr 2018 09:49:33 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2018/04/postgresql-data-types-json/</guid>
      <description>&lt;p&gt;Continuing our series of &lt;a href=&#34;https://tapoueh.org/tags/data-types/&#34;&gt;PostgreSQL Data Types&lt;/a&gt; today
we&amp;rsquo;re going to introduce the PostgreSQL JSON type.&lt;/p&gt;

&lt;p&gt;PostgreSQL has built-in support for JSON with a great range of processing
functions and operators, and complete indexing support. The documentation
covers all the details in the chapters entitled &lt;a href=&#34;https://www.postgresql.org/docs/current/static/datatype-json.html&#34;&gt;JSON
Types&lt;/a&gt;
and &lt;a href=&#34;https://www.postgresql.org/docs/current/static/functions-json.html&#34;&gt;JSON Functions and
Operators&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>PostgreSQL Data Types: XML</title>
      <link>https://tapoueh.org/blog/2018/04/postgresql-data-types-xml/</link>
      <pubDate>Mon, 23 Apr 2018 18:18:48 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2018/04/postgresql-data-types-xml/</guid>
      <description>&lt;p&gt;Continuing our series of &lt;a href=&#34;https://tapoueh.org/tags/data-types/&#34;&gt;PostgreSQL Data Types&lt;/a&gt; today
we&amp;rsquo;re going to introduce the PostgreSQL XML type.&lt;/p&gt;

&lt;p&gt;The SQL standard includes a &lt;a href=&#34;https://en.wikipedia.org/wiki/SQL/XML&#34;&gt;SQL/XML&lt;/a&gt;
which &lt;em&gt;introduces the predefined data type XML together with constructors,
several routines, functions, and XML-to-SQL data type mappings to support
manipulation and storage of XML in a SQL database&lt;/em&gt;, as per the Wikipedia
page.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>PostgreSQL Data Types: Arrays</title>
      <link>https://tapoueh.org/blog/2018/04/postgresql-data-types-arrays/</link>
      <pubDate>Fri, 20 Apr 2018 14:47:25 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2018/04/postgresql-data-types-arrays/</guid>
      <description>&lt;p&gt;Continuing our series of &lt;a href=&#34;https://tapoueh.org/tags/data-types/&#34;&gt;PostgreSQL Data Types&lt;/a&gt; today
we&amp;rsquo;re going to introduce the PostgreSQL array data types.&lt;/p&gt;

&lt;p&gt;Arrays can be used to denormalize data and avoid lookup tables. A good rule
of thumb for using them that way is that you mostly use the array as a
whole, even if you might at times search for elements in the array. Heavier
processing is going to be more complex than a lookup table.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>PostgreSQL Data Types: Ranges</title>
      <link>https://tapoueh.org/blog/2018/04/postgresql-data-types-ranges/</link>
      <pubDate>Wed, 18 Apr 2018 13:41:12 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2018/04/postgresql-data-types-ranges/</guid>
      <description>&lt;p&gt;Continuing our series of &lt;a href=&#34;https://tapoueh.org/tags/data-types/&#34;&gt;PostgreSQL Data Types&lt;/a&gt; today
we&amp;rsquo;re going to introduce the PostgreSQL ranges data type.&lt;/p&gt;

&lt;p&gt;Range types are a unique feature of PostgreSQL, managing two dimensions of
data in a single column, and allowing advanced processing. The main example
is the &lt;em&gt;daterange&lt;/em&gt; data type, which stores as a single value a lower and an
upper bound of the range as a single value. This allows PostgreSQL to
implement a concurrent safe check against &lt;em&gt;overlapping&lt;/em&gt; ranges, as we&amp;rsquo;re
going to see in this article.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>PostgreSQL Data Types: Network Addresses</title>
      <link>https://tapoueh.org/blog/2018/04/postgresql-data-types-network-addresses/</link>
      <pubDate>Mon, 16 Apr 2018 12:32:53 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2018/04/postgresql-data-types-network-addresses/</guid>
      <description>&lt;p&gt;Continuing our series of &lt;a href=&#34;https://tapoueh.org/tags/data-types/&#34;&gt;PostgreSQL Data Types&lt;/a&gt; today
we&amp;rsquo;re going to introduce network address types.&lt;/p&gt;

&lt;p&gt;PostgreSQL includes support for both &lt;em&gt;cidr&lt;/em&gt;, &lt;em&gt;inet&lt;/em&gt;, and &lt;em&gt;macaddr&lt;/em&gt; data
types. Again, those types are bundled with indexing support and advanced
functions and operator support.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>PostgreSQL Data Types: Date and Time Processing</title>
      <link>https://tapoueh.org/blog/2018/04/postgresql-data-types-date-and-time-processing/</link>
      <pubDate>Fri, 13 Apr 2018 13:35:47 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2018/04/postgresql-data-types-date-and-time-processing/</guid>
      <description>&lt;p&gt;Continuing our series of &lt;a href=&#34;https://tapoueh.org/tags/data-types/&#34;&gt;PostgreSQL Data Types&lt;/a&gt; today
we&amp;rsquo;re going to introduce date and time based processing functions.&lt;/p&gt;

&lt;p&gt;Once the application&amp;rsquo;s data, or rather the user data is properly stored as
timestamp with time zone, PostgreSQL allows implementing all the processing
you need to. In this article we dive into a set of examples to help you get
started with time based processing in your database. Can we boost your
reporting skills?&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>PostgreSQL Data Types: Text Processing</title>
      <link>https://tapoueh.org/blog/2018/04/postgresql-data-types-text-processing/</link>
      <pubDate>Wed, 11 Apr 2018 23:15:42 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2018/04/postgresql-data-types-text-processing/</guid>
      <description>&lt;p&gt;Continuing our series of &lt;a href=&#34;https://tapoueh.org/tags/data-types/&#34;&gt;PostgreSQL Data Types&lt;/a&gt; today
we&amp;rsquo;re going to introduce some of the PostgreSQL text processing functions.&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;s a very rich set of PostgreSQL functions to process text — you can
find them all in the &lt;a href=&#34;https://www.postgresql.org/docs/current/static/functions-string.html&#34;&gt;string functions and
operators&lt;/a&gt;
documentation chapter — with functions such as &lt;em&gt;overlay()&lt;/em&gt;, &lt;em&gt;substring()&lt;/em&gt;,
&lt;em&gt;position()&lt;/em&gt; or &lt;em&gt;trim()&lt;/em&gt;. Or aggregates such as &lt;em&gt;string_agg()&lt;/em&gt;. There are
also &lt;em&gt;regular expression&lt;/em&gt; functions, including the very powerful
&lt;em&gt;regexp_split_to_table()&lt;/em&gt;. In this article we see practical example putting
them in practice.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>PostgreSQL Data Types: Text Encoding</title>
      <link>https://tapoueh.org/blog/2018/04/postgresql-data-types-text-encoding/</link>
      <pubDate>Mon, 09 Apr 2018 13:33:01 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2018/04/postgresql-data-types-text-encoding/</guid>
      <description>&lt;p&gt;Continuing our series of &lt;a href=&#34;https://tapoueh.org/tags/data-types/&#34;&gt;PostgreSQL Data Types&lt;/a&gt; today
we&amp;rsquo;re going to introduce the PostgreSQL text data type. The first notion to
understand when processing text in any program is of course the notion of
encoding.&lt;/p&gt;

&lt;p&gt;So when addressing the text datatype we must mention encoding settings, and
possibly also issues. An encoding is a particular representation of
characters in bits and bytes. In the ASCII encoding the letter &lt;code&gt;A&lt;/code&gt; is
encoded as the 7-bits byte &lt;code&gt;1000001&lt;/code&gt;, or 65 in decimal, or 41 in
hexadecimal. All those numbers are going to be written the same way on-disk,
and the letter &lt;code&gt;A&lt;/code&gt; too.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>PostgreSQL Data Types: an intro</title>
      <link>https://tapoueh.org/blog/2018/04/postgresql-data-types-an-intro/</link>
      <pubDate>Fri, 06 Apr 2018 11:32:43 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2018/04/postgresql-data-types-an-intro/</guid>
      <description>&lt;p&gt;Today, we&amp;rsquo;re going to begin a dive into the PostgreSQL Data Types. As my
colleague &lt;a href=&#34;https://bitfission.com&#34;&gt;Will Leinweber&lt;/a&gt; said recently in his talk
&lt;a href=&#34;https://www.postgresql.eu/events/pgdayparis2018/schedule/session/1835-constraints-a-developers-secret-weapon/&#34;&gt;Constraints: a Developer&amp;rsquo;s Secret
Weapon&lt;/a&gt;
that he gave at &lt;a href=&#34;https://2018.pgday.paris&#34;&gt;pgDay Paris&lt;/a&gt;: &lt;a href=&#34;https://www.citusdata.com/blog/2018/03/19/postgres-database-constraints/&#34;&gt;database
constraints in Postgres are the last line of
defense&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The most important of those constraints is the data type, or the &lt;em&gt;attribute
domain&lt;/em&gt; in normalization slang. By declaring an attribute to be of a certain
data type, then PostgreSQL ensures that this property is always true, and
then implements advanced processing features for each data type, so that you
may push the computation to the data, when needed.&lt;/p&gt;

&lt;p&gt;This article is the first of a series that will go through many of the
PostgreSQL data types, and we open the journey with &lt;code&gt;boolean&lt;/code&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Object Relational Database Management System</title>
      <link>https://tapoueh.org/blog/2018/03/object-relational-database-management-system/</link>
      <pubDate>Thu, 22 Mar 2018 17:40:39 +0100</pubDate>
      
      <guid>https://tapoueh.org/blog/2018/03/object-relational-database-management-system/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.postgresql.org&#34;&gt;PostgreSQL&lt;/a&gt; is the world&amp;rsquo;s most advanced open
source database, and per the &lt;a href=&#34;https://en.wikipedia.org/wiki/PostgreSQL&#34;&gt;PostgreSQL Wikipedia
page&lt;/a&gt; it is &lt;em&gt;an
&lt;strong&gt;object-relational&lt;/strong&gt; database management system (ORDBMS) with an emphasis
on extensibility and standards compliance&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;In this article, we try to understand why would PostgreSQL be named an
&lt;em&gt;object-relational&lt;/em&gt; thing. What is Object Oriented Programming and how does
that apply to a database system?&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Database Normalization and Primary Keys</title>
      <link>https://tapoueh.org/blog/2018/03/database-normalization-and-primary-keys/</link>
      <pubDate>Fri, 09 Mar 2018 18:41:33 +0100</pubDate>
      
      <guid>https://tapoueh.org/blog/2018/03/database-normalization-and-primary-keys/</guid>
      <description>&lt;p&gt;In our previous article we saw three classic &lt;a href=&#34;https://tapoueh.org/blog/2018/03/database-modelization-anti-patterns/&#34;&gt;Database Modelization
Anti-Patterns&lt;/a&gt;. The
article also contains a reference to a Primary Key section of my book
&lt;a href=&#34;https://masteringpostgresql.com&#34;&gt;Mastering PostgreSQL in Application
Development&lt;/a&gt;, so it&amp;rsquo;s only fair that I
would now publish said Primary Key section!&lt;/p&gt;

&lt;p&gt;So in this article, we dive into Primary Keys as being a cornerstone of
database normalization. It&amp;rsquo;s so important to get Primary Keys right that you
would think everybody knows how to do it, and yet, most of the primary key
constraints I&amp;rsquo;ve seen used in database design are actually not primary keys
at all.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Database Modelization Anti-Patterns</title>
      <link>https://tapoueh.org/blog/2018/03/database-modelization-anti-patterns/</link>
      <pubDate>Thu, 08 Mar 2018 18:00:43 +0100</pubDate>
      
      <guid>https://tapoueh.org/blog/2018/03/database-modelization-anti-patterns/</guid>
      <description>&lt;p&gt;Next week we see two awesome PostgreSQL conferences in Europe, back to back,
with a day in between just so that people may attend both! In chronological
order we have first &lt;a href=&#34;https://2018.nordicpgday.org&#34;&gt;Nordic pgDay&lt;/a&gt; in Oslo
where I will have the pleasure to talk about &lt;a href=&#34;https://www.postgresql.eu/events/nordicpgday2018/schedule/session/1896-data-modeling-normalization-and-denormalization/&#34;&gt;Data Modeling, Normalization
and
Denormalization&lt;/a&gt;.
Then we have &lt;a href=&#34;https://2018.pgday.paris&#34;&gt;pgday.paris&lt;/a&gt; with an awesome
schedule and a strong focus on the needs of application developers!&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Sustainable Open Source Development</title>
      <link>https://tapoueh.org/blog/2018/02/sustainable-open-source-development/</link>
      <pubDate>Tue, 20 Feb 2018 21:08:38 +0100</pubDate>
      
      <guid>https://tapoueh.org/blog/2018/02/sustainable-open-source-development/</guid>
      <description>&lt;p&gt;Current trend in software deployments is to rely on open source software for
entire production stacks. You can find open source software in the core
technical stacks of every startup out there, I&amp;rsquo;m told. If you&amp;rsquo;re using Cloud
based offerings, most of Cloud providers are running &lt;a href=&#34;https://www.gnu.org/philosophy/floss-and-foss.en.html&#34;&gt;Free/Libre Open Source
Software&lt;/a&gt; as their
foundation.&lt;/p&gt;

&lt;p&gt;This article is a deep dive into the economic models behind successful open
source projects and communities, and how as a professional, enterprise grade
user, you depend on the long-term sustainability of all the open source
projects you&amp;rsquo;re using. And because you depend on the projects you&amp;rsquo;re using
to be successful, how to contribute and guarantee their success.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Find the number of the longest continuously rising days for a stock</title>
      <link>https://tapoueh.org/blog/2018/02/find-the-number-of-the-longest-continuously-rising-days-for-a-stock/</link>
      <pubDate>Tue, 06 Feb 2018 23:24:17 +0100</pubDate>
      
      <guid>https://tapoueh.org/blog/2018/02/find-the-number-of-the-longest-continuously-rising-days-for-a-stock/</guid>
      <description>&lt;p&gt;Today I want to react to an article that claims that &lt;a href=&#34;https://www.datasciencecentral.com/profiles/blogs/relational-algebra-is-the-root-of-sql-problems&#34;&gt;Relational Algebra Is
the Root of SQL
Problems&lt;/a&gt;
in which the author hand-waves the following position:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;SQL becomes more a hindrance to data manipulation than an efficient tool.
SQL’s greatest problem isn’t in the implementation level, but at its
theory foundation. The problem can’t be solved by application
optimization. Relational algebra isn’t sophisticated enough for handling
the complicated data manipulation scenarios.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Then they go on to several &lt;em&gt;arguments from authority&lt;/em&gt; to “prove” their
point. My reading of the article is that SQL is very hard when you didn&amp;rsquo;t
care to learn it, as most technologies are.&lt;/p&gt;

&lt;p&gt;In this article, we&amp;rsquo;re going to look at the &lt;em&gt;simple examples&lt;/em&gt; provided where
apparently SQL makes it so much harder to find a solution compared to
writing some Java or C++ code. Contrary to the original article, we go as
far as to actually writing both the SQL solution and a complete Python
solution, so that we can compare.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Exporting a Hierarchy in JSON: with recursive queries</title>
      <link>https://tapoueh.org/blog/2018/01/exporting-a-hierarchy-in-json-with-recursive-queries/</link>
      <pubDate>Wed, 31 Jan 2018 18:00:01 +0100</pubDate>
      
      <guid>https://tapoueh.org/blog/2018/01/exporting-a-hierarchy-in-json-with-recursive-queries/</guid>
      <description>&lt;p&gt;In another article here, entitled &lt;a href=&#34;https://tapoueh.org/blog/2017/09/on-json-and-sql/&#34;&gt;on JSON and
SQL&lt;/a&gt;, we saw in great details how to import
a data set only available as a giant JSON file. Then we normalized the data
set, so as to be able to write SQL and process our data. This approach is
sometimes very useful and was a good way to learn some of the JSON functions
provided by PostgreSQL.&lt;/p&gt;

&lt;p&gt;In this article, we&amp;rsquo;re going to use SQL to export the data from our
relational model into a JSON document. The trick that makes it complex in
this example is that we have a recursive data model, with a notion of a
&lt;em&gt;parent&lt;/em&gt; row that exists in the same table as the current one. That&amp;rsquo;s a nice
excuse to learn more about the SQL construct &lt;a href=&#34;https://www.postgresql.org/docs/current/static/queries-with.html&#34;&gt;WITH
RECURSIVE&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>A Year in Review: Most Read Articles in 2017</title>
      <link>https://tapoueh.org/blog/2018/01/a-year-in-review-most-read-articles-in-2017/</link>
      <pubDate>Sun, 28 Jan 2018 22:46:43 +0100</pubDate>
      
      <guid>https://tapoueh.org/blog/2018/01/a-year-in-review-most-read-articles-in-2017/</guid>
      <description>&lt;p&gt;It seems to be usual nowadays to review the previous year, and readers
apparently like Top-N Lists — &lt;em&gt;that&amp;rsquo;s you now, so let&amp;rsquo;s hope that my
understanding works with you too&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Of course 2018 will see its own amount of new and original content added to
this blog, with a continuous focus towards how to make the best out of the
SQL powerful programming language, and its advanced concurrency semantics.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Working at Citus Data</title>
      <link>https://tapoueh.org/blog/2018/01/working-at-citus-data/</link>
      <pubDate>Thu, 18 Jan 2018 12:05:13 +0100</pubDate>
      
      <guid>https://tapoueh.org/blog/2018/01/working-at-citus-data/</guid>
      <description>&lt;p&gt;You might have read it in the news already in Citus&amp;rsquo; blog post by &lt;a href=&#34;https://www.citusdata.com/about/team/&#34;&gt;Sumedh
Pathak&lt;/a&gt;: &lt;a href=&#34;https://www.citusdata.com/blog/2018/01/12/dimitri-fontaine-postgresql-contributor-joins-citus-data/&#34;&gt;PostgreSQL Expert Dimitri
Fontaine joins Citus
Data&lt;/a&gt;.
I am very happy to join a talented team here at Citus, and excited to work
on an Open Source solution for distributed SQL on-top of PostgreSQL! In this
article I&amp;rsquo;m going to cover my first technical contributions to Citus
database, as it happens that a few patches of mine made it to the main
source tree already.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt; It&amp;rsquo;s good to be working on PostgreSQL related source code again,
and to have the opportunity to solve PostgreSQL related problems at scale!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Migrating to PostgreSQL, the White Paper</title>
      <link>https://tapoueh.org/blog/2018/01/migrating-to-postgresql-the-white-paper/</link>
      <pubDate>Thu, 11 Jan 2018 10:01:45 +0100</pubDate>
      
      <guid>https://tapoueh.org/blog/2018/01/migrating-to-postgresql-the-white-paper/</guid>
      <description>&lt;p&gt;After having been involved in many migration projects over the last 10
years, I decided to publish the following &lt;a href=&#34;https://pgloader.io/white-paper&#34;&gt;White
Paper&lt;/a&gt; in order to share my learnings.&lt;/p&gt;

&lt;p&gt;The paper is titled &lt;a href=&#34;https://pgloader.io/white-paper&#34;&gt;Migrating to PostgreSQL, Tools and
Methodology&lt;/a&gt; and details the &lt;a href=&#34;https://pgloader.io/blog/continuous-migration/&#34;&gt;Continuous
Migration&lt;/a&gt; approach. It
describes how to migrate from another relational database server technology
to PostgreSQL. The reasons to do so are many, and first among them is often
the licensing model.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Submit a Talk to pgDay Paris Today!</title>
      <link>https://tapoueh.org/blog/2017/12/submit-a-talk-to-pgday-paris-today/</link>
      <pubDate>Tue, 26 Dec 2017 16:06:34 +0100</pubDate>
      
      <guid>https://tapoueh.org/blog/2017/12/submit-a-talk-to-pgday-paris-today/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://2018.pgday.paris&#34;&gt;pgDay Paris&lt;/a&gt; 4th edition happens March 15, 2018!
And the &lt;a href=&#34;https://2018.pgday.paris/callforpapers/&#34;&gt;Call for Papers&lt;/a&gt; is still
open, as it closes with the year: &lt;em&gt;the submission deadline is December 31st,
2017&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;As I&amp;rsquo;ve been organizing the first edition of &lt;em&gt;pgDay Paris&lt;/em&gt; back in 2015,
this very conference is special to me! It&amp;rsquo;s been an amazing experience
launching a new &lt;a href=&#34;https://www.postgresql.org&#34;&gt;PostgreSQL&lt;/a&gt; conference…&lt;/p&gt;

&lt;p&gt;Hats off to &lt;a href=&#34;https://www.hagander.net&#34;&gt;Magnus&lt;/a&gt; without whom the first
edition would never have happened, and to
&lt;a href=&#34;https://twitter.com/pg_xocolatl&#34;&gt;Vik&lt;/a&gt; of course, who took over the
organisation of the conference!&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Setting up psql, the PostgreSQL CLI</title>
      <link>https://tapoueh.org/blog/2017/12/setting-up-psql-the-postgresql-cli/</link>
      <pubDate>Fri, 22 Dec 2017 15:23:43 +0100</pubDate>
      
      <guid>https://tapoueh.org/blog/2017/12/setting-up-psql-the-postgresql-cli/</guid>
      <description>&lt;p&gt;PostgreSQL ships with an interactive console with the command line tool
named &lt;a href=&#34;https://www.postgresql.org/docs/current/static/app-psql.html&#34;&gt;psql&lt;/a&gt;.
It can be used both for scripting and interactive usage and is moreover
quite a powerful tool. Interactive features includes &lt;em&gt;autocompletion&lt;/em&gt;,
&lt;em&gt;readline&lt;/em&gt; support (history searches, modern keyboard movements, etc), input
and output redirection, formatted output, and more.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Mastering PostgreSQL: a reader&#39;s interview</title>
      <link>https://tapoueh.org/blog/2017/12/mastering-postgresql-a-readers-interview/</link>
      <pubDate>Wed, 13 Dec 2017 17:05:56 +0100</pubDate>
      
      <guid>https://tapoueh.org/blog/2017/12/mastering-postgresql-a-readers-interview/</guid>
      <description>&lt;p&gt;&lt;strong&gt;&lt;em&gt;Florent Fourcot&lt;/em&gt;&lt;/strong&gt; has read &lt;a href=&#34;http://masteringpostgresql.com&#34;&gt;Mastering PostgreSQL in Application
Development&lt;/a&gt; and has seen tremendous
inprovements in his production setup from reading the first chapters and
applying the book advices to his use case.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s an interview run with Florent where he explains the context in which
such improvements has been made!&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Mastering PostgreSQL: more about the docker image</title>
      <link>https://tapoueh.org/blog/2017/12/mastering-postgresql-more-about-the-docker-image/</link>
      <pubDate>Tue, 12 Dec 2017 23:27:55 +0100</pubDate>
      
      <guid>https://tapoueh.org/blog/2017/12/mastering-postgresql-more-about-the-docker-image/</guid>
      <description>&lt;p&gt;The Enterprise Edition of &lt;a href=&#34;https://masteringpostgresql.com&#34;&gt;Mastering PostgreSQL in Application
Development&lt;/a&gt; ships with a docker image that
hosts both a PostgreSQL server instance with a pre-loaded database, the one
that&amp;rsquo;s used throughout the book examples, and also with a Jupyter Network
notebook that hosts SQL queries thanks to the
&lt;a href=&#34;https://github.com/pivotal/sql_magic&#34;&gt;sql_magic&lt;/a&gt; plugin.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Queen, Princesses, and Workers</title>
      <link>https://tapoueh.org/blog/2017/12/queen-princesses-and-workers/</link>
      <pubDate>Mon, 11 Dec 2017 10:19:24 +0100</pubDate>
      
      <guid>https://tapoueh.org/blog/2017/12/queen-princesses-and-workers/</guid>
      <description>&lt;p&gt;The &lt;a href=&#34;https://postgresql.org&#34;&gt;PostgreSQL&lt;/a&gt; community made the explicit choice
some times ago that they would not use the infamous &lt;em&gt;master&lt;/em&gt; and &lt;em&gt;slave&lt;/em&gt;
terminology. Instead, the documentation introduces the concepts of &lt;a href=&#34;https://www.postgresql.org/docs/current/static/high-availability.html&#34;&gt;High
Availability, Load Balancing, and
Replication&lt;/a&gt;
with the terms &lt;em&gt;Primary&lt;/em&gt; and &lt;em&gt;Standby&lt;/em&gt;, and the even more generic term
&lt;em&gt;Replica&lt;/em&gt; is used in contexts when only the data flow is considered, rather
than the particular role of a node.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Scaling Python released</title>
      <link>https://tapoueh.org/blog/2017/12/scaling-python-released/</link>
      <pubDate>Tue, 05 Dec 2017 18:10:00 +0100</pubDate>
      
      <guid>https://tapoueh.org/blog/2017/12/scaling-python-released/</guid>
      <description>&lt;p&gt;Today I am very pleased to announce the release of the book &lt;a href=&#34;https://scaling-python.com&#34;&gt;Scaling
Python&lt;/a&gt; from my good friend &lt;a href=&#34;https://julien.danjou.info/&#34;&gt;Julien
Danjou&lt;/a&gt;!&lt;/p&gt;

&lt;p&gt;As Julien says, &lt;strong&gt;Python applications can handle millions of requests.&lt;/strong&gt;
Well, we know here that it&amp;rsquo;s easier on them when they are using PostgreSQL
of course!&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Simple Data Modeling with a Test Data Set</title>
      <link>https://tapoueh.org/blog/2017/11/simple-data-modeling-with-a-test-data-set/</link>
      <pubDate>Mon, 27 Nov 2017 16:23:44 +0100</pubDate>
      
      <guid>https://tapoueh.org/blog/2017/11/simple-data-modeling-with-a-test-data-set/</guid>
      <description>&lt;p&gt;In &lt;a href=&#34;https://tapoueh.org/blog/2017/06/how-to-write-sql/&#34;&gt;How to Write SQL&lt;/a&gt; we saw how to write
SQL queries as separate &lt;code&gt;.sql&lt;/code&gt; files, and we learnt about using query
parameters with the &lt;em&gt;psql&lt;/em&gt; syntax for that (&lt;code&gt;:variable&lt;/code&gt;, &lt;code&gt;:&#39;variable&#39;&lt;/code&gt;, and
&lt;code&gt;:&amp;quot;identifier&amp;quot;&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;For writing our database model, the same tooling is all we need. An
important aspect of using &lt;em&gt;psql&lt;/em&gt; is its capacity to provide immediate
feedback, and we can also have that with modeling too.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>The Mode Ordered-Set Aggregate Function</title>
      <link>https://tapoueh.org/blog/2017/11/the-mode-ordered-set-aggregate-function/</link>
      <pubDate>Mon, 13 Nov 2017 18:15:51 +0100</pubDate>
      
      <guid>https://tapoueh.org/blog/2017/11/the-mode-ordered-set-aggregate-function/</guid>
      <description>&lt;p&gt;In our article &lt;a href=&#34;https://tapoueh.org/blog/2017/06/exploring-a-data-set-in-sql/&#34;&gt;Exploring a Data Set in
SQL&lt;/a&gt; we discovered a data set
related to music: the &lt;a href=&#34;https://github.com/lerocha/chinook-database&#34;&gt;Chinook&lt;/a&gt;
sample database.&lt;/p&gt;

&lt;p&gt;Our discovery led us to find albums containing tracks of multiple genres,
and for the analytics we were then pursuing, we wanted to &lt;em&gt;clean&lt;/em&gt; the data
set and assign a single genre per album. We did that in SQL of course, and
didn&amp;rsquo;t actually edit the data.&lt;/p&gt;

&lt;p&gt;Finding the most frequent input value in a group is a job for the &lt;code&gt;mode()
WITHIN GROUP (ORDER BY sort_expression)&lt;/code&gt; Ordered-Set Aggregate Function, as
documented in the PostgreSQL page about &lt;a href=&#34;https://www.postgresql.org/docs/current/static/functions-aggregate.html#FUNCTIONS-ORDEREDSET-TABLE&#34;&gt;Aggregate
Functions&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>What&#39;s in a name: “Mastering”</title>
      <link>https://tapoueh.org/blog/2017/11/whats-in-a-name-mastering/</link>
      <pubDate>Thu, 09 Nov 2017 09:34:36 +0100</pubDate>
      
      <guid>https://tapoueh.org/blog/2017/11/whats-in-a-name-mastering/</guid>
      <description>&lt;p&gt;Now that my book &lt;a href=&#34;https://masteringpostgresql.com&#34;&gt;Mastering PostgreSQL in Application
Development&lt;/a&gt; is released (and selling well,
thanks guys!), I&amp;rsquo;ve had some questions about the title.&lt;/p&gt;

&lt;p&gt;The idea is that to become good at anything, we need to practice. We
practice a lot, and it&amp;rsquo;s even better when we are actively trying to learn,
following what&amp;rsquo;s named &lt;a href=&#34;https://en.wikipedia.org/wiki/Practice_(learning_method)#Deliberate_practice&#34;&gt;deliberate
practice&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Mastering PostgreSQL in Application Development launches!</title>
      <link>https://tapoueh.org/blog/2017/11/mastering-postgresql-in-application-development-launches/</link>
      <pubDate>Mon, 06 Nov 2017 10:48:43 +0100</pubDate>
      
      <guid>https://tapoueh.org/blog/2017/11/mastering-postgresql-in-application-development-launches/</guid>
      <description>Today is the day my book Mastering PostgreSQL in Application Development launches! I&amp;rsquo;m all excited that everybody interested is now able to actually read my book!
Mastering PostgreSQL in Application Development targets application developers who want to learn SQL properly, and actually master this programming language. Most developers don&amp;rsquo;t think of SQL as a programming language, mainly because they don&amp;rsquo;t have full control of the execution plan of their queries.</description>
    </item>
    
    <item>
      <title>Set Returning Functions and PostgreSQL 10</title>
      <link>https://tapoueh.org/blog/2017/10/set-returning-functions-and-postgresql-10/</link>
      <pubDate>Fri, 13 Oct 2017 13:25:21 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2017/10/set-returning-functions-and-postgresql-10/</guid>
      <description>&lt;p&gt;PostgreSQL 10 is now available for everyone to use, and hinted by &lt;a href=&#34;http://fetter.org&#34;&gt;David
Fetter&lt;/a&gt; I had to review my previous article &lt;a href=&#34;https://tapoueh.org/blog/2017/09/on-json-and-sql/&#34;&gt;on Json and
SQL&lt;/a&gt; to adapt to &lt;em&gt;Set Returning Functions&lt;/em&gt;
changes.&lt;/p&gt;

&lt;p&gt;A &lt;em&gt;Set Returning Function&lt;/em&gt; is a PostgreSQL &lt;em&gt;Stored Procedure&lt;/em&gt; that can be
used as a relation: from a single call it returns an entire result set, much
like a subquery or a table.&lt;/p&gt;

&lt;p&gt;It used to be possible to use &lt;em&gt;SRF&lt;/em&gt; in the &lt;em&gt;SELECT&lt;/em&gt; clause, with dubious
(but useful at times) semantics, and also in &lt;em&gt;scalar&lt;/em&gt; contexts. The
semantics have been fixed and are now much clearer, and the uses in scalar
contexts are forbidden — they were a hack and never made sense anyway.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>on Json and SQL</title>
      <link>https://tapoueh.org/blog/2017/09/on-json-and-sql/</link>
      <pubDate>Mon, 18 Sep 2017 10:49:03 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2017/09/on-json-and-sql/</guid>
      <description>&lt;p&gt;PostgreSQL has had proper
&lt;a href=&#34;https://www.postgresql.org/docs/current/static/datatype-json.html&#34;&gt;json&lt;/a&gt;
support for a while now. The unique extensibility approach of the PostgreSQL
system allows it to enable native &lt;a href=&#34;https://www.postgresql.org/docs/current/static/functions-json.html&#34;&gt;SQL friendly JSON
processing&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In this article we&amp;rsquo;ll play with the &lt;a href=&#34;https://mtgjson.com&#34;&gt;Magic: the Gathering card data in JSON
format&lt;/a&gt; data set, provided with a
&lt;a href=&#34;https://creativecommons.org/choose/zero/&#34;&gt;CC0&lt;/a&gt; licence, and process the
information provided. We also see how to normalize a JSON document into a
proper database model that benefits from some PostgreSQL advanced features,
and how to then inject the JSON documents into the normalized database
schema. Finally, we compare some non-trivial processing done against both
versions of the database schema.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Mastering PostgreSQL in Application Development</title>
      <link>https://tapoueh.org/blog/2017/09/mastering-postgresql-in-application-development/</link>
      <pubDate>Mon, 11 Sep 2017 15:22:23 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2017/09/mastering-postgresql-in-application-development/</guid>
      <description>Mastering PostgreSQL in Application Development is the full title of the book I am currently writing. Running the PostgreSQL is YeSQL series of blog posts has shown me developers need a PostgreSQL book for developers. A book with the same properties as the YeSQL series articles in this blog:
 we use real world data sets to put every query and SQL technique we learn in the context of a user story or business case,</description>
    </item>
    
    <item>
      <title>Regular Expressions and Grouping Sets</title>
      <link>https://tapoueh.org/blog/2017/08/regular-expressions-and-grouping-sets/</link>
      <pubDate>Mon, 14 Aug 2017 16:37:53 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2017/08/regular-expressions-and-grouping-sets/</guid>
      <description>&lt;p&gt;There&amp;rsquo;s a very rich set of PostgreSQL functions to process text, you can
find them all at
the
&lt;a href=&#34;https://www.postgresql.org/docs/current/static/functions-string.html&#34;&gt;String Functions and Operators&lt;/a&gt; documentation
chapter, with functions such as &lt;em&gt;overlay&lt;/em&gt;, &lt;em&gt;substring&lt;/em&gt;, &lt;em&gt;position&lt;/em&gt; or
&lt;em&gt;trim&lt;/em&gt;. Or aggregates such as &lt;em&gt;string_agg&lt;/em&gt;. And then &lt;em&gt;regular expression&lt;/em&gt;
functions, including the very powerful &lt;em&gt;regexp_split_to_table&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>SQL Regression Tests</title>
      <link>https://tapoueh.org/blog/2017/08/sql-regression-tests/</link>
      <pubDate>Tue, 08 Aug 2017 17:55:51 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2017/08/sql-regression-tests/</guid>
      <description>&lt;p&gt;In a previous article here we
saw &lt;a href=&#34;https://tapoueh.org/blog/2017/06/how-to-write-sql/&#34;&gt;How to Write SQL&lt;/a&gt; in your application
code. The main idea in that article is to maintain your queries in separate
SQL files, where it&amp;rsquo;s easier to maintain them. In particular if you want to
be able to test them again in production, and when you have to work and
rewrite queries.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>from MySQL to PostgreSQL</title>
      <link>https://tapoueh.org/blog/2017/07/from-mysql-to-postgresql/</link>
      <pubDate>Thu, 06 Jul 2017 17:30:11 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2017/07/from-mysql-to-postgresql/</guid>
      <description>&lt;p&gt;Today
&lt;a href=&#34;https://github.com/dimitri/pgloader/releases/tag/v3.4.1&#34;&gt;pgloader v3.4.1&lt;/a&gt;
is released and available! This new release comes with 110 commits as show
in
&lt;a href=&#34;https://github.com/dimitri/pgloader/compare/v3.3.2...v3.4.1&#34;&gt;github compare view&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This release of &lt;a href=&#34;http://pgloader.io&#34;&gt;pgloader&lt;/a&gt; is following the tradition of
simplifying things for users, or if you allow me to
quote &lt;a href=&#34;https://en.wikiquote.org/wiki/Alan_Kay&#34;&gt;Alan Kay&lt;/a&gt;, I believe that if
&lt;em&gt;simple things should be simple, complex things should be possible.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Playing with Unicode</title>
      <link>https://tapoueh.org/blog/2017/07/playing-with-unicode/</link>
      <pubDate>Mon, 03 Jul 2017 14:32:29 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2017/07/playing-with-unicode/</guid>
      <description>&lt;p&gt;The reason why I like Unicode a lot is because it allows me to code in text
based environments and still have nice output. Today, we&amp;rsquo;re going to play
with
&lt;a href=&#34;https://en.wikipedia.org/wiki/Regional_Indicator_Symbol&#34;&gt;Regional Indicator Symbol&lt;/a&gt;,
which is implemented as a Unicode combinaison of letters from 🇦 to 🇿. For
instance, if you display 🇫 then 🇷 concatenated together, you get 🇫🇷. Let&amp;rsquo;s
try that from our &lt;a href=&#34;https://www.postgresql.org/&#34;&gt;PostgreSQL&lt;/a&gt; prompt!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>PostgreSQL and the calendar</title>
      <link>https://tapoueh.org/blog/2017/06/postgresql-and-the-calendar/</link>
      <pubDate>Fri, 30 Jun 2017 14:35:59 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2017/06/postgresql-and-the-calendar/</guid>
      <description>&lt;p&gt;The modern calendar is a trap for the young engineer&amp;rsquo;s mind. We deal with
the calendar on a daily basis and until exposed to its insanity it&amp;rsquo;s rather
common to think that calendar based computations are easy. That&amp;rsquo;s until
you&amp;rsquo;ve tried to do it once. A very good read about how the current calendar
came to be the way it is now is Erik&amp;rsquo;s
Naggum &lt;a href=&#34;http://naggum.no/lugm-time.html&#34;&gt;The Long, Painful History of Time&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>SQL and Business Logic</title>
      <link>https://tapoueh.org/blog/2017/06/sql-and-business-logic/</link>
      <pubDate>Mon, 19 Jun 2017 13:30:19 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2017/06/sql-and-business-logic/</guid>
      <description>&lt;p&gt;Business logic is &lt;em&gt;supposed to be&lt;/em&gt; the part of the application where you
deal with customer or user facing decisions and computations. It is often
argued that this part should be well separated from the rest of the
technical infrastructure of your code. Of course, SQL and relational
database design is meant to support your business cases (or user stories),
so then we can ask ourselves if SQL should be part of your business logic
implementation. Or actually, how much of your business logic should be SQL?&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Exploring a Data Set in SQL</title>
      <link>https://tapoueh.org/blog/2017/06/exploring-a-data-set-in-sql/</link>
      <pubDate>Tue, 13 Jun 2017 13:47:08 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2017/06/exploring-a-data-set-in-sql/</guid>
      <description>&lt;p&gt;Sometimes you need to dive in an existing data set that you know very little
about. Let&amp;rsquo;s say we&amp;rsquo;ve been lucky to have had a high level description of
the business case covered by a database, and then access to it. Our next
step is figuring out data organisation, content and quality. Our tool box:
&lt;em&gt;the world&amp;rsquo;s most advanced open source
database&lt;/em&gt;, &lt;a href=&#34;https://www.postgresql.org&#34;&gt;PostgreSQL&lt;/a&gt;, and its &lt;em&gt;Structured
Query Language&lt;/em&gt;, SQL.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>How to Write SQL</title>
      <link>https://tapoueh.org/blog/2017/06/how-to-write-sql/</link>
      <pubDate>Thu, 08 Jun 2017 13:23:26 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2017/06/how-to-write-sql/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://twitter.com/krisajenkins&#34;&gt;Kris Jenkins&lt;/a&gt; cooked up a very nice way
to embed SQL in your
code: &lt;a href=&#34;https://github.com/krisajenkins/yesql&#34;&gt;YeSQL for Clojure&lt;/a&gt;. The main
idea is that you should be writing your SQL queries in &lt;code&gt;.sql&lt;/code&gt; files in your
code repository and maintain them there.&lt;/p&gt;

&lt;p&gt;The idea is very good and it is now possible to find alternative
implementations of the &lt;a href=&#34;https://clojure.org&#34;&gt;Clojure&lt;/a&gt; &lt;em&gt;yesql&lt;/em&gt; library in
other languages. Today, we are going to have a look at one of them for
the &lt;a href=&#34;https://www.python.org&#34;&gt;python&lt;/a&gt; programming
language: &lt;a href=&#34;https://github.com/honza/anosql&#34;&gt;anosql&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Find The Missing Integer</title>
      <link>https://tapoueh.org/blog/2017/05/find-the-missing-integer/</link>
      <pubDate>Tue, 30 May 2017 19:56:54 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2017/05/find-the-missing-integer/</guid>
      <description>&lt;p&gt;A recent interview question that I had to review was spelled like this:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Find missing int element into array 1..100&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Of course at first read I got it wrong, you have only one integer to look
for into the array. So while the obvious idea was to apply classic &lt;em&gt;sorting&lt;/em&gt;
techniques and minimize array traversal to handle complexity (time and
space), it turns out there&amp;rsquo;s a much simpler way to do it if you remember
your math lessons from younger. But is it that much simpler?&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Meetup PostgreSQL à Paris</title>
      <link>https://tapoueh.org/blog/2014/10/meetup-postgresql-%C3%A0-paris/</link>
      <pubDate>Thu, 02 Oct 2014 11:49:00 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2014/10/meetup-postgresql-%C3%A0-paris/</guid>
      <description>&lt;p&gt;Mercredi 8 octobre se tiendra le prochain
&lt;a href=&#34;http://www.meetup.com/PostgreSQL-User-Group-Paris/events/209650432/&#34;&gt;Meetup PostgreSQL à Paris&lt;/a&gt; dans les
locaux de
&lt;a href=&#34;https://www.mozilla.org/en-US/contact/spaces/paris/&#34;&gt;Mozilla Europe&lt;/a&gt;, dont la capacité est de 90 personnes ! Venez
nombreux !&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>PostgreSQL, Aggregates and Histograms</title>
      <link>https://tapoueh.org/blog/2014/02/postgresql-aggregates-and-histograms/</link>
      <pubDate>Fri, 21 Feb 2014 13:25:00 +0100</pubDate>
      
      <guid>https://tapoueh.org/blog/2014/02/postgresql-aggregates-and-histograms/</guid>
      <description>&lt;p&gt;In our previous article
&lt;a href=&#34;https://tapoueh.org/blog/2014/02/17-aggregating-nba-data-PostgreSQL-vs-MongoDB&#34;&gt;Aggregating NBA data, PostgreSQL vs MongoDB&lt;/a&gt; we spent
time comparing the pretty new
&lt;em&gt;MongoDB Aggregation Framework&lt;/em&gt; with the decades
old SQL aggregates. Today, let&amp;rsquo;s showcase more of those SQL aggregates,
producing a nice
&lt;em&gt;histogram&lt;/em&gt; right from our SQL console.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Aggregating NBA data, PostgreSQL vs MongoDB</title>
      <link>https://tapoueh.org/blog/2014/02/aggregating-nba-data-postgresql-vs-mongodb/</link>
      <pubDate>Mon, 17 Feb 2014 23:40:00 +0100</pubDate>
      
      <guid>https://tapoueh.org/blog/2014/02/aggregating-nba-data-postgresql-vs-mongodb/</guid>
      <description>&lt;p&gt;When reading the article
&lt;a href=&#34;http://thecodebarbarian.wordpress.com/2014/02/14/crunching-30-years-of-nba-data-with-mongodb-aggregation/&#34;&gt;Crunching 30 Years of NBA Data with MongoDB Aggregation&lt;/a&gt; I coulnd&amp;rsquo;t help but
think that we&amp;rsquo;ve been enjoying
&lt;a href=&#34;http://www.postgresql.org/docs/current/static/tutorial-agg.html&#34;&gt;aggregates&lt;/a&gt; in SQL for 3 or 4 decades already.
When using
&lt;a href=&#34;http://www.postgresql.org/&#34;&gt;PostgreSQL&lt;/a&gt; it&amp;rsquo;s even easy to actually add your own aggregates
given the SQL command
&lt;a href=&#34;http://www.postgresql.org/docs/current/static/sql-createaggregate.html&#34;&gt;create aggregate&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Denormalizing Tags</title>
      <link>https://tapoueh.org/blog/2013/10/denormalizing-tags/</link>
      <pubDate>Thu, 24 Oct 2013 13:40:00 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2013/10/denormalizing-tags/</guid>
      <description>In our Tour of Extensions today&amp;rsquo;s article is about advanced tag indexing. We have a great data collection to play with and our goal today is to be able to quickly find data matching a complex set of tags. So, let&amp;rsquo;s find out those lastfm tracks that are tagged as blues and rhythm and blues, for instance.
 We&amp;rsquo;re going to use the Last.fm dataset from the Million Song Dataset project here.</description>
    </item>
    
    <item>
      <title>An Interview about MariaDB and PostgreSQL</title>
      <link>https://tapoueh.org/blog/2013/10/an-interview-about-mariadb-and-postgresql/</link>
      <pubDate>Wed, 16 Oct 2013 21:07:00 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2013/10/an-interview-about-mariadb-and-postgresql/</guid>
      <description>At the Open World Forum two weeks ago I had the pleasure to meet with Colin Charles. We had a nice talk about the current state of both MariaDB and PostgreSQL, and even were both interviewed by the Open World Forum Team. The interview is now available online. Dear French readers, it&amp;rsquo;s in English.
Here&amp;rsquo;s the video:
  Executive Summary: MariaDB is a drop-in fully Open Source replacement for MySQL and sees quite some progress and innovation being made, and PostgreSQL is YeSQL!</description>
    </item>
    
    <item>
      <title>PostgreSQL Autonomous Transaction</title>
      <link>https://tapoueh.org/blog/2013/10/postgresql-autonomous-transaction/</link>
      <pubDate>Mon, 14 Oct 2013 11:25:00 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2013/10/postgresql-autonomous-transaction/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://www.postgresql.org/&#34;&gt;PostgreSQL&lt;/a&gt; is an all round impressive
&lt;em&gt;Relational DataBase Management System&lt;/em&gt;
which implements the SQL standard (see the very useful reference page
&lt;a href=&#34;http://troels.arvin.dk/db/rdbms/&#34;&gt;Comparison of different SQL implementations&lt;/a&gt; for details). PostgreSQL also
provides with unique solutions in the database market and has been leading
innovation for some years now. Still, there&amp;rsquo;s no support for
&lt;strong&gt;&lt;em&gt;Autonomous
Transactions&lt;/em&gt;&lt;/strong&gt; within the server itself. Let&amp;rsquo;s have a look at how to easily
implement them with
&lt;a href=&#34;http://wiki.postgresql.org/wiki/PL/Proxy&#34;&gt;PL/Proxy&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>PostgreSQL data recovery</title>
      <link>https://tapoueh.org/blog/2013/09/postgresql-data-recovery/</link>
      <pubDate>Tue, 17 Sep 2013 10:39:00 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2013/09/postgresql-data-recovery/</guid>
      <description>The following story is only interesting to read if you like it when bad things happen, or if you don&amp;rsquo;t have a trustworthy backup policy in place. By trustworthy I mean that each backup you take must be tested with a test recovery job. Only tested backups will prove useful when you need them. So go read our Backup and Restore documentation chapter then learn how to setup Barman for handling physical backups and Point In Time Recovery.</description>
    </item>
    
    <item>
      <title>Using trigrams against typos</title>
      <link>https://tapoueh.org/blog/2013/09/using-trigrams-against-typos/</link>
      <pubDate>Fri, 06 Sep 2013 16:15:00 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2013/09/using-trigrams-against-typos/</guid>
      <description>&lt;p&gt;In our ongoing
&lt;a href=&#34;https://tapoueh.org/tags/extensions&#34;&gt;Tour of Extensions&lt;/a&gt; we played with
&lt;a href=&#34;http://www.postgresql.org/docs/current/static/earthdistance.html&#34;&gt;earth distance&lt;/a&gt; in
&lt;a href=&#34;https://tapoueh.org/blog/2013/08/05-earthdistance&#34;&gt;How far is the nearest pub?&lt;/a&gt; then with
&lt;a href=&#34;http://www.postgresql.org/docs/current/static/hstore.html&#34;&gt;hstore&lt;/a&gt; in a series about trigger,
first to generalize
&lt;a href=&#34;https://tapoueh.org/blog/2013/08/23-parametrized-triggers&#34;&gt;Trigger Parameters&lt;/a&gt; then to enable us to
&lt;a href=&#34;https://tapoueh.org/blog/2013/08/27-auditing-changes-with-hstore&#34;&gt;Auditing Changes with Hstore&lt;/a&gt;. Today we are going to work with
&lt;a href=&#34;http://www.postgresql.org/docs/current/static/pgtrgm.html&#34;&gt;pg_trgm&lt;/a&gt; which
is the
&lt;em&gt;trigrams&lt;/em&gt; PostgreSQL extension: its usage got seriously enhanced in
recent PostgreSQL releases and it&amp;rsquo;s now a poor&amp;rsquo;s man
&lt;a href=&#34;http://www.postgresql.org/docs/current/static/textsearch.html&#34;&gt;Full Text Search&lt;/a&gt;
engine.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Auditing Changes with Hstore</title>
      <link>https://tapoueh.org/blog/2013/08/auditing-changes-with-hstore/</link>
      <pubDate>Tue, 27 Aug 2013 17:35:00 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2013/08/auditing-changes-with-hstore/</guid>
      <description>&lt;p&gt;In a previous article about
&lt;a href=&#34;https://tapoueh.org/blog/2013/08/23-parametrized-triggers&#34;&gt;Trigger Parameters&lt;/a&gt; we have been using the
extension
&lt;a href=&#34;http://www.postgresql.org/docs/9.2/static/hstore.html&#34;&gt;hstore&lt;/a&gt; in order to compute some extra field in our records, where
the fields used both for the computation and for storing the results were
passed in as
&lt;em&gt;dynamic parameters&lt;/em&gt;. Today we&amp;rsquo;re going to see another
&lt;em&gt;trigger&lt;/em&gt;
use case for
&lt;em&gt;hstore&lt;/em&gt;: we are going to record changes made to our tuples.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Trigger Parameters</title>
      <link>https://tapoueh.org/blog/2013/08/trigger-parameters/</link>
      <pubDate>Fri, 23 Aug 2013 12:08:00 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2013/08/trigger-parameters/</guid>
      <description>&lt;p&gt;Sometimes you want to compute values automatically at
&lt;code&gt;INSERT&lt;/code&gt; time, like for
example a
&lt;em&gt;duration&lt;/em&gt; column out of a
&lt;em&gt;start&lt;/em&gt; and an
&lt;em&gt;end&lt;/em&gt; column, both
&lt;em&gt;timestamptz&lt;/em&gt;. It&amp;rsquo;s easy enough to do with a
&lt;code&gt;BEFORE TRIGGER&lt;/code&gt; on your table.
What&amp;rsquo;s more complex is to come up with a parametrized spelling of the
trigger, where you can attach the same
&lt;em&gt;stored procedure&lt;/em&gt; to any table even
when the column names are different from one another.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Understanding Window Functions</title>
      <link>https://tapoueh.org/blog/2013/08/understanding-window-functions/</link>
      <pubDate>Tue, 20 Aug 2013 12:04:00 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2013/08/understanding-window-functions/</guid>
      <description>&lt;p&gt;There was SQL
before
&lt;a href=&#34;http://www.postgresql.org/docs/current/static/tutorial-window.html&#34;&gt;window functions&lt;/a&gt; and
SQL after &lt;em&gt;window functions&lt;/em&gt;: that&amp;rsquo;s how powerful this tool is. Being that
of a deal breaker unfortunately means that it can be quite hard to grasp the
feature. This article aims at making it crystal clear so that you can begin
using it today and are able to reason about it and recognize cases where you
want to be using &lt;em&gt;window functions&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;


 
  
  
  
  
    
      
    
  
    
  
    
      
    
  

&lt;div class=&#34;figure fig50 dim-margin&#34; &gt;
  
    &lt;a class=&#34;fancybox&#34; href=&#34;https://tapoueh.org/img/old/moving_window.gif&#34; data-fancybox-group=&#34;&#34;&gt;
  
    &lt;img class=&#34;fig-img&#34; src=&#34;https://tapoueh.org/img/old/moving_window.gif&#34; &gt;
  
    &lt;/a&gt;
  
  
&lt;/div&gt;

&lt;/center&gt;
&lt;center&gt;&lt;em&gt;We see a part of the data as if through a little window&lt;/em&gt;&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>How far is the nearest pub?</title>
      <link>https://tapoueh.org/blog/2013/08/how-far-is-the-nearest-pub/</link>
      <pubDate>Mon, 05 Aug 2013 08:11:00 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2013/08/how-far-is-the-nearest-pub/</guid>
      <description>&lt;p&gt;In our recent article
about &lt;a href=&#34;https://tapoueh.org/blog/2013/08/02-pub-names-knn&#34;&gt;The Most Popular Pub Names&lt;/a&gt; we did
have a look at how to find the pubs nearby, but didn&amp;rsquo;t compute the
&lt;strong&gt;distance&lt;/strong&gt; in between that pub and us. That&amp;rsquo;s because how to compute a
distance given a position on the earth expressed as &lt;em&gt;longitude&lt;/em&gt; and
&lt;em&gt;latitude&lt;/em&gt; is not that easy. Today, we are going to solve that problem
nonetheless, thanks
to
&lt;a href=&#34;http://www.postgresql.org/docs/9.2/interactive/extend-extensions.html&#34;&gt;PostgreSQL Extensions&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>The Most Popular Pub Names</title>
      <link>https://tapoueh.org/blog/2013/08/the-most-popular-pub-names/</link>
      <pubDate>Fri, 02 Aug 2013 10:19:00 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2013/08/the-most-popular-pub-names/</guid>
      <description>&lt;p&gt;In his article
titled
&lt;a href=&#34;http://blog.mongodb.org/post/56876800071/the-most-popular-pub-names?utm_content=buffer4922c&amp;amp;utm_source=buffer&amp;amp;utm_medium=facebook&amp;amp;utm_campaign=Buffer&#34;&gt;The Most Popular Pub Names&lt;/a&gt; &lt;em&gt;Ross
Lawley&lt;/em&gt; did show us how to perform some quite interesting &lt;em&gt;geographic
queries&lt;/em&gt; against &lt;a href=&#34;http://www.mongodb.org/&#34;&gt;MongoDB&lt;/a&gt;, using some nice &lt;em&gt;Open
Data&lt;/em&gt; found at the &lt;a href=&#34;http://www.openstreetmap.org/&#34;&gt;Open Street Map&lt;/a&gt; project.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Archiving data as fast as possible</title>
      <link>https://tapoueh.org/blog/2013/07/archiving-data-as-fast-as-possible/</link>
      <pubDate>Fri, 05 Jul 2013 15:30:00 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2013/07/archiving-data-as-fast-as-possible/</guid>
      <description>&lt;p&gt;In a recent article here we&amp;rsquo;ve been talking about how do do
&lt;a href=&#34;https://tapoueh.org/blog/2013/03/15-batch-update&#34;&gt;Batch Updates&lt;/a&gt; in
a very efficient way, using the
&lt;em&gt;Writable CTE&lt;/em&gt; features available in
PostgreSQL 9.1. I sometime read how
&lt;a href=&#34;http://www.postgresql.org/docs/current/interactive/queries-with.html&#34;&gt;Common Table Expressions&lt;/a&gt; changed the
life of fellow DBAs and developers, and would say that
&lt;em&gt;Writable CTE&lt;/em&gt; are at
least the same boost again.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Simple Case for Pivoting in SQL</title>
      <link>https://tapoueh.org/blog/2013/07/simple-case-for-pivoting-in-sql/</link>
      <pubDate>Thu, 04 Jul 2013 15:55:00 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2013/07/simple-case-for-pivoting-in-sql/</guid>
      <description>In a recent article Craig Kerstiens from Heroku did demo the really useful crosstab extension. That function allows you to pivot a table so that you can see the data from different categories in separate columns in the same row rather than in separate rows. The article from Craig is Pivoting in Postgres.
Pivoting a matrix, also known as a matrix transposition
Let&amp;rsquo;s do the same setup as he did, with a table containing some randomly generated data about hypothetical visits to a web page, say, by date then by operating system.</description>
    </item>
    
    <item>
      <title>Nearest Big City</title>
      <link>https://tapoueh.org/blog/2013/05/nearest-big-city/</link>
      <pubDate>Thu, 02 May 2013 11:34:00 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2013/05/nearest-big-city/</guid>
      <description>In this article, we want to find the town with the greatest number of inhabitants near a given location.
 A very localized example We first need to find and import some data, and I found at the following place a CSV listing of french cities with coordinates and population and some numbers of interest for the exercise here.
To import the data set, we first need a table, then a COPY command:</description>
    </item>
    
    <item>
      <title>Bulk Replication</title>
      <link>https://tapoueh.org/blog/2013/03/bulk-replication/</link>
      <pubDate>Mon, 18 Mar 2013 14:54:00 +0100</pubDate>
      
      <guid>https://tapoueh.org/blog/2013/03/bulk-replication/</guid>
      <description>In the previous article here we talked about how to properly update more than one row at a time, under the title Batch Update. We did consider performances, including network round trips, and did look at the behavior of our results when used concurrently.
A case where we want to apply the previous article approach is when replicating data with a trigger based solution, such as SkyTools and londiste. Well, maybe not in all cases, we need to have a amount of UPDATE trafic worthy of setting up the solution.</description>
    </item>
    
    <item>
      <title>Batch Update</title>
      <link>https://tapoueh.org/blog/2013/03/batch-update/</link>
      <pubDate>Fri, 15 Mar 2013 10:47:00 +0100</pubDate>
      
      <guid>https://tapoueh.org/blog/2013/03/batch-update/</guid>
      <description>&lt;p&gt;Performance consulting involves some tricks that you have to teach over and
over again. One of them is that SQL tends to be so much better at dealing
with plenty of rows in a single statement when compared to running as many
statements, each one against a single row.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>HyperLogLog Unions</title>
      <link>https://tapoueh.org/blog/2013/02/hyperloglog-unions/</link>
      <pubDate>Tue, 26 Feb 2013 12:44:00 +0100</pubDate>
      
      <guid>https://tapoueh.org/blog/2013/02/hyperloglog-unions/</guid>
      <description>In the article from yesterday we talked about PostgreSQL HyperLogLog with some details. The real magic of that extension has been skimmed over though, and needs another very small article all by itself, in case you missed it.
Which Set Operation do you want for counting unique values?
The first query here has the default level of magic in it, really. What happens is that each time we do an update of the HyperLogLog hash value, we update some data which are allowing us to compute its cardinality.</description>
    </item>
    
    <item>
      <title>PostgreSQL HyperLogLog</title>
      <link>https://tapoueh.org/blog/2013/02/postgresql-hyperloglog/</link>
      <pubDate>Mon, 25 Feb 2013 10:23:00 +0100</pubDate>
      
      <guid>https://tapoueh.org/blog/2013/02/postgresql-hyperloglog/</guid>
      <description>&lt;p&gt;If you&amp;rsquo;ve been following along at home the newer statistics developments,
you might have heard about this new
&lt;a href=&#34;http://research.google.com/pubs/pub40671.html&#34;&gt;State of The Art Cardinality Estimation Algorithm&lt;/a&gt; called
&lt;a href=&#34;http://metamarkets.com/2012/fast-cheap-and-98-right-cardinality-estimation-for-big-data/&#34;&gt;HyperLogLog&lt;/a&gt;. This
technique is now available for PostgreSQL in the extension
&lt;a href=&#34;http://blog.aggregateknowledge.com/2013/02/04/open-source-release-postgresql-hll/&#34;&gt;postgresql-hll&lt;/a&gt;
available at
&lt;a href=&#34;https://github.com/aggregateknowledge/postgresql-hll&#34;&gt;https://github.com/aggregateknowledge/postgresql-hll&lt;/a&gt; and soon
to be in
&lt;code&gt;debian&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Extensions Templates</title>
      <link>https://tapoueh.org/blog/2013/01/extensions-templates/</link>
      <pubDate>Tue, 08 Jan 2013 17:53:00 +0100</pubDate>
      
      <guid>https://tapoueh.org/blog/2013/01/extensions-templates/</guid>
      <description>In a recent article titled Inline Extensions we detailed the problem of how to distribute an extension&amp;rsquo;s package to a remote server without having access to its file system at all. The solution to that problem is non trivial, let&amp;rsquo;s say. But thanks to the awesome PostgreSQL Community we finaly have some practical ideas on how to address the problem as discussed on pgsql-hackers, our development mailing list.
PostgreSQL is first an Awesome Community</description>
    </item>
    
    <item>
      <title>Inline Extensions</title>
      <link>https://tapoueh.org/blog/2012/12/inline-extensions/</link>
      <pubDate>Thu, 13 Dec 2012 11:34:00 +0100</pubDate>
      
      <guid>https://tapoueh.org/blog/2012/12/inline-extensions/</guid>
      <description>We&amp;rsquo;ve been having the CREATE EXTENSION feature in PostgreSQL for a couple of releases now, so let&amp;rsquo;s talk about how to go from here. The first goal of the extension facility has been to allow for a clean dump and restore process of contrib modules. As such it&amp;rsquo;s been tailored to the needs of deploying files on the file system because there&amp;rsquo;s no escaping from that when you have to ship binary and executable files, those infamous .</description>
    </item>
    
    <item>
      <title>Prefixes and Ranges</title>
      <link>https://tapoueh.org/blog/2012/10/prefixes-and-ranges/</link>
      <pubDate>Tue, 16 Oct 2012 10:47:00 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2012/10/prefixes-and-ranges/</guid>
      <description>It&amp;rsquo;s been a long time since I last had some time to spend on the prefix PostgreSQL extension and its prefix_range data type. With PostgreSQL 9.2 out, some users wanted me to update the extension for that release, and hinted me that it was high time that I fix that old bug for which I already had a patch.
prefix_range release 1.2.0 I&amp;rsquo;m sorry it took that long. It&amp;rsquo;s now done, you can have prefix 1.</description>
    </item>
    
    <item>
      <title>Reset Counter</title>
      <link>https://tapoueh.org/blog/2012/10/reset-counter/</link>
      <pubDate>Fri, 05 Oct 2012 09:44:00 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2012/10/reset-counter/</guid>
      <description>&lt;p&gt;I&amp;rsquo;ve been given a nice puzzle that I think is a good blog article
opportunity, as it involves some thinking and &lt;a href=&#34;https://tapoueh.org/blog/2013/08/understanding-window-functions/&#34;&gt;window
functions&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>PostgreSQL 9.3</title>
      <link>https://tapoueh.org/blog/2012/09/postgresql-9.3/</link>
      <pubDate>Sat, 15 Sep 2012 18:43:00 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2012/09/postgresql-9.3/</guid>
      <description>PostgreSQL 9.2 is released! It&amp;rsquo;s an awesome new release that I urge you to consider trying and adopting, an upgrade from even 9.1 should be very well worth it, as your hardware could suddenly be able to process a much higher load. Indeed, better performances mean more work done on the same budget, that&amp;rsquo;s the name of the game!
As a PostgreSQL contributor though, the release of 9.2 mainly means to me that it&amp;rsquo;s time to fully concentrate on preparing 9.</description>
    </item>
    
    <item>
      <title>Clean PGQ Subconsumers</title>
      <link>https://tapoueh.org/blog/2012/04/clean-pgq-subconsumers/</link>
      <pubDate>Thu, 26 Apr 2012 15:05:00 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2012/04/clean-pgq-subconsumers/</guid>
      <description>Now that you&amp;rsquo;re all using the wonders of Cooperative Consumers to help you efficiently and reliably implement your business constraints and offload them from the main user transactions, you&amp;rsquo;re reaching a point where you have to clean up your development environment (because that&amp;rsquo;s what happens to development environments, right?), and you want a way to start again from a clean empty place.
Here we go. It used to be much more simple than that, so if you&amp;rsquo;re still using PGQ from Skytools2, just jump to the next step.</description>
    </item>
    
    <item>
      <title>PGQ Coop Consumers</title>
      <link>https://tapoueh.org/blog/2012/03/pgq-coop-consumers/</link>
      <pubDate>Mon, 12 Mar 2012 14:43:00 +0100</pubDate>
      
      <guid>https://tapoueh.org/blog/2012/03/pgq-coop-consumers/</guid>
      <description>While working a new PostgreSQL architecture for an high scale project that used to be in the top 10 of internet popular web sites (in terms of visitors), I needed to be able to off load some processing from the main path: that&amp;rsquo;s called a batch job. This needs to be transactional: don&amp;rsquo;t run the job if we did rollback; the transaction, process all events that were part of the same transaction in the same transaction, etc.</description>
    </item>
    
    <item>
      <title>Extension White Listing</title>
      <link>https://tapoueh.org/blog/2012/03/extension-white-listing/</link>
      <pubDate>Thu, 08 Mar 2012 14:25:00 +0100</pubDate>
      
      <guid>https://tapoueh.org/blog/2012/03/extension-white-listing/</guid>
      <description>PostgreSQL 9.1 includes proper extension support, as you might well know if you ever read this very blog here. Some hosting facilities are playing with PostgreSQL at big scale (hello Heroku!) and still meet with small caveats making their life uneasy.
To be specific, only superusers are allowed to install C coded stored procedures, and that impacts a lot of very useful PostgreSQL extension: all those shiped in the contrib package are coded in C.</description>
    </item>
    
    <item>
      <title>pgbouncer munin plugin</title>
      <link>https://tapoueh.org/blog/2011/11/pgbouncer-munin-plugin/</link>
      <pubDate>Wed, 16 Nov 2011 14:00:00 +0100</pubDate>
      
      <guid>https://tapoueh.org/blog/2011/11/pgbouncer-munin-plugin/</guid>
      <description>It seems that if you search for a munin plugin for pgbouncer it&amp;rsquo;s easy enough to reach an old page of mine with an old version of my plugin, and a broken link. Let&amp;rsquo;s remedy that by publishing here the newer version of the plugin. To be honest, I though it already made its way into the official munin 1.4 set of plugins, but I&amp;rsquo;ve not been following closely enough.</description>
    </item>
    
    <item>
      <title>Extensions en simple SQL</title>
      <link>https://tapoueh.org/blog/2011/10/extensions-en-simple-sql/</link>
      <pubDate>Mon, 31 Oct 2011 14:22:00 +0100</pubDate>
      
      <guid>https://tapoueh.org/blog/2011/10/extensions-en-simple-sql/</guid>
      <description>La conférence européenne à Amsterdam était un très bon évènement de la communauté, avec une organisation impeccable dans un hôtel accueillant. J&amp;rsquo;ai eu le plaisir d&amp;rsquo;y parler des extensions et de leur usage dans le cadre du développement applicatif « interne », sous le titre Extensions are good for business logic.
   
L&amp;rsquo;idée de ma présentation, que la plupart d&amp;rsquo;entre vous a loupé je suppose (en tout cas je n&amp;rsquo;avais qu&amp;rsquo;une petite poignée de français dans la salle, et j&amp;rsquo;espère avoir des lecteurs qui n&amp;rsquo;étaient pas à Amsterdam), l&amp;rsquo;idée est d&amp;rsquo;utiliser les mécanismes offerts par les extensions afin de maintenir le code PL que vous utilisez en production.</description>
    </item>
    
    <item>
      <title>Scaling Stored Procedures</title>
      <link>https://tapoueh.org/blog/2011/10/scaling-stored-procedures/</link>
      <pubDate>Thu, 06 Oct 2011 18:23:00 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2011/10/scaling-stored-procedures/</guid>
      <description>In the news recently stored procedures where used as an excuse for moving away logic from the database layer to application layer, and to migrate away from a powerful technology to a simpler one, now that there&amp;rsquo;s no logic anymore in the database.
It&amp;rsquo;s not the way I would typically approach scaling problems, and apparently I&amp;rsquo;m not alone on the Stored Procedures camp. Did you read this nice blog post Mythbusters: Stored Procedures Edition already?</description>
    </item>
    
    <item>
      <title>Skytools3: walmgr</title>
      <link>https://tapoueh.org/blog/2011/09/skytools3-walmgr/</link>
      <pubDate>Wed, 21 Sep 2011 17:21:00 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2011/09/skytools3-walmgr/</guid>
      <description>Let&amp;rsquo;s begin the Skytools 3 documentation effort, which is long overdue. The code is waiting for you over at github, and is stable and working. Why is it still in release candidate status, I hear you asking? Well because it&amp;rsquo;s missing updated documentation.
WalMgr is the Skytools component that manages WAL shipping for you, and archiving too. It knows how to prepare your master and standby setup, how to take a base backup and push it to the standby&amp;rsquo;s system, how to archive (at the satndby) master&amp;rsquo;s WAL files as they are produced and have the standby restore from this archive.</description>
    </item>
    
    <item>
      <title>PostgreSQL 9.1</title>
      <link>https://tapoueh.org/blog/2011/09/postgresql-9.1/</link>
      <pubDate>Wed, 14 Sep 2011 10:00:00 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2011/09/postgresql-9.1/</guid>
      <description>PostgreSQL 9.1 est dans les bacs ! Vous n&amp;rsquo;avez pas encore cette nouvelle version en production ? Pas encore évalué pourquoi vous devriez envisager de migrer à cette version ? Il existe beaucoup de bonnes raisons de passer à cette version, et peu de pièges.
Nous commençons à lire des articles qui reprennent la nouvelle dans la presse française, et j&amp;rsquo;ai le plaisir de mentionner celui de programmez.com qui annonce « un système d&amp;rsquo;extensions inégalé ».</description>
    </item>
    
    <item>
      <title>Éviter les injections SQL</title>
      <link>https://tapoueh.org/blog/2011/09/%C3%A9viter-les-injections-sql/</link>
      <pubDate>Wed, 07 Sep 2011 11:36:00 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2011/09/%C3%A9viter-les-injections-sql/</guid>
      <description>Nous avons parlé la dernière fois les règles d&amp;rsquo; échappement de chaînes avec PostgreSQL, et mentionné qu&amp;rsquo;utiliser ces techniques afin de protéger les données insérées dans les requêtes SQL n&amp;rsquo;était pas une bonne idée dans la mesure où PostgreSQL offre une fonctionnalité bien plus adaptée.
Nous faisons face ici à un problème de sécurité très bien décrit dans le billet humoristique de Little Boby Tables, dont je vous recommande la lecture.</description>
    </item>
    
    <item>
      <title>Skytools, version 3</title>
      <link>https://tapoueh.org/blog/2011/08/skytools-version-3/</link>
      <pubDate>Fri, 26 Aug 2011 21:30:00 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2011/08/skytools-version-3/</guid>
      <description>You can find skytools3 in debian experimental already, it&amp;rsquo;s in release candidate status. What&amp;rsquo;s missing is the documentation, so here&amp;rsquo;s an idea: I&amp;rsquo;m going to make a blog post series about skytools next features, how to use them, what they are good for, etc. This first article of the series will just list what are those new features.
Here are the slides from the CHAR(11) talk I made last month, about that very subject:</description>
    </item>
    
    <item>
      <title>Échappement de chaînes</title>
      <link>https://tapoueh.org/blog/2011/08/%C3%A9chappement-de-cha%C3%AEnes/</link>
      <pubDate>Thu, 18 Aug 2011 19:01:00 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2011/08/%C3%A9chappement-de-cha%C3%AEnes/</guid>
      <description>Parmis les nouveautés de la prochaine version de PostgreSQL, la fameuse 9.1, il faut signaler le changement de valeur par défaut de la variable standard_conforming_strings, qui passe à vraie.
En effet, l&amp;rsquo;utilisation d&amp;rsquo;échappements avec le caractère « anti-slash » n&amp;rsquo;est pas conforme au standard SQL. Le paramètre standard_conforming_strings permet de contrôler le comportement de PostgreSQL lorsqu&amp;rsquo;il lit une chaîne de caractère dans une requête SQL.
Voyons quelques exemples :
dimitri=# set standard_conforming_strings to true; SET dimitri=# select &#39;hop&#39;&#39;&#39;; ?</description>
    </item>
    
    <item>
      <title>See Tsung in action</title>
      <link>https://tapoueh.org/blog/2011/08/see-tsung-in-action/</link>
      <pubDate>Tue, 02 Aug 2011 10:30:00 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2011/08/see-tsung-in-action/</guid>
      <description>Tsung is an open-source multi-protocol distributed load testing tool and a mature project. It&amp;rsquo;s been available for about 10 years and is built with the Erlang system. It supports several protocols, including the PostgreSQL one.
When you want to benchmark your own application, to know how many more clients it can handle or how much gain you will see with some new shiny hardware, Tsung is the tool to use. It will allow you to record a number of sessions then replay them at high scale.</description>
    </item>
    
    <item>
      <title>Next month partitions</title>
      <link>https://tapoueh.org/blog/2011/07/next-month-partitions/</link>
      <pubDate>Wed, 27 Jul 2011 22:35:00 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2011/07/next-month-partitions/</guid>
      <description>&lt;p&gt;When you do partition your tables monthly, then comes the question of when
to create next partitions.  I tend to create them just the week before next
month and I have some nice
&lt;a href=&#34;http://www.nagios.org/&#34;&gt;nagios&lt;/a&gt; scripts to alert me in case I&amp;rsquo;ve forgotten
to do so.  How to check that by hand in the end of a month?&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Preparing for PGCON</title>
      <link>https://tapoueh.org/blog/2011/05/preparing-for-pgcon/</link>
      <pubDate>Thu, 12 May 2011 10:30:00 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2011/05/preparing-for-pgcon/</guid>
      <description>It&amp;rsquo;s this time of the year again, the main international PostgreSQL Conference is next week in Ottawa, Canada. If previous years are any indication, this will be great event where to meet with a lot of the members of your community. The core team will be there, developers will be there, and we will meet with users and their challenging use cases.
This is a very good time to review both what you did in the project those last 12 months, and what you plan to do next year.</description>
    </item>
    
    <item>
      <title>Tables and Views dependencies</title>
      <link>https://tapoueh.org/blog/2011/05/tables-and-views-dependencies/</link>
      <pubDate>Wed, 04 May 2011 11:45:00 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2011/05/tables-and-views-dependencies/</guid>
      <description>Let&amp;rsquo;s say you need to ALTER TABLE foo ALTER COLUMN bar TYPE bigint;, and PostgreSQL is helpfully telling you that no you can&amp;rsquo;t because such and such views depend on the column. The basic way to deal with that is to copy paste from the error message the names of the views involved, then prepare a script wherein you first DROP VIEW then ALTER TABLE and finally CREATE VIEW again, all in the same transaction.</description>
    </item>
    
    <item>
      <title>Dynamic Triggers in PLpgSQL</title>
      <link>https://tapoueh.org/blog/2010/11/dynamic-triggers-in-plpgsql/</link>
      <pubDate>Wed, 24 Nov 2010 16:45:00 +0100</pubDate>
      
      <guid>https://tapoueh.org/blog/2010/11/dynamic-triggers-in-plpgsql/</guid>
      <description>You certainly know that implementing dynamic triggers in PLpgSQL is impossible. But I had a very bad night, being up from as soon as 3:30 am today, so that when a developer asked me about reusing the same trigger function code from more than one table and for a dynamic column name, I didn&amp;rsquo;t remember about it being impossible.
Here&amp;rsquo;s what happens in such cases, after a long time on the problem (yes, overall, that&amp;rsquo;s a slow day).</description>
    </item>
    
    <item>
      <title>pg_basebackup</title>
      <link>https://tapoueh.org/blog/2010/11/pg_basebackup/</link>
      <pubDate>Sun, 07 Nov 2010 13:45:00 +0100</pubDate>
      
      <guid>https://tapoueh.org/blog/2010/11/pg_basebackup/</guid>
      <description>Hannu just gave me a good idea in this email on -hackers, proposing that pg_basebackup should get the xlog files again and again in a loop for the whole duration of the base backup. That&amp;rsquo;s now done in the aforementioned tool, whose options got a little more useful now:
Usage: pg_basebackup.py [-v] [-f] [-j jobs] &amp;quot;dsn&amp;quot; dest Options: -h, --help show this help message and exit --version show version and quit -x, --pg_xlog backup the pg_xlog files -v, --verbose be verbose and about processing progress -d, --debug show debug information, including SQL queries -f, --force remove destination directory if it exists -j JOBS, --jobs=JOBS how many helper jobs to launch -D DELAY, --delay=DELAY pg_xlog subprocess loop delay, see -x -S, --slave auxilliary process --stdin get list of files to backup from stdin  Yeah, as implementing the xlog idea required having some kind of parallelism, I built on it and the script now has a --jobs option for you to setup how many processes to launch in parallel, all fetching some base backup files in its own standard ( libpq) PostgreSQL connection, in compressed chunks of 8 MB (so that&amp;rsquo;s not 8 MB chunks sent over).</description>
    </item>
    
    <item>
      <title>Extensions: writing a patch for PostgreSQL</title>
      <link>https://tapoueh.org/blog/2010/10/extensions-writing-a-patch-for-postgresql/</link>
      <pubDate>Fri, 15 Oct 2010 11:30:00 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2010/10/extensions-writing-a-patch-for-postgresql/</guid>
      <description>These days, thanks to my community oriented job, I&amp;rsquo;m working full time on a PostgreSQL patch to terminate basic support for extending SQL. First thing I want to share is that patching the backend code is not as hard as one would think. Second one is that git really is helping.
“Not as hard as one would think, are you kidding me?”, I hear some say. Well, that&amp;rsquo;s true. It&amp;rsquo;s C code in there, but with a very good layer of abstractions so that you&amp;rsquo;re not dealing with subtle problems that much.</description>
    </item>
    
    <item>
      <title>Date puzzle for starters</title>
      <link>https://tapoueh.org/blog/2010/10/date-puzzle-for-starters/</link>
      <pubDate>Fri, 08 Oct 2010 10:00:00 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2010/10/date-puzzle-for-starters/</guid>
      <description>The PostgreSQL IRC channel is a good place to be, for all the very good help you can get there, because people are always wanting to remain helpful, because of the off-topics discussions sometime, or to get to talk with community core members. And to start up your day too.
This morning&amp;rsquo;s question started simple : “how can I check if today is the &amp;ldquo;first sunday fo the month&amp;rdquo;. or &amp;ldquo;the second tuesday of the month&amp;rdquo; etc?</description>
    </item>
    
    <item>
      <title>Resuming work on Extensions, first little step</title>
      <link>https://tapoueh.org/blog/2010/10/resuming-work-on-extensions-first-little-step/</link>
      <pubDate>Thu, 07 Oct 2010 17:15:00 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2010/10/resuming-work-on-extensions-first-little-step/</guid>
      <description>Yeah I&amp;rsquo;m back on working on my part of the extension thing in PostgreSQL.
First step is a little one, but as it has public consequences, I figured I&amp;rsquo;d talk about it already. I&amp;rsquo;ve just refreshed my git repository to follow the new master one, and you can see that here http://git.postgresql.org/gitweb?p=postgresql-extension.git;a=commitdiff;h=9a88e9de246218e93c04b6b97e1ef61d97925430.
It&amp;rsquo;s been easier than I feared, mainly:
$ git --no-pager diff master..extension $ git --no-pager format-patch master..extension $ cp 0001-First-stab-at-writing-pg_execute_from_file-function.</description>
    </item>
    
    <item>
      <title>Window Functions example remix</title>
      <link>https://tapoueh.org/blog/2010/09/window-functions-example-remix/</link>
      <pubDate>Sun, 12 Sep 2010 21:35:00 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2010/09/window-functions-example-remix/</guid>
      <description>The drawback of hosting a static only website is, obviously, the lack of comments. What happens actually, though, is that I receive very few comments by direct mail. As I don&amp;rsquo;t get another spam source to cleanup, I&amp;rsquo;m left unconvinced that&amp;rsquo;s such a drawback. I still miss the low probability of seeing blog readers exchange directly, but I think a tapoueh.org mailing list would be my answer, here&amp;hellip;
Anyway, David Fetter took the time to send me a comment by mail with a cleaned up rewrite of the previous entry SQL, here&amp;rsquo;s it for your pleasure!</description>
    </item>
    
    <item>
      <title>Window Functions example</title>
      <link>https://tapoueh.org/blog/2010/09/window-functions-example/</link>
      <pubDate>Thu, 09 Sep 2010 16:35:00 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2010/09/window-functions-example/</guid>
      <description>So, when 8.4 came out there was all those comments about how getting window functions was an awesome addition. Now, it seems that a lot of people seeking for help in #postgresql just don&amp;rsquo;t know what kind of problem this feature helps solving. I&amp;rsquo;ve already been using them in some cases here in this blog, for getting some nice overview about Partitioning: relation size per “group”.
That&amp;rsquo;s another way to count change</description>
    </item>
    
    <item>
      <title>Synchronous Replication</title>
      <link>https://tapoueh.org/blog/2010/09/synchronous-replication/</link>
      <pubDate>Mon, 06 Sep 2010 18:05:00 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2010/09/synchronous-replication/</guid>
      <description>Although the new asynchronous replication facility that ships with 9.0 ain&amp;rsquo;t released to the wide public yet, our hackers hero are already working on the synchronous version of it. A part of the facility is rather easy to design, we want something comparable to DRBD flexibility, but specific to our database world. So synchronous would either mean recv, fsync or apply, depending on what you need the standby to have already done when the master acknowledges the COMMIT.</description>
    </item>
    
    <item>
      <title>Happy Numbers</title>
      <link>https://tapoueh.org/blog/2010/08/happy-numbers/</link>
      <pubDate>Mon, 30 Aug 2010 11:00:00 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2010/08/happy-numbers/</guid>
      <description>After discovering the excellent Gwene service, which allows you to subscribe to newsgroups to read RSS content ( blogs, planets, commits, etc), I came to read this nice article about Happy Numbers. That&amp;rsquo;s a little problem that fits well an interview style question, so I first solved it yesterday evening in Emacs Lisp as that&amp;rsquo;s the language I use the most those days.
 A happy number is defined by the following process.</description>
    </item>
    
    <item>
      <title>Playing with bit strings</title>
      <link>https://tapoueh.org/blog/2010/08/playing-with-bit-strings/</link>
      <pubDate>Thu, 26 Aug 2010 17:45:00 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2010/08/playing-with-bit-strings/</guid>
      <description>The idea of the day ain&amp;rsquo;t directly from me, I&amp;rsquo;m just helping with a very thin subpart of the problem. The problem, I can&amp;rsquo;t say much about, let&amp;rsquo;s just assume you want to reduce the storage of MD5 in your database, so you want to abuse bit strings. A solution to use them works fine, but the datatype is still missing some facilities, for example going from and to hexadecimal representation in text.</description>
    </item>
    
    <item>
      <title>Editing constants in constraints</title>
      <link>https://tapoueh.org/blog/2010/08/editing-constants-in-constraints/</link>
      <pubDate>Mon, 09 Aug 2010 14:45:00 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2010/08/editing-constants-in-constraints/</guid>
      <description>&lt;p&gt;We&amp;rsquo;re using constants in some constraints here, for example in cases where
several servers are replicating to the same
&lt;em&gt;federating&lt;/em&gt; one: each origin
server has his own schema, and all is replicated nicely on the central host,
thanks to
&lt;a href=&#34;http://wiki.postgresql.org/wiki/Londiste_Tutorial#Federated_database&#34;&gt;Londiste&lt;/a&gt;, as you might have guessed already.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Querying the Catalog to plan an upgrade</title>
      <link>https://tapoueh.org/blog/2010/08/querying-the-catalog-to-plan-an-upgrade/</link>
      <pubDate>Thu, 05 Aug 2010 11:00:00 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2010/08/querying-the-catalog-to-plan-an-upgrade/</guid>
      <description>Some user on IRC was reading the releases notes in order to plan for a minor upgrade of his 8.3.3 installation, and was puzzled about potential needs for rebuilding GIST indexes. That&amp;rsquo;s from the 8.3.5 release notes, and from the 8.3.8 notes you see that you need to consider hash indexes on interval columns too. Now the question is, how to find out if any such beasts are in use in your database?</description>
    </item>
    
    <item>
      <title>Database Virtual Machines</title>
      <link>https://tapoueh.org/blog/2010/08/database-virtual-machines/</link>
      <pubDate>Tue, 03 Aug 2010 13:30:00 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2010/08/database-virtual-machines/</guid>
      <description>Today I&amp;rsquo;m being told once again about SQLite as an embedded database software. That one ain&amp;rsquo;t a database server but a software library that you can use straight into your main program. I&amp;rsquo;m yet to use it, but it looks like its SQL support is good enough for simple things — and that covers loads of things. I guess read-only cache and configuration storage would be the obvious ones, because it seems that SQLite use cases aren&amp;rsquo;t including mixed concurrency, that is workloads with concurrent readers and writers.</description>
    </item>
    
    <item>
      <title>Partitioning: relation size per “group”</title>
      <link>https://tapoueh.org/blog/2010/07/partitioning-relation-size-per-group/</link>
      <pubDate>Mon, 26 Jul 2010 17:00:00 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2010/07/partitioning-relation-size-per-group/</guid>
      <description>This time, we are trying to figure out where is the bulk of the data on disk. The trick is that we&amp;rsquo;re using DDL partitioning, but we want a “nice” view of size per partition set. Meaning that if you have for example a parent table foo with partitions foo_201006 and foo_201007, you would want to see a single category foo containing the accumulated size of all the partitions underneath foo.</description>
    </item>
    
    <item>
      <title>Background writers</title>
      <link>https://tapoueh.org/blog/2010/07/background-writers/</link>
      <pubDate>Mon, 19 Jul 2010 16:30:00 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2010/07/background-writers/</guid>
      <description>There&amp;rsquo;s currently a thread on hackers about bg worker: overview and a series of 6 patches. Thanks a lot Markus! This is all about generalizing a concept already in use in the autovacuum process, where you have an independent subsystem that require having an autonomous daemon running and able to start its own workers.
I&amp;rsquo;ve been advocating about generalizing this concept for awhile already, in order to have postmaster able to communicate to subsystems when to shut down and start and reload, etc.</description>
    </item>
    
    <item>
      <title>Logs analysis</title>
      <link>https://tapoueh.org/blog/2010/07/logs-analysis/</link>
      <pubDate>Tue, 13 Jul 2010 14:15:00 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2010/07/logs-analysis/</guid>
      <description>Nowadays to analyze logs and provide insights, the more common tool to use is pgfouine, which does an excellent job. But there has been some improvements in logs capabilities that we&amp;rsquo;re not benefiting from yet, and I&amp;rsquo;m thinking about the CSV log format.
So the idea would be to turn pgfouine into a set of SQL queries against the logs themselves once imported into the database. Wait. What about having our next PostgreSQL version, which is meant (I believe) to include CSV support in SQL/MED, to directly expose its logs as a system view?</description>
    </item>
    
    <item>
      <title>Using indexes as column store?</title>
      <link>https://tapoueh.org/blog/2010/07/using-indexes-as-column-store/</link>
      <pubDate>Thu, 08 Jul 2010 11:15:00 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2010/07/using-indexes-as-column-store/</guid>
      <description>There&amp;rsquo;s a big trend nowadays about using column storage as opposed to what PostgreSQL is doing, which would be row storage. The difference is that if you have the same column value in a lot of rows, you could get to a point where you have this value only once in the underlying storage file. That means high compression. Then you tweak the executor to be able to load this value only once, not once per row, and you win another huge source of data traffic (often enough, from disk).</description>
    </item>
    
    <item>
      <title>Back from PgCon2010</title>
      <link>https://tapoueh.org/blog/2010/05/back-from-pgcon2010/</link>
      <pubDate>Thu, 27 May 2010 14:26:00 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2010/05/back-from-pgcon2010/</guid>
      <description>This year&amp;rsquo;s edition has been the best pgcon ever for me. Granted, it&amp;rsquo;s only my third time, but still :) As Josh said the &amp;ldquo;Hall Track&amp;rdquo; in particular was very good, and the Dev Meeting has been very effective!
Extensions This time I prepared some slides to present the extension design and I tried hard to make it so that we get to agree on a plan, even recognizing it&amp;rsquo;s not solving all of our problems from the get go.</description>
    </item>
    
    <item>
      <title>Finding orphaned sequences</title>
      <link>https://tapoueh.org/blog/2010/03/finding-orphaned-sequences/</link>
      <pubDate>Wed, 17 Mar 2010 13:35:00 +0100</pubDate>
      
      <guid>https://tapoueh.org/blog/2010/03/finding-orphaned-sequences/</guid>
      <description>This time we&amp;rsquo;re having a database where sequences were used, but not systematically as a default value of a given column. It&amp;rsquo;s mainly an historic bad idea, but you know the usual excuse with bad ideas and bad code: the first 6 months it&amp;rsquo;s experimental, after that it&amp;rsquo;s historic.
Not talking about genome orphaned sequences here, though
Still, here&amp;rsquo;s a query for 8.4 that will allow you to list those sequences you have that are not used as a default value in any of your tables:</description>
    </item>
    
    <item>
      <title>Getting out of SQL_ASCII, part 2</title>
      <link>https://tapoueh.org/blog/2010/02/getting-out-of-sql_ascii-part-2/</link>
      <pubDate>Tue, 23 Feb 2010 17:30:00 +0100</pubDate>
      
      <guid>https://tapoueh.org/blog/2010/02/getting-out-of-sql_ascii-part-2/</guid>
      <description>&lt;p&gt;So, if you followed the previous blog entry, now you have a new database
containing all the
&lt;em&gt;static&lt;/em&gt; tables encoded in
&lt;code&gt;UTF-8&lt;/code&gt; rather than
&lt;code&gt;SQL_ASCII&lt;/code&gt;. Because if it was not yet the case, you now severely distrust
this non-encoding.&lt;/p&gt;

&lt;p&gt;Now is the time to have a look at properly encoding the
&lt;em&gt;live&lt;/em&gt; data, those
stored in tables that continue to receive write traffic. The idea is to use
the
&lt;code&gt;UPDATE&lt;/code&gt; facilities of PostgreSQL to tweak the data, and too fix the
applications so as not to continue inserting badly encoded strings in there.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Getting out of SQL_ASCII, part 1</title>
      <link>https://tapoueh.org/blog/2010/02/getting-out-of-sql_ascii-part-1/</link>
      <pubDate>Thu, 18 Feb 2010 11:37:00 +0100</pubDate>
      
      <guid>https://tapoueh.org/blog/2010/02/getting-out-of-sql_ascii-part-1/</guid>
      <description>It happens that you have to manage databases designed by your predecessor, and it even happens that the team used to not have a DBA. Those histerical raisins can lead to having a SQL_ASCII database. The horror!
What SQL_ASCII means, if you&amp;rsquo;re not already familiar with the consequences of such a choice, is that all the text and varchar data that you put in the database is accepted as-is. No checks.</description>
    </item>
    
    <item>
      <title>Resetting sequences. All of them, please!</title>
      <link>https://tapoueh.org/blog/2010/02/resetting-sequences.-all-of-them-please/</link>
      <pubDate>Tue, 16 Feb 2010 16:23:00 +0100</pubDate>
      
      <guid>https://tapoueh.org/blog/2010/02/resetting-sequences.-all-of-them-please/</guid>
      <description>So, after restoring a production dump with intermediate filtering, none of our sequences were set to the right value. I could have tried to review the process of filtering the dump here, but it&amp;rsquo;s a one-shot action and you know what that sometimes mean. With some pressure you don&amp;rsquo;t script enough of it and you just crawl more and more.
Still, I think how I solved it is worthy of a blog entry.</description>
    </item>
    
    <item>
      <title>PgCon 2009</title>
      <link>https://tapoueh.org/blog/2009/05/pgcon-2009/</link>
      <pubDate>Wed, 27 May 2009 14:30:00 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2009/05/pgcon-2009/</guid>
      <description>I can&amp;rsquo;t really compare PgCon 2009 with previous years versions, last time I enjoyed the event it was in 2006, in Toronto. But still I found the experience to be a great one, and I hope I&amp;rsquo;ll be there next year too!
I&amp;rsquo;ve met a lot of known people in the community, some of them I already had the chance to run into at Toronto or Prato, but this was the first time I got to talk to many of them about interresting projects and ideas.</description>
    </item>
    
    <item>
      <title>Skytools 3.0 reaches alpha1</title>
      <link>https://tapoueh.org/blog/2009/04/skytools-3.0-reaches-alpha1/</link>
      <pubDate>Tue, 14 Apr 2009 00:00:00 +0200</pubDate>
      
      <guid>https://tapoueh.org/blog/2009/04/skytools-3.0-reaches-alpha1/</guid>
      <description>It&amp;rsquo;s time for Skytools news again! First, we did improve documentation of current stable branch with hosting high level presentations and tutorials on the PostgreSQL wiki. Do check out the Londiste Tutorial, it seems that&amp;rsquo;s what people hesitating to try out londiste were missing the most.
The other things people miss out a lot in current stable Skytools (version 2.1.9 currently) are cascading replication (which allows for switchover and failover) and DDL support.</description>
    </item>
    
    <item>
      <title>Importing XML content from file</title>
      <link>https://tapoueh.org/blog/2009/02/importing-xml-content-from-file/</link>
      <pubDate>Thu, 05 Feb 2009 00:00:00 +0100</pubDate>
      
      <guid>https://tapoueh.org/blog/2009/02/importing-xml-content-from-file/</guid>
      <description>The problem was raised this week on IRC and this time again I felt it would be a good occasion for a blog entry: how to load an XML file content into a single field?
The usual tool used to import files is COPY, but it&amp;rsquo;ll want each line of the file to host a text representation of a database tuple, so it doesn&amp;rsquo;t apply to the case at hand. RhodiumToad was online and offered the following code to solve the problem:</description>
    </item>
    
    <item>
      <title>Skytools ticker daemon and londiste</title>
      <link>https://tapoueh.org/blog/2009/02/skytools-ticker-daemon-and-londiste/</link>
      <pubDate>Tue, 03 Feb 2009 00:00:00 +0100</pubDate>
      
      <guid>https://tapoueh.org/blog/2009/02/skytools-ticker-daemon-and-londiste/</guid>
      <description>One of the difficulties in getting to understand and configure londiste reside in the relation between the ticker and the replication. This question was raised once more on IRC yesterday, so I made a new FAQ entry about it: How do this ticker thing relates to londiste?</description>
    </item>
    
    <item>
      <title>Comparing Londiste and Slony</title>
      <link>https://tapoueh.org/blog/2009/01/comparing-londiste-and-slony/</link>
      <pubDate>Sat, 31 Jan 2009 00:00:00 +0100</pubDate>
      
      <guid>https://tapoueh.org/blog/2009/01/comparing-londiste-and-slony/</guid>
      <description>In the page about Skytools I&amp;rsquo;ve encouraged people to ask some more questions in order for me to be able to try and answer them. That just happened, as usual on the #postgresql IRC, and the question is What does londiste lack that slony has?</description>
    </item>
    
    <item>
      <title>Controling HOT usage in 8.3</title>
      <link>https://tapoueh.org/blog/2009/01/controling-hot-usage-in-8.3/</link>
      <pubDate>Wed, 28 Jan 2009 00:00:00 +0100</pubDate>
      
      <guid>https://tapoueh.org/blog/2009/01/controling-hot-usage-in-8.3/</guid>
      <description>&lt;p&gt;As it happens, I&amp;rsquo;ve got some environments where I want to make sure &lt;em&gt;HOT&lt;/em&gt; (
&lt;em&gt;aka Heap Only Tuples&lt;/em&gt;) is in use. Because we&amp;rsquo;re doing so much updates a
second that I want to get sure it&amp;rsquo;s not killing my database server. I not
only wrote some checking view to see about it, but also made
a
&lt;a href=&#34;http://www.postgresql.fr/support:trucs_et_astuces:controler_l_utilisation_de_hot_a_partir_de_la_8.3&#34;&gt;quick article&lt;/a&gt; about
it in the &lt;a href=&#34;http://postgresql.fr/&#34;&gt;French PostgreSQL website&lt;/a&gt;. Handling
around in &lt;code&gt;#postgresql&lt;/code&gt; means that I&amp;rsquo;m now bound to write about it in
English too!&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Londiste Trick</title>
      <link>https://tapoueh.org/blog/2009/01/londiste-trick/</link>
      <pubDate>Wed, 21 Jan 2009 00:00:00 +0100</pubDate>
      
      <guid>https://tapoueh.org/blog/2009/01/londiste-trick/</guid>
      <description>So, you&amp;rsquo;re using londiste and the ticker has not been running all night long, due to some restart glitch in your procedures, and the on call admin didn&amp;rsquo;t notice the restart failure. If you blindly restart the replication daemon, it will load in memory all those events produced during the night, at once, because you now have only one tick where to put them all.
The following query allows you to count how many events that represents, with the magic tick numbers coming from pgq.</description>
    </item>
    
    <item>
      <title>new site, using new software</title>
      <link>https://tapoueh.org/blog/2008/12/new-site-using-new-software/</link>
      <pubDate>Sat, 06 Dec 2008 00:00:00 +0100</pubDate>
      
      <guid>https://tapoueh.org/blog/2008/12/new-site-using-new-software/</guid>
      <description>Oh and check out the skytools page too. Emacs Muse is so great a project that instead of just working on how to publish a website with this tool, I found myself editing a rather large document about londite.py.</description>
    </item>
    
  </channel>
</rss>