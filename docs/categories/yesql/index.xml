<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Yesql on Dimitri Fontaine, Expert PostgreSQL</title>
    <link>http://tapoueh.org/categories/yesql/</link>
    <description>Recent content in Yesql on Dimitri Fontaine, Expert PostgreSQL</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 21 Feb 2014 13:25:00 +0100</lastBuildDate>
    
	<atom:link href="http://tapoueh.org/categories/yesql/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>PostgreSQL, Aggregates and Histograms</title>
      <link>http://tapoueh.org/blog/2014/02/postgresql-aggregates-and-histograms/</link>
      <pubDate>Fri, 21 Feb 2014 13:25:00 +0100</pubDate>
      
      <guid>http://tapoueh.org/blog/2014/02/postgresql-aggregates-and-histograms/</guid>
      <description>In our previous article Aggregating NBA data, PostgreSQL vs MongoDB we spent time comparing the pretty new MongoDB Aggregation Framework with the decades old SQL aggregates. Today, let&amp;rsquo;s showcase more of those SQL aggregates, producing a nice histogram right from our SQL console.
PostgreSQL and Mathematics The other day while giving a Practical SQL training my attention drifted to the width_bucket function available as part of the Mathematical Functions and Operators PostgreSQL is offering to its fearless SQL users.</description>
    </item>
    
    <item>
      <title>PostgreSQL, Aggregates and Histograms</title>
      <link>http://tapoueh.org/manual-post/2014/02/postgresql-histogram/</link>
      <pubDate>Fri, 21 Feb 2014 13:25:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/manual-post/2014/02/postgresql-histogram/</guid>
      <description>&lt;p&gt;In our previous
article
&lt;a href=&#34;http://tapoueh.org/blog/2014/02/17-aggregating-nba-data-PostgreSQL-vs-MongoDB&#34;&gt;Aggregating NBA data, PostgreSQL vs MongoDB&lt;/a&gt; we
spent time comparing the pretty new &lt;em&gt;MongoDB Aggregation Framework&lt;/em&gt; with the
decades old SQL aggregates. Today, let&amp;rsquo;s showcase more of those SQL
aggregates, producing a nice &lt;em&gt;histogram&lt;/em&gt; right from our SQL console.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Aggregating NBA data, PostgreSQL vs MongoDB</title>
      <link>http://tapoueh.org/blog/2014/02/aggregating-nba-data-postgresql-vs-mongodb/</link>
      <pubDate>Mon, 17 Feb 2014 23:40:00 +0100</pubDate>
      
      <guid>http://tapoueh.org/blog/2014/02/aggregating-nba-data-postgresql-vs-mongodb/</guid>
      <description>When reading the article Crunching 30 Years of NBA Data with MongoDB Aggregation I coulnd&amp;rsquo;t help but think that we&amp;rsquo;ve been enjoying aggregates in SQL for 3 or 4 decades already. When using PostgreSQL it&amp;rsquo;s even easy to actually add your own aggregates given the SQL command create aggregate.
Photo Credit: Copyright All rights reserved by Segward Graupner
The next step after thinking how obvious the queries written in the mentionned article would be to express in SQL was to actually load the data into PostgreSQL and write the aggregate queries, of course.</description>
    </item>
    
    <item>
      <title>Aggregating NBA data, PostgreSQL vs MongoDB</title>
      <link>http://tapoueh.org/manual-post/2014/02/aggregating-nba-data-postgresql-vs-mongodb/</link>
      <pubDate>Mon, 17 Feb 2014 23:40:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/manual-post/2014/02/aggregating-nba-data-postgresql-vs-mongodb/</guid>
      <description>&lt;p&gt;When reading the
article
&lt;a href=&#34;http://thecodebarbarian.wordpress.com/2014/02/14/crunching-30-years-of-nba-data-with-mongodb-aggregation/&#34;&gt;Crunching 30 Years of NBA Data with MongoDB Aggregation&lt;/a&gt; I
coulnd&amp;rsquo;t help but think that we&amp;rsquo;ve been
enjoying
&lt;a href=&#34;http://www.postgresql.org/docs/current/static/tutorial-agg.html&#34;&gt;aggregates&lt;/a&gt; in
SQL for 3 or 4 decades already. When
using &lt;a href=&#34;http://www.postgresql.org/&#34;&gt;PostgreSQL&lt;/a&gt; it&amp;rsquo;s even easy to actually
add your own aggregates given the SQL
command
&lt;a href=&#34;http://www.postgresql.org/docs/current/static/sql-createaggregate.html&#34;&gt;create aggregate&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Denormalizing Tags</title>
      <link>http://tapoueh.org/blog/2013/10/denormalizing-tags/</link>
      <pubDate>Thu, 24 Oct 2013 13:40:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2013/10/denormalizing-tags/</guid>
      <description>In our Tour of Extensions today&amp;rsquo;s article is about advanced tag indexing. We have a great data collection to play with and our goal today is to be able to quickly find data matching a complex set of tags. So, let&amp;rsquo;s find out those lastfm tracks that are tagged as blues and rhythm and blues, for instance.
In this article, we&amp;rsquo;re going to play with music related tags
We&amp;rsquo;re going to use the Last.</description>
    </item>
    
    <item>
      <title>An Interview about MariaDB and PostgreSQL</title>
      <link>http://tapoueh.org/blog/2013/10/an-interview-about-mariadb-and-postgresql/</link>
      <pubDate>Wed, 16 Oct 2013 21:07:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2013/10/an-interview-about-mariadb-and-postgresql/</guid>
      <description>At the Open World Forum two weeks ago I had the pleasure to meet with Colin Charles. We had a nice talk about the current state of both MariaDB and PostgreSQL, and even were both interviewed by the Open World Forum Team. The interview is now available online. Dear French readers, it&amp;rsquo;s in English.
Allo Mum? Yeah, I&amp;rsquo;m on TV. Well, Actually, Internet TV.
Here&amp;rsquo;s the video:
 Executive Summary: MariaDB is a drop-in fully Open Source replacement for MySQL and sees quite some progress and innovation being made, and PostgreSQL is YeSQL!</description>
    </item>
    
    <item>
      <title>PostgreSQL Autonomous Transaction</title>
      <link>http://tapoueh.org/blog/2013/10/postgresql-autonomous-transaction/</link>
      <pubDate>Mon, 14 Oct 2013 11:25:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2013/10/postgresql-autonomous-transaction/</guid>
      <description>PostgreSQL is an all round impressive Relational DataBase Management System which implements the SQL standard (see the very useful reference page Comparison of different SQL implementations for details). PostgreSQL also provides with unique solutions in the database market and has been leading innovation for some years now. Still, there&amp;rsquo;s no support for Autonomous Transactions within the server itself. Let&amp;rsquo;s have a look at how to easily implement them with PL/Proxy.</description>
    </item>
    
    <item>
      <title>Geolocation with PostgreSQL</title>
      <link>http://tapoueh.org/blog/2013/10/geolocation-with-postgresql/</link>
      <pubDate>Wed, 09 Oct 2013 17:42:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2013/10/geolocation-with-postgresql/</guid>
      <description>Let&amp;rsquo;s get back to our Tour of Extensions that had to be kept aside for awhile with other concerns such as last chance PostgreSQL data recovery. Now that we have a data loading tool up to the task (read about it in the Loading Geolocation Data article) we&amp;rsquo;re going to be able to play with the awesome ip4r extension from RhodiumToad.
The name of the game is to put IP adresses on a map</description>
    </item>
    
    <item>
      <title>Using trigrams against typos</title>
      <link>http://tapoueh.org/blog/2013/09/using-trigrams-against-typos/</link>
      <pubDate>Fri, 06 Sep 2013 16:15:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2013/09/using-trigrams-against-typos/</guid>
      <description>In our ongoing Tour of Extensions we played with earth distance in How far is the nearest pub? then with hstore in a series about trigger, first to generalize Trigger Parameters then to enable us to Auditing Changes with Hstore. Today we are going to work with pg_trgm which is the trigrams PostgreSQL extension: its usage got seriously enhanced in recent PostgreSQL releases and it&amp;rsquo;s now a poor&amp;rsquo;s man Full Text Search engine.</description>
    </item>
    
    <item>
      <title>Auditing Changes with Hstore</title>
      <link>http://tapoueh.org/blog/2013/08/auditing-changes-with-hstore/</link>
      <pubDate>Tue, 27 Aug 2013 17:35:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2013/08/auditing-changes-with-hstore/</guid>
      <description>In a previous article about Trigger Parameters we have been using the extension hstore in order to compute some extra field in our records, where the fields used both for the computation and for storing the results were passed in as dynamic parameters. Today we&amp;rsquo;re going to see another trigger use case for hstore: we are going to record changes made to our tuples.
Comparing hstores One of the operators that hstore propose is the hstore - hstore operator whose documentation says that it will delete matching pairs from left operand.</description>
    </item>
    
    <item>
      <title>Trigger Parameters</title>
      <link>http://tapoueh.org/blog/2013/08/trigger-parameters/</link>
      <pubDate>Fri, 23 Aug 2013 12:08:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2013/08/trigger-parameters/</guid>
      <description>Sometimes you want to compute values automatically at INSERT time, like for example a duration column out of a start and an end column, both timestamptz. It&amp;rsquo;s easy enough to do with a BEFORE TRIGGER on your table. What&amp;rsquo;s more complex is to come up with a parametrized spelling of the trigger, where you can attach the same stored procedure to any table even when the column names are different from one another.</description>
    </item>
    
    <item>
      <title>Understanding Window Functions</title>
      <link>http://tapoueh.org/manual-post/2013/08/window-functions/</link>
      <pubDate>Tue, 20 Aug 2013 12:04:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/manual-post/2013/08/window-functions/</guid>
      <description>&lt;p&gt;There was SQL
before
&lt;a href=&#34;http://www.postgresql.org/docs/current/static/tutorial-window.html&#34;&gt;window functions&lt;/a&gt; and
SQL after &lt;em&gt;window functions&lt;/em&gt;: that&amp;rsquo;s how powerful this tool is. Being that
of a deal breaker unfortunately means that it can be quite hard to grasp the
feature. This article aims at making it crystal clear so that you can begin
using it today and are able to reason about it and recognize cases where you
want to be using &lt;em&gt;window functions&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Understanding Window Functions</title>
      <link>http://tapoueh.org/blog/2013/08/understanding-window-functions/</link>
      <pubDate>Tue, 20 Aug 2013 12:04:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2013/08/understanding-window-functions/</guid>
      <description>There was SQL before window functions and SQL after window functions: that&amp;rsquo;s how powerful this tool is. Being that of a deal breaker unfortunately means that it can be quite hard to grasp the feature. This article aims at making it crystal clear so that you can begin using it today and are able to reason about it and recognize cases where you want to be using window functions.</description>
    </item>
    
    <item>
      <title>How far is the nearest pub?</title>
      <link>http://tapoueh.org/manual-post/2013/08/earthdistance/</link>
      <pubDate>Mon, 05 Aug 2013 08:11:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/manual-post/2013/08/earthdistance/</guid>
      <description>&lt;p&gt;In our recent article
about &lt;a href=&#34;http://tapoueh.org/blog/2013/08/02-pub-names-knn&#34;&gt;The Most Popular Pub Names&lt;/a&gt; we did
have a look at how to find the pubs nearby, but didn&amp;rsquo;t compute the
&lt;strong&gt;distance&lt;/strong&gt; in between that pub and us. That&amp;rsquo;s because how to compute a
distance given a position on the earth expressed as &lt;em&gt;longitude&lt;/em&gt; and
&lt;em&gt;latitude&lt;/em&gt; is not that easy. Today, we are going to solve that problem
nonetheless, thanks
to
&lt;a href=&#34;http://www.postgresql.org/docs/9.2/interactive/extend-extensions.html&#34;&gt;PostgreSQL Extensions&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>How far is the nearest pub?</title>
      <link>http://tapoueh.org/blog/2013/08/how-far-is-the-nearest-pub/</link>
      <pubDate>Mon, 05 Aug 2013 08:11:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2013/08/how-far-is-the-nearest-pub/</guid>
      <description>In our recent article about The Most Popular Pub Names we did have a look at how to find the pubs nearby, but didn&amp;rsquo;t compute the distance in between that pub and us. That&amp;rsquo;s because how to compute a distance given a position on the earth expressed as longitude and latitude is not that easy. Today, we are going to solve that problem nonetheless, thanks to PostgreSQL Extensions.
Some math are required to go from (long, lat) to distance on earth</description>
    </item>
    
    <item>
      <title>The Most Popular Pub Names</title>
      <link>http://tapoueh.org/manual-post/2013/08/pub-names-knn/</link>
      <pubDate>Fri, 02 Aug 2013 10:19:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/manual-post/2013/08/pub-names-knn/</guid>
      <description>&lt;p&gt;In his article
titled
&lt;a href=&#34;http://blog.mongodb.org/post/56876800071/the-most-popular-pub-names?utm_content=buffer4922c&amp;amp;utm_source=buffer&amp;amp;utm_medium=facebook&amp;amp;utm_campaign=Buffer&#34;&gt;The Most Popular Pub Names&lt;/a&gt; &lt;em&gt;Ross
Lawley&lt;/em&gt; did show us how to perform some quite interesting &lt;em&gt;geographic
queries&lt;/em&gt; against &lt;a href=&#34;http://www.mongodb.org/&#34;&gt;MongoDB&lt;/a&gt;, using some nice &lt;em&gt;Open
Data&lt;/em&gt; found at the &lt;a href=&#34;http://www.openstreetmap.org/&#34;&gt;Open Street Map&lt;/a&gt; project.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>The Most Popular Pub Names</title>
      <link>http://tapoueh.org/blog/2013/08/the-most-popular-pub-names/</link>
      <pubDate>Fri, 02 Aug 2013 10:19:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2013/08/the-most-popular-pub-names/</guid>
      <description>In his article titled The Most Popular Pub Names Ross Lawley did show us how to perform some quite interesting geographic queries against MongoDB, using some nice Open Data found at the Open Street Map project.
  The Open Street Map project publishes a lot of information!
I found the idea behind that article really neat: using easily accessible data produced by an Open Source project to show off some nice queries with real data is what we should do more often.</description>
    </item>
    
    <item>
      <title>Archiving data as fast as possible</title>
      <link>http://tapoueh.org/blog/2013/07/archiving-data-as-fast-as-possible/</link>
      <pubDate>Fri, 05 Jul 2013 15:30:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2013/07/archiving-data-as-fast-as-possible/</guid>
      <description>In a recent article here we&amp;rsquo;ve been talking about how do do Batch Updates in a very efficient way, using the Writable CTE features available in PostgreSQL 9.1. I sometime read how Common Table Expressions changed the life of fellow DBAs and developers, and would say that Writable CTE are at least the same boost again.
Writable CTEs allow to easily implement data processing pipelines
In the case of archiving data into side tables the pipeline we&amp;rsquo;re talking about aims to move data out of a table (that&amp;rsquo;s a DELETE) then store it on the destination ( archiving) table, and that&amp;rsquo;s an INSERT:</description>
    </item>
    
    <item>
      <title>Simple Case for Pivoting in SQL</title>
      <link>http://tapoueh.org/blog/2013/07/simple-case-for-pivoting-in-sql/</link>
      <pubDate>Thu, 04 Jul 2013 15:55:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2013/07/simple-case-for-pivoting-in-sql/</guid>
      <description>In a recent article Craig Kerstiens from Heroku did demo the really useful crosstab extension. That function allows you to pivot a table so that you can see the data from different categories in separate columns in the same row rather than in separate rows. The article from Craig is Pivoting in Postgres.
Pivoting a matrix, also known as a matrix transposition
Let&amp;rsquo;s do the same setup as he did, with a table containing some randomly generated data about hypothetical visits to a web page, say, by date then by operating system.</description>
    </item>
    
    <item>
      <title>Nearest Big City</title>
      <link>http://tapoueh.org/blog/2013/05/nearest-big-city/</link>
      <pubDate>Thu, 02 May 2013 11:34:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2013/05/nearest-big-city/</guid>
      <description>In this article, we want to find the town with the greatest number of inhabitants near a given location.
A very localized example We first need to find and import some data, and I found at the following place a CSV listing of french cities with coordinates and population and some numbers of interest for the exercise here.
To import the data set, we first need a table, then a COPY command:</description>
    </item>
    
    <item>
      <title>The Need For Speed</title>
      <link>http://tapoueh.org/blog/2013/03/the-need-for-speed/</link>
      <pubDate>Fri, 29 Mar 2013 09:49:00 +0100</pubDate>
      
      <guid>http://tapoueh.org/blog/2013/03/the-need-for-speed/</guid>
      <description>Hier se tenait la cinquième édition de la conférence organisée par dalibo, où des intervenants extérieurs sont régulièrement invités. Le thème hier était à la fois clair et très vaste : la performance.
  J&amp;rsquo;ai eu le plaisir de réaliser une présentation intitulée « The Need for Speed » dans laquelle on replace l&amp;rsquo;effort d&amp;rsquo;optimisation dans son contexte métier, afin de faire une étude des coûts et bénéfices et de savoir non seulement à quoi s&amp;rsquo;attendre mais aussi quand s&amp;rsquo;arrêter.</description>
    </item>
    
    <item>
      <title>Batch Update</title>
      <link>http://tapoueh.org/blog/2013/03/batch-update/</link>
      <pubDate>Fri, 15 Mar 2013 10:47:00 +0100</pubDate>
      
      <guid>http://tapoueh.org/blog/2013/03/batch-update/</guid>
      <description>Performance consulting involves some tricks that you have to teach over and over again. One of them is that SQL tends to be so much better at dealing with plenty of rows in a single statement when compared to running as many statements, each one against a single row.
Another kind of Batch to update
So when you need to UPDATE a bunch of rows from a given source, remember that you can actually use a JOIN in the update statement.</description>
    </item>
    
    <item>
      <title>HyperLogLog Unions</title>
      <link>http://tapoueh.org/blog/2013/02/hyperloglog-unions/</link>
      <pubDate>Tue, 26 Feb 2013 12:44:00 +0100</pubDate>
      
      <guid>http://tapoueh.org/blog/2013/02/hyperloglog-unions/</guid>
      <description>In the article from yesterday we talked about PostgreSQL HyperLogLog with some details. The real magic of that extension has been skimmed over though, and needs another very small article all by itself, in case you missed it.
Which Set Operation do you want for counting unique values?
The first query here has the default level of magic in it, really. What happens is that each time we do an update of the HyperLogLog hash value, we update some data which are allowing us to compute its cardinality.</description>
    </item>
    
    <item>
      <title>PostgreSQL HyperLogLog</title>
      <link>http://tapoueh.org/blog/2013/02/postgresql-hyperloglog/</link>
      <pubDate>Mon, 25 Feb 2013 10:23:00 +0100</pubDate>
      
      <guid>http://tapoueh.org/blog/2013/02/postgresql-hyperloglog/</guid>
      <description>If you&amp;rsquo;ve been following along at home the newer statistics developments, you might have heard about this new State of The Art Cardinality Estimation Algorithm called HyperLogLog. This technique is now available for PostgreSQL in the extension postgresql-hll available at https://github.com/aggregateknowledge/postgresql-hll and soon to be in debian.
How to Compute Cardinality?
Installing postgresql-hll It&amp;rsquo;s as simple as CREATE EXTENSION hll; really, even if to get there you must have installed the package on your system.</description>
    </item>
    
    <item>
      <title>Reset Counter</title>
      <link>http://tapoueh.org/blog/2012/10/reset-counter/</link>
      <pubDate>Fri, 05 Oct 2012 09:44:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2012/10/reset-counter/</guid>
      <description>I&amp;rsquo;ve been given a nice puzzle that I think is a good blog article opportunity, as it involves some thinking and window functions.
What&amp;rsquo;s to solve Say we store in a table entries from a counter that only increases and the time stamp when we did the measurement. So that when you read 30 then later 40 in fact that means we counted 10 more the second reading when compared to the first, in other words the first 30 are counted again in the second counter value, 40.</description>
    </item>
    
    <item>
      <title>Tables and Views dependencies</title>
      <link>http://tapoueh.org/blog/2011/05/tables-and-views-dependencies/</link>
      <pubDate>Wed, 04 May 2011 11:45:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2011/05/tables-and-views-dependencies/</guid>
      <description>Let&amp;rsquo;s say you need to ALTER TABLE foo ALTER COLUMN bar TYPE bigint;, and PostgreSQL is helpfully telling you that no you can&amp;rsquo;t because such and such views depend on the column. The basic way to deal with that is to copy paste from the error message the names of the views involved, then prepare a script wherein you first DROP VIEW ...; then ALTER TABLE and finally CREATE VIEW again, all in the same transaction.</description>
    </item>
    
    <item>
      <title>Dynamic Triggers in PLpgSQL</title>
      <link>http://tapoueh.org/blog/2010/11/dynamic-triggers-in-plpgsql/</link>
      <pubDate>Wed, 24 Nov 2010 16:45:00 +0100</pubDate>
      
      <guid>http://tapoueh.org/blog/2010/11/dynamic-triggers-in-plpgsql/</guid>
      <description>You certainly know that implementing dynamic triggers in PLpgSQL is impossible. But I had a very bad night, being up from as soon as 3:30 am today, so that when a developer asked me about reusing the same trigger function code from more than one table and for a dynamic column name, I didn&amp;rsquo;t remember about it being impossible.
Here&amp;rsquo;s what happens in such cases, after a long time on the problem (yes, overall, that&amp;rsquo;s a slow day).</description>
    </item>
    
    <item>
      <title>Window Functions example remix</title>
      <link>http://tapoueh.org/blog/2010/09/window-functions-example-remix/</link>
      <pubDate>Sun, 12 Sep 2010 21:35:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2010/09/window-functions-example-remix/</guid>
      <description>The drawback of hosting a static only website is, obviously, the lack of comments. What happens actually, though, is that I receive very few comments by direct mail. As I don&amp;rsquo;t get another spam source to cleanup, I&amp;rsquo;m left unconvinced that&amp;rsquo;s such a drawback. I still miss the low probability of seeing blog readers exchange directly, but I think a tapoueh.org mailing list would be my answer, here&amp;hellip;
Anyway, David Fetter took the time to send me a comment by mail with a cleaned up rewrite of the previous entry SQL, here&amp;rsquo;s it for your pleasure!</description>
    </item>
    
    <item>
      <title>Window Functions example</title>
      <link>http://tapoueh.org/blog/2010/09/window-functions-example/</link>
      <pubDate>Thu, 09 Sep 2010 16:35:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2010/09/window-functions-example/</guid>
      <description>So, when 8.4 came out there was all those comments about how getting window functions was an awesome addition. Now, it seems that a lot of people seeking for help in #postgresql just don&amp;rsquo;t know what kind of problem this feature helps solving. I&amp;rsquo;ve already been using them in some cases here in this blog, for getting some nice overview about Partitioning: relation size per “group”.
That&amp;rsquo;s another way to count change</description>
    </item>
    
    <item>
      <title>Happy Numbers</title>
      <link>http://tapoueh.org/blog/2010/08/happy-numbers/</link>
      <pubDate>Mon, 30 Aug 2010 11:00:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2010/08/happy-numbers/</guid>
      <description>After discovering the excellent Gwene service, which allows you to subscribe to newsgroups to read RSS content ( blogs, planets, commits, etc), I came to read this nice article about Happy Numbers. That&amp;rsquo;s a little problem that fits well an interview style question, so I first solved it yesterday evening in Emacs Lisp as that&amp;rsquo;s the language I use the most those days.
 A happy number is defined by the following process.</description>
    </item>
    
    <item>
      <title>Playing with bit strings</title>
      <link>http://tapoueh.org/blog/2010/08/playing-with-bit-strings/</link>
      <pubDate>Thu, 26 Aug 2010 17:45:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2010/08/playing-with-bit-strings/</guid>
      <description>The idea of the day ain&amp;rsquo;t directly from me, I&amp;rsquo;m just helping with a very thin subpart of the problem. The problem, I can&amp;rsquo;t say much about, let&amp;rsquo;s just assume you want to reduce the storage of MD5 in your database, so you want to abuse bit strings. A solution to use them works fine, but the datatype is still missing some facilities, for example going from and to hexadecimal representation in text.</description>
    </item>
    
    <item>
      <title>Partitioning: relation size per “group”</title>
      <link>http://tapoueh.org/blog/2010/07/partitioning-relation-size-per-group/</link>
      <pubDate>Mon, 26 Jul 2010 17:00:00 +0200</pubDate>
      
      <guid>http://tapoueh.org/blog/2010/07/partitioning-relation-size-per-group/</guid>
      <description>This time, we are trying to figure out where is the bulk of the data on disk. The trick is that we&amp;rsquo;re using DDL partitioning, but we want a “nice” view of size per partition set. Meaning that if you have for example a parent table foo with partitions foo_201006 and foo_201007, you would want to see a single category foo containing the accumulated size of all the partitions underneath foo.</description>
    </item>
    
    <item>
      <title>Finding orphaned sequences</title>
      <link>http://tapoueh.org/blog/2010/03/finding-orphaned-sequences/</link>
      <pubDate>Wed, 17 Mar 2010 13:35:00 +0100</pubDate>
      
      <guid>http://tapoueh.org/blog/2010/03/finding-orphaned-sequences/</guid>
      <description>This time we&amp;rsquo;re having a database where sequences were used, but not systematically as a default value of a given column. It&amp;rsquo;s mainly an historic bad idea, but you know the usual excuse with bad ideas and bad code: the first 6 months it&amp;rsquo;s experimental, after that it&amp;rsquo;s historic.
Not talking about genome orphaned sequences here, though
Still, here&amp;rsquo;s a query for 8.4 that will allow you to list those sequences you have that are not used as a default value in any of your tables:</description>
    </item>
    
    <item>
      <title>Importing XML content from file</title>
      <link>http://tapoueh.org/blog/2009/02/importing-xml-content-from-file/</link>
      <pubDate>Thu, 05 Feb 2009 00:00:00 +0100</pubDate>
      
      <guid>http://tapoueh.org/blog/2009/02/importing-xml-content-from-file/</guid>
      <description>The problem was raised this week on IRC and this time again I felt it would be a good occasion for a blog entry: how to load an XML file content into a single field?
The usual tool used to import files is COPY, but it&amp;rsquo;ll want each line of the file to host a text representation of a database tuple, so it doesn&amp;rsquo;t apply to the case at hand. RhodiumToad was online and offered the following code to solve the problem:</description>
    </item>
    
  </channel>
</rss>