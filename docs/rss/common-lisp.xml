<?xml version="1.0" encoding="utf-8"?>
<rss version='2.0' xmlns:atom='http://www.w3.org/2005/Atom'>
 <channel>
   <title>tail -f /dev/dim</title>
   <link>http://tapoueh.org/blog</link>
   <description>Dimitri Fontaine's blog</description>
   <language>en-us</language>
   <generator>Emacs Muse and Tapoueh's Common Lisp</generator>
   <atom:link href='http://tapoueh.org/rss/common-lisp.xml'
               rel='self'
              type='application/rss+xml' />

<item>
  <title>Quicklisp and debian
  </title>
  <link>http://tapoueh.org/blog/2015/05/02-ql-to-deb.html
  </link>
  <description><![CDATA[<p>Common Lisp users are very happy to use 
<a href='https://www.quicklisp.org/beta/'>Quicklisp</a> when it comes to
downloading and maintaining dependencies between their own code and the
<a href='http://quickdocs.org/'>librairies</a> it is using.
</p><center><img src='http://tapoueh.org/images/lisplogo-640.png' /></center><p>Sometimes I am pointed that when compared to other programming languages
Common Lisp is lacking a lot in the 
<em>batteries included</em> area. After having
had to 
<a href='https://qa.debian.org/developer.php?login=dim%40tapoueh.org'>package about 50 common lisp librairies for debian</a> I can tell you
that I politely disagree with that.
</p><p>And this post is about the tool and process I use to maintain all those librairies.
</p><p>Quicklisp is good at ensuring a proper distribution of all those libs it
supports and actually tests that they all compile and load together, so I've
been using it as my upstream for debian packaging purposes. Using Quicklisp
here makes my life much simpler as I can grovel through its metadata and
automate most of the maintenance of my cl related packages.
</p><center><img src='http://tapoueh.org/images/quicklisp.png' /></center><p>It's all automated in the 
<a href='https://github.com/dimitri/ql-to-deb'>ql-to-deb</a> software which, unsurprisingly, has been
written in Common Lisp itself. It's a kind of a Quicklisp client that will
fetch Quicklisp current list of 
<em>releases</em> with version numbers and compare to
the list of managed 
<em>packages</em> for debian in order to then 
<em>build</em> new version
automatically.
</p><p>The current workflow I'm using begins with using `ql-to-deb` is to `check`
for the work to be done today:
</p><pre><code>$ /vagrant/build/bin/ql-to-deb check
Fetching &quot;http://beta.quicklisp.org/dist/quicklisp.txt&quot;
Fetching &quot;http://beta.quicklisp.org/dist/quicklisp/2015-04-07/releases.txt&quot;
update: cl+ssl cl-csv cl-db3 drakma esrap graph hunchentoot local-time lparallel nibbles qmynd trivial-backtrace
upload: hunchentoot
</code></pre><p>After careful manual review of the automatic decision, let's just `update`
all what `check` decided would have to be:
</p><pre><code>$ /vagrant/build/bin/ql-to-deb update
Fetching &quot;http://beta.quicklisp.org/dist/quicklisp.txt&quot;
Fetching &quot;http://beta.quicklisp.org/dist/quicklisp/2015-04-07/releases.txt&quot;

Updating package cl-plus-ssl from 20140826 to 20150302.
     see logs in &quot;//tmp/ql-to-deb/logs//cl-plus-ssl.log&quot;
Fetching &quot;http://beta.quicklisp.org/archive/cl+ssl/2015-03-02/cl+ssl-20150302-git.tgz&quot;
Checksum test passed.
     File: &quot;/tmp/ql-to-deb/archives/cl+ssl-20150302-git.tgz&quot;
      md5: 61d9d164d37ab5c91048827dfccd6835
Building package cl-plus-ssl

Updating package cl-csv from 20140826 to 20150302.
     see logs in &quot;//tmp/ql-to-deb/logs//cl-csv.log&quot;
Fetching &quot;http://beta.quicklisp.org/archive/cl-csv/2015-03-02/cl-csv-20150302-git.tgz&quot;
Checksum test passed.
     File: &quot;/tmp/ql-to-deb/archives/cl-csv-20150302-git.tgz&quot;
      md5: 32f6484a899fdc5b690f01c244cd9f55
Building package cl-csv

Updating package cl-db3 from 20131111 to 20150302.
     see logs in &quot;//tmp/ql-to-deb/logs//cl-db3.log&quot;
Fetching &quot;http://beta.quicklisp.org/archive/cl-db3/2015-03-02/cl-db3-20150302-git.tgz&quot;
Checksum test passed.
     File: &quot;/tmp/ql-to-deb/archives/cl-db3-20150302-git.tgz&quot;
      md5: 578896a3f60f474742f240b703f8c5f5
Building package cl-db3

Updating package cl-drakma from 1.3.11 to 1.3.13.
     see logs in &quot;//tmp/ql-to-deb/logs//cl-drakma.log&quot;
Fetching &quot;http://beta.quicklisp.org/archive/drakma/2015-04-07/drakma-1.3.13.tgz&quot;
Checksum test passed.
     File: &quot;/tmp/ql-to-deb/archives/drakma-1.3.13.tgz&quot;
      md5: 3b548bce10728c7a058f19444c8477c3
Building package cl-drakma

Updating package cl-esrap from 20150113 to 20150302.
     see logs in &quot;//tmp/ql-to-deb/logs//cl-esrap.log&quot;
Fetching &quot;http://beta.quicklisp.org/archive/esrap/2015-03-02/esrap-20150302-git.tgz&quot;
Checksum test passed.
     File: &quot;/tmp/ql-to-deb/archives/esrap-20150302-git.tgz&quot;
      md5: 8b198d26c27afcd1e9ce320820b0e569
Building package cl-esrap

Updating package cl-graph from 20141106 to 20150407.
     see logs in &quot;//tmp/ql-to-deb/logs//cl-graph.log&quot;
Fetching &quot;http://beta.quicklisp.org/archive/graph/2015-04-07/graph-20150407-git.tgz&quot;
Checksum test passed.
     File: &quot;/tmp/ql-to-deb/archives/graph-20150407-git.tgz&quot;
      md5: 3894ef9262c0912378aa3b6e8861de79
Building package cl-graph

Updating package hunchentoot from 1.2.29 to 1.2.31.
     see logs in &quot;//tmp/ql-to-deb/logs//hunchentoot.log&quot;
Fetching &quot;http://beta.quicklisp.org/archive/hunchentoot/2015-04-07/hunchentoot-1.2.31.tgz&quot;
Checksum test passed.
     File: &quot;/tmp/ql-to-deb/archives/hunchentoot-1.2.31.tgz&quot;
      md5: 973eccfef87e81f1922424cb19884d63
Building package hunchentoot

Updating package cl-local-time from 20150113 to 20150407.
     see logs in &quot;//tmp/ql-to-deb/logs//cl-local-time.log&quot;
Fetching &quot;http://beta.quicklisp.org/archive/local-time/2015-04-07/local-time-20150407-git.tgz&quot;
Checksum test passed.
     File: &quot;/tmp/ql-to-deb/archives/local-time-20150407-git.tgz&quot;
      md5: 7be4a31d692f5862014426a53eb1e48e
Building package cl-local-time

Updating package cl-lparallel from 20141106 to 20150302.
     see logs in &quot;//tmp/ql-to-deb/logs//cl-lparallel.log&quot;
Fetching &quot;http://beta.quicklisp.org/archive/lparallel/2015-03-02/lparallel-20150302-git.tgz&quot;
Checksum test passed.
     File: &quot;/tmp/ql-to-deb/archives/lparallel-20150302-git.tgz&quot;
      md5: dbda879d0e3abb02a09b326e14fa665d
Building package cl-lparallel

Updating package cl-nibbles from 20141106 to 20150407.
     see logs in &quot;//tmp/ql-to-deb/logs//cl-nibbles.log&quot;
Fetching &quot;http://beta.quicklisp.org/archive/nibbles/2015-04-07/nibbles-20150407-git.tgz&quot;
Checksum test passed.
     File: &quot;/tmp/ql-to-deb/archives/nibbles-20150407-git.tgz&quot;
      md5: 2ffb26241a1b3f49d48d28e7a61b1ab1
Building package cl-nibbles

Updating package cl-qmynd from 20141217 to 20150302.
     see logs in &quot;//tmp/ql-to-deb/logs//cl-qmynd.log&quot;
Fetching &quot;http://beta.quicklisp.org/archive/qmynd/2015-03-02/qmynd-20150302-git.tgz&quot;
Checksum test passed.
     File: &quot;/tmp/ql-to-deb/archives/qmynd-20150302-git.tgz&quot;
      md5: b1cc35f90b0daeb9ba507fd4e1518882
Building package cl-qmynd

Updating package cl-trivial-backtrace from 20120909 to 20150407.
     see logs in &quot;//tmp/ql-to-deb/logs//cl-trivial-backtrace.log&quot;
Fetching &quot;http://beta.quicklisp.org/archive/trivial-backtrace/2015-04-07/trivial-backtrace-20150407-git.tgz&quot;
Checksum test passed.
     File: &quot;/tmp/ql-to-deb/archives/trivial-backtrace-20150407-git.tgz&quot;
      md5: 762b0acf757dc8a2a6812d2f0f2614d9
Building package cl-trivial-backtrace
</code></pre><p>Quite simple.
</p><p>To be totally honnest, I first had a problem with the parser generator
library 
<a href='http://nikodemus.github.io/esrap/'>esrap</a> wherein the 
<code>README</code> documentation changed to be a 
<code>README.org</code>
file, and I had to tell my debian packaging about that. See the
<a href='https://github.com/dimitri/ql-to-deb/commit/0ef669579cf7c07280eae7fe6f61f1bd664d337e'>0ef669579cf7c07280eae7fe6f61f1bd664d337e</a> commit to 
<em>ql-to-deb</em> for details.
</p><p>What about trying to install those packages locally? That's usually a very
good test. Sometimes some dependencies are missing at the 
<code>dpkg</code> command line,
so another 
<code>apt-get install -f</code> is needed:
</p><pre><code>$ /vagrant/build/bin/ql-to-deb install
sudo dpkg -i /tmp/ql-to-deb/cl-plus-ssl_20150302-1_all.deb /tmp/ql-to-deb/cl-csv_20150302-1_all.deb /tmp/ql-to-deb/cl-csv-clsql_20150302-1_all.deb /tmp/ql-to-deb/cl-csv-data-table_20150302-1_all.deb /tmp/ql-to-deb/cl-db3_20150302-1_all.deb /tmp/ql-to-deb/cl-drakma_1.3.13-1_all.deb /tmp/ql-to-deb/cl-esrap_20150302-1_all.deb /tmp/ql-to-deb/cl-graph_20150407-1_all.deb /tmp/ql-to-deb/cl-hunchentoot_1.2.31-1_all.deb /tmp/ql-to-deb/cl-local-time_20150407-1_all.deb /tmp/ql-to-deb/cl-lparallel_20150302-1_all.deb /tmp/ql-to-deb/cl-nibbles_20150407-1_all.deb /tmp/ql-to-deb/cl-qmynd_20150302-1_all.deb /tmp/ql-to-deb/cl-trivial-backtrace_20150407-1_all.deb
(Reading database ... 79689 files and directories currently installed.)
Preparing to unpack .../cl-plus-ssl_20150302-1_all.deb ...
Unpacking cl-plus-ssl (20150302-1) over (20140826-1) ...
Selecting previously unselected package cl-csv.
Preparing to unpack .../cl-csv_20150302-1_all.deb ...
Unpacking cl-csv (20150302-1) ...
Selecting previously unselected package cl-csv-clsql.
Preparing to unpack .../cl-csv-clsql_20150302-1_all.deb ...
Unpacking cl-csv-clsql (20150302-1) ...
Selecting previously unselected package cl-csv-data-table.
Preparing to unpack .../cl-csv-data-table_20150302-1_all.deb ...
Unpacking cl-csv-data-table (20150302-1) ...
Selecting previously unselected package cl-db3.
Preparing to unpack .../cl-db3_20150302-1_all.deb ...
Unpacking cl-db3 (20150302-1) ...
Preparing to unpack .../cl-drakma_1.3.13-1_all.deb ...
Unpacking cl-drakma (1.3.13-1) over (1.3.11-1) ...
Preparing to unpack .../cl-esrap_20150302-1_all.deb ...
Unpacking cl-esrap (20150302-1) over (20150113-1) ...
Preparing to unpack .../cl-graph_20150407-1_all.deb ...
Unpacking cl-graph (20150407-1) over (20141106-1) ...
Preparing to unpack .../cl-hunchentoot_1.2.31-1_all.deb ...
Unpacking cl-hunchentoot (1.2.31-1) over (1.2.29-1) ...
Preparing to unpack .../cl-local-time_20150407-1_all.deb ...
Unpacking cl-local-time (20150407-1) over (20150113-1) ...
Preparing to unpack .../cl-lparallel_20150302-1_all.deb ...
Unpacking cl-lparallel (20150302-1) over (20141106-1) ...
Preparing to unpack .../cl-nibbles_20150407-1_all.deb ...
Unpacking cl-nibbles (20150407-1) over (20141106-1) ...
Preparing to unpack .../cl-qmynd_20150302-1_all.deb ...
Unpacking cl-qmynd (20150302-1) over (20141217-1) ...
Preparing to unpack .../cl-trivial-backtrace_20150407-1_all.deb ...
Unpacking cl-trivial-backtrace (20150407-1) over (20120909-2) ...
Setting up cl-plus-ssl (20150302-1) ...
dpkg: dependency problems prevent configuration of cl-csv:
 cl-csv depends on cl-interpol; however:
  Package cl-interpol is not installed.

dpkg: error processing package cl-csv (--install):
 dependency problems - leaving unconfigured
dpkg: dependency problems prevent configuration of cl-csv-clsql:
 cl-csv-clsql depends on cl-csv; however:
  Package cl-csv is not configured yet.

dpkg: error processing package cl-csv-clsql (--install):
 dependency problems - leaving unconfigured
dpkg: dependency problems prevent configuration of cl-csv-data-table:
 cl-csv-data-table depends on cl-csv; however:
  Package cl-csv is not configured yet.

dpkg: error processing package cl-csv-data-table (--install):
 dependency problems - leaving unconfigured
Setting up cl-db3 (20150302-1) ...
Setting up cl-drakma (1.3.13-1) ...
Setting up cl-esrap (20150302-1) ...
Setting up cl-graph (20150407-1) ...
Setting up cl-local-time (20150407-1) ...
Setting up cl-lparallel (20150302-1) ...
Setting up cl-nibbles (20150407-1) ...
Setting up cl-qmynd (20150302-1) ...
Setting up cl-trivial-backtrace (20150407-1) ...
Setting up cl-hunchentoot (1.2.31-1) ...
Errors were encountered while processing:
 cl-csv
 cl-csv-clsql
 cl-csv-data-table
</code></pre><p>Let's make sure that our sid users will be happy with the update here:
</p><pre><code>$ sudo apt-get install -f
Reading package lists... Done
Building dependency tree       
Reading state information... Done
Correcting dependencies... Done
The following packages were automatically installed and are no longer required:
  g++-4.7 git git-man html2text libaugeas-ruby1.8 libbind9-80
  libclass-isa-perl libcurl3-gnutls libdns88 libdrm-nouveau1a
  libegl1-mesa-drivers libffi5 libgraphite3 libgssglue1 libisc84 libisccc80
  libisccfg82 liblcms1 liblwres80 libmpc2 libopenjpeg2 libopenvg1-mesa
  libpoppler19 librtmp0 libswitch-perl libtiff4 libwayland-egl1-mesa luatex
  openssh-blacklist openssh-blacklist-extra python-chardet python-debian
  python-magic python-pkg-resources python-six ttf-dejavu-core ttf-marvosym
Use &#039;apt-get autoremove&#039; to remove them.
The following extra packages will be installed:
  cl-interpol
The following NEW packages will be installed:
  cl-interpol
0 upgraded, 1 newly installed, 0 to remove and 51 not upgraded.
3 not fully installed or removed.
Need to get 20.7 kB of archives.
After this operation, 135 kB of additional disk space will be used.
Do you want to continue? [Y/n] 
Get:1 http://ftp.fr.debian.org/debian/ sid/main cl-interpol all 0.2.1-2 [20.7 kB]
Fetched 20.7 kB in 0s (84.5 kB/s)
debconf: unable to initialize frontend: Dialog
debconf: (Dialog frontend will not work on a dumb terminal, an emacs shell buffer, or without a controlling terminal.)
debconf: falling back to frontend: Readline
Selecting previously unselected package cl-interpol.
(Reading database ... 79725 files and directories currently installed.)
Preparing to unpack .../cl-interpol_0.2.1-2_all.deb ...
Unpacking cl-interpol (0.2.1-2) ...
Setting up cl-interpol (0.2.1-2) ...
Setting up cl-csv (20150302-1) ...
Setting up cl-csv-clsql (20150302-1) ...
Setting up cl-csv-data-table (20150302-1) ...
</code></pre><p>All looks fine, time to 
<em>sign</em> those packages. There's a trick here, where you
want to be sure you're using a GnuPG setup that allows you to enter your
passphrase only once, see 
<a href='https://github.com/dimitri/ql-to-deb/tree/master/conf'>ql-to-deb vm setup</a> for details, and the usual
documentations about all that if you're interested into the details.
</p><pre><code>$ /vagrant/build/bin/ql-to-deb sign
 signfile /tmp/ql-to-deb/cl-plus-ssl_20150302-1.dsc 60B1CB4E
 signfile /tmp/ql-to-deb/cl-plus-ssl_20150302-1_amd64.changes 60B1CB4E
Successfully signed dsc and changes files
 signfile /tmp/ql-to-deb/cl-csv_20150302-1.dsc 60B1CB4E
 signfile /tmp/ql-to-deb/cl-csv_20150302-1_amd64.changes 60B1CB4E
Successfully signed dsc and changes files
 signfile /tmp/ql-to-deb/cl-db3_20150302-1.dsc 60B1CB4E
 signfile /tmp/ql-to-deb/cl-db3_20150302-1_amd64.changes 60B1CB4E
Successfully signed dsc and changes files
 signfile /tmp/ql-to-deb/cl-drakma_1.3.13-1.dsc 60B1CB4E
 signfile /tmp/ql-to-deb/cl-drakma_1.3.13-1_amd64.changes 60B1CB4E
Successfully signed dsc and changes files
 signfile /tmp/ql-to-deb/cl-esrap_20150302-1.dsc 60B1CB4E
 signfile /tmp/ql-to-deb/cl-esrap_20150302-1_amd64.changes 60B1CB4E
Successfully signed dsc and changes files
 signfile /tmp/ql-to-deb/cl-graph_20150407-1.dsc 60B1CB4E
 signfile /tmp/ql-to-deb/cl-graph_20150407-1_amd64.changes 60B1CB4E
Successfully signed dsc and changes files
 signfile /tmp/ql-to-deb/hunchentoot_1.2.31-1.dsc 60B1CB4E
 signfile /tmp/ql-to-deb/hunchentoot_1.2.31-1_amd64.changes 60B1CB4E
Successfully signed dsc and changes files
 signfile /tmp/ql-to-deb/cl-local-time_20150407-1.dsc 60B1CB4E
 signfile /tmp/ql-to-deb/cl-local-time_20150407-1_amd64.changes 60B1CB4E
Successfully signed dsc and changes files
 signfile /tmp/ql-to-deb/cl-lparallel_20150302-1.dsc 60B1CB4E
 signfile /tmp/ql-to-deb/cl-lparallel_20150302-1_amd64.changes 60B1CB4E
Successfully signed dsc and changes files
 signfile /tmp/ql-to-deb/cl-nibbles_20150407-1.dsc 60B1CB4E
 signfile /tmp/ql-to-deb/cl-nibbles_20150407-1_amd64.changes 60B1CB4E
Successfully signed dsc and changes files
 signfile /tmp/ql-to-deb/cl-qmynd_20150302-1.dsc 60B1CB4E
 signfile /tmp/ql-to-deb/cl-qmynd_20150302-1_amd64.changes 60B1CB4E
Successfully signed dsc and changes files
 signfile /tmp/ql-to-deb/cl-trivial-backtrace_20150407-1.dsc 60B1CB4E
 signfile /tmp/ql-to-deb/cl-trivial-backtrace_20150407-1_amd64.changes 60B1CB4E
Successfully signed dsc and changes files
</code></pre><p>Ok, with all tested and signed, it's time we 
<em>upload</em> our packages on debian
servers for our dear debian users to be able to use newer and better
versions of their beloved Common Lisp librairies:
</p><pre><code>$ /vagrant/build/bin/ql-to-deb upload
Trying to upload package to ftp-master (ftp.upload.debian.org)
Checking signature on .changes
gpg: Signature made Sat 02 May 2015 05:06:48 PM MSK using RSA key ID 60B1CB4E
gpg: Good signature from &quot;Dimitri Fontaine &lt;dim@tapoueh.org&gt;&quot;
Good signature on /tmp/ql-to-deb/cl-plus-ssl_20150302-1_amd64.changes.
Checking signature on .dsc
gpg: Signature made Sat 02 May 2015 05:06:46 PM MSK using RSA key ID 60B1CB4E
gpg: Good signature from &quot;Dimitri Fontaine &lt;dim@tapoueh.org&gt;&quot;
Good signature on /tmp/ql-to-deb/cl-plus-ssl_20150302-1.dsc.
Uploading to ftp-master (via ftp to ftp.upload.debian.org):
  Uploading cl-plus-ssl_20150302-1.dsc: done.
  Uploading cl-plus-ssl_20150302.orig.tar.gz: done.
  Uploading cl-plus-ssl_20150302-1.debian.tar.xz: done.
  Uploading cl-plus-ssl_20150302-1_all.deb: done.
  Uploading cl-plus-ssl_20150302-1_amd64.changes: done.
Successfully uploaded packages.
</code></pre><p>Of course the same text or abouts is then repeated for all the other packages.
</p><p>Enjoy using Common Lisp in debian!
</p><center><img src='http://tapoueh.org/images/toy-loader.640.jpg' /></center><p>Oh and remember, the only reason I've written 
<a href='https://github.com/dimitri/ql-to-deb'>ql-to-deb</a> and signed myself up
to maintain those upteens Common Lisp librairies as debian package is to be
able to properly package 
<a href='http://pgloader.io/'>pgloader</a> in debian, as you can see at
<a href='https://packages.debian.org/sid/pgloader'>https://packages.debian.org/sid/pgloader</a> and in particular in the 
<em>Other
Packages Related to pgloader</em> section of the debian source package for
pgloader at 
<a href='https://packages.debian.org/source/sid/pgloader'>https://packages.debian.org/source/sid/pgloader</a>.
</p><p>That level of effort is done to ensure that we respect the
<a href='https://www.debian.org/social_contract'>Debian Social Contract</a> wherein debian ensures its users that it's possible
to rebuild anything from sources as found in the debian repositories.
</p>]]>
  </description>
  <author>dim@tapoueh.org (Dimitri Fontaine)
  </author><pubDate>Sat, 02 May 2015 16:06:00 +0200</pubDate><guid isPermaLink="true">http://tapoueh.org/blog/2015/05/02-ql-to-deb.html</guid>
</item>

<item>
  <title>My First Slashdot Effect
  </title>
  <link>http://tapoueh.org/blog/2015/01/22-my-first-slashdot-effect.html
  </link>
  <description><![CDATA[<p>Thanks to the 
<a href='http://postgresweekly.com/issues/89'>Postgres Weekly issue #89</a> and a post to 
<a href='https://news.ycombinator.com/news'>Hacker News</a> front page
(see 
<a href='https://news.ycombinator.com/item?id=8924270'>Pgloader: A High-speed PostgreSQL Swiss Army Knife, Written in Lisp</a> it
well seems that I just had my first 
<a href='http://en.wikipedia.org/wiki/Slashdot_effect'>Slashdot effect</a>...
</p><center><img src='http://tapoueh.org/images/wi-killservers.jpg' /></center><center><em>Well actually you know what? I don't...</em></center><p>So please consider using the new mirror 
<a href='http://dimitri.github.io/pgloader/'>http://dimitri.github.io/pgloader/</a>
and maybe voting on 
<em>Hacker News</em> for either tooling around
<a href='http://postgresql.org'>your favorite database system, PostgreSQL</a> or
<a href='http://www.gigamonkeys.com/book/introduction-why-lisp.html'>your favorite programming language, Common Lisp</a>...
</p><p>It all happens at 
<a href='https://news.ycombinator.com/item?id=8924270'>https://news.ycombinator.com/item?id=8924270</a>.
</p><h2>Coming to FOSDEM?</h2><p>If you want to know more about 
<em>pgloader</em> and are visiting 
<a href='http://fosdem2015.pgconf.eu/'>FOSDEM PGDAY</a> or
plain 
<a href='https://fosdem.org/2015/'>FOSDEM</a> I'll be there talking about
<a href='http://www.postgresql.eu/events/schedule/fosdem2015/session/827-migrating-to-postgresql-the-new-story/'>Migrating to PostgreSQL, the new story</a> (that's 
<em>pgloader</em>) and about some more
reasons why 
<a href='https://fosdem.org/2015/schedule/event/youd_better_have_tested_backups/'>You'd better have tested backups...</a>
</p><p>If you're not there on the Friday but still want to talk about 
<em>pgloader</em>,
join us at the PostgreSQL devroom and booth!
</p>]]>
  </description>
  <author>dim@tapoueh.org (Dimitri Fontaine)
  </author><pubDate>Thu, 22 Jan 2015 01:48:00 +0100</pubDate><guid isPermaLink="true">http://tapoueh.org/blog/2015/01/22-my-first-slashdot-effect.html</guid>
</item>

<item>
  <title>New release: pgloader 3.2
  </title>
  <link>http://tapoueh.org/blog/2015/01/16-pgloader-3-2.html
  </link>
  <description><![CDATA[<p>PostgreSQL comes with an awesome bulk copy protocol and tooling best known
as the 
<a href='http://www.postgresql.org/docs/current/static/sql-copy.html'>COPY</a> and 
<code>\copy</code> commands. Being a transactional system, PostgreSQL
COPY implementation will 
<code>ROLLBACK</code> any work done if a single error is found
in the data set you're importing. That's the reason why 
<a href='http://pgloader.io/'>pgloader</a> got
started: it provides with error handling for the 
<a href='http://www.postgresql.org/docs/9.3/static/protocol-flow.html#PROTOCOL-COPY'>COPY protocol</a>.
</p><center><img src='http://tapoueh.org/images/keep-calm-and-on-error-resume-next.png' /></center><center><em>That's basically what pgloader used to be all about</em></center><p>As soon as we have the capability to load data from unreliable sources,
another use case appears on the horizon, and soon enough 
<a href='http://pgloader.io/'>pgloader</a> grew the
capacity to load data from other databases, some having a more liberal
notion of what is sane data type input.
</p><p>To be able to adapt to advanced use cases in database data migration
support, pgloader has grown an advanced command language wherein you can
define your own load-time data projection and transformations, and your own
type casting rules too.
</p><p>New in 
<em>version 3.2</em> is that in simple cases, you don't need that command file
any more. Check out the 
<a href='http://pgloader.io/howto/quickstart.html'>pgloader quick start</a> page to see some examples where
you can use pgloader all from your command line!
</p><p>Here's one such example, migrating a whole MySQL database data set over to
PostgreSQL, including automated schema discovery, automated type casting and
on-the-fly data cleanup (think about zero dates or booleans in 
<code>tinyint(1)</code>
disguise), support for indexes, primary keys, foreign keys and comments.
It's as simple as:
</p><pre><code>$ createdb sakila
$ pgloader mysql://root@localhost/sakila pgsql:///sakila
2015-01-16T09:49:36.068000+01:00 LOG Main logs in &#039;/private/tmp/pgloader/pgloader.log&#039;
2015-01-16T09:49:36.074000+01:00 LOG Data errors in &#039;/private/tmp/pgloader/&#039;
                    table name       read   imported     errors            time
------------------------------  ---------  ---------  ---------  --------------
               fetch meta data         43         43          0          0.222s
                  create, drop          0         36          0          0.130s
------------------------------  ---------  ---------  ---------  --------------
                         actor        200        200          0          0.133s
                       address        603        603          0          0.035s
                      category         16         16          0          0.027s
                          city        600        600          0          0.018s
                       country        109        109          0          0.017s
                      customer        599        599          0          0.035s
                          film       1000       1000          0          0.075s
                    film_actor       5462       5462          0          0.147s
                 film_category       1000       1000          0          0.035s
                     film_text       1000       1000          0          0.053s
                     inventory       4581       4581          0          0.086s
                      language          6          6          0          0.041s
                       payment      16049      16049          0          0.436s
                        rental      16044      16044          0          0.474s
                         staff          2          2          0          0.170s
                         store          2          2          0          0.010s
        Index Build Completion          0          0          0          0.000s
------------------------------  ---------  ---------  ---------  --------------
                Create Indexes         40         40          0          0.343s
               Reset Sequences          0         13          0          0.026s
                  Primary Keys         16         14          2          0.013s
                  Foreign Keys         22         22          0          0.078s
                      Comments          0          0          0          0.000s
------------------------------  ---------  ---------  ---------  --------------
             Total import time      47273      47273          0          2.261s
</code></pre><p>Other options are available to support a variety of input file formats,
including compressed csv files found on a remote location, as in:
</p><pre><code>curl http://pgsql.tapoueh.org/temp/2013_Gaz_113CDs_national.txt.gz \
    | gunzip -c                                                        \
    | pgloader --type csv                                              \
               --field &quot;usps,geoid,aland,awater,aland_sqmi,awater_sqmi,intptlat,intptlong&quot; \
               --with &quot;skip header = 1&quot;                                \
               --with &quot;fields terminated by &#039;\t&#039;&quot;                      \
               -                                                       \
               postgresql:///pgloader?districts_longlat

2015-01-16T10:09:06.027000+01:00 LOG Main logs in &#039;/private/tmp/pgloader/pgloader.log&#039;
2015-01-16T10:09:06.032000+01:00 LOG Data errors in &#039;/private/tmp/pgloader/&#039;
                    table name       read   imported     errors            time
------------------------------  ---------  ---------  ---------  --------------
                         fetch          0          0          0          0.010s
------------------------------  ---------  ---------  ---------  --------------
             districts_longlat        440        440          0          0.087s
------------------------------  ---------  ---------  ---------  --------------
             Total import time        440        440          0          0.097s
</code></pre><p>As usual in unix commands, the 
<code>-</code> input filename stands for 
<em>standard input</em>
and allows streaming data from a remote compressed file down to PostgreSQL.
</p><p>So if you have any data loading job, including data migrations from SQLite,
MySQL or MS SQL server: have a look at 
<a href='http://pgloader.io/'>pgloader</a>!
</p>]]>
  </description>
  <author>dim@tapoueh.org (Dimitri Fontaine)
  </author><pubDate>Fri, 16 Jan 2015 09:35:00 +0100</pubDate><guid isPermaLink="true">http://tapoueh.org/blog/2015/01/16-pgloader-3-2.html</guid>
</item>

<item>
  <title>Turn your PostgreSQL queries into Charts
  </title>
  <link>http://tapoueh.org/blog/2014/08/25-pgcharts.html
  </link>
  <description><![CDATA[<p>Earlier this year we did compare compare
<a href='/blog/2014/02/17-aggregating-nba-data-PostgreSQL-vs-MongoDB'>Aggregating NBA data, PostgreSQL vs MongoDB</a> then talked about
<a href='/blog/2014/02/21-PostgreSQL-histogram'>PostgreSQL, Aggregates and histograms</a> where we even produced a nice
<em>Histogram</em> chart directly within the awesome 
<code>psql</code> console. Today, let's get
that same idea to the next level, with 
<a href='https://github.com/dimitri/pgcharts'>pgcharts</a>:
</p><center><a href='https://github.com/dimitri/pgcharts'><img src='http://tapoueh.org/images/pgcharts-chart.640.png' /></a></center><center><em>The new <a href='https://github.com/dimitri/pgcharts'>pgcharts</a> application</em></center><p>The application's specifications are quite simple: edit an SQL query, set
your 
<em>categories</em> and your 
<em>data series</em>, add in some 
<em>legends</em>, and get a nice
chart. Currently supported are 
<em>bar</em>, 
<em>column</em>, 
<em>pie</em> and 
<em>donut</em> charts, and we
should be able to add anything that 
<a href='Highcharts'>http://www.highcharts.com/</a> has support
for.
</p><center><img src='http://tapoueh.org/images/pgcharts-query.640.png' /></center><p>Currently, you need to compile the application yourself, and for that you
need to install the 
<a href='http://sbcl.org/platform-table.html'>SBCL</a> compiler. Soon enough you will have a 
<em>debian
package</em> to play with! The 
<a href='https://github.com/dimitri/pgcharts/blob/master/README.md'>README</a> at the 
<a href='https://github.com/dimitri/pgcharts'>pgcharts github place</a> has the
details to get you started. Enjoy!
</p>]]>
  </description>
  <author>dim@tapoueh.org (Dimitri Fontaine)
  </author><pubDate>Mon, 25 Aug 2014 14:09:00 +0200</pubDate><guid isPermaLink="true">http://tapoueh.org/blog/2014/08/25-pgcharts.html</guid>
</item>

<item>
  <title>Why is pgloader so much faster?
  </title>
  <link>http://tapoueh.org/blog/2014/05/14-pgloader-got-faster.html
  </link>
  <description><![CDATA[<p><a href='http://pgloader.io/'>pgloader</a> loads data into PostgreSQL. The new version is stable enough
nowadays that it's soon to be released, the last piece of the 
<code>3.1.0</code> puzzle
being full 
<a href='https://www.debian.org/'>debian</a> packaging of the tool.
</p><center><img src='http://tapoueh.org/images/toy-loader.320.jpg' /></center><center><em>The pgloader logo is a loader truck, just because</em>.</center><p>As you might have noticed if you've read my blog before, I decided that
<a href='http://pgloader.io/'>pgloader</a> needed a full rewrite in order for it to be able to enter the
current decade as a relevant tool. pgloader used to be written in the
<a href='https://www.python.org/'>python programming language</a>, which is used by lots of people and generally
quite appreciated by its users.
</p><h2>Why changing</h2><p>Still, python is not without problems, the main ones I had to deal with
being 
<em>poor performances</em> and lack of threading capabilities. Also, the
pgloader setup design was pretty hard to maintain, and adding compatiblity
to other 
<em>loader</em> products from competitors was harder than it should.
</p><p>As I said in my 
<a href='http://tapoueh.org/confs/2014/05/05-ELS-2014'>pgloader lightning talk</a> at the 
<a href='http://www.european-lisp-symposium.org/'>7th European Lisp Symposium</a>
last week, in searching for a 
<em><strong>modern programming language</strong></em> the best candidate
I found was actually 
<a href='http://en.wikipedia.org/wiki/Common_Lisp'>Common Lisp</a>.
</p><center><a href='http://tapoueh.org/images/confs/ELS_2014_pgloader.pdf'><img src='http://tapoueh.org/images/confs/ELS_2014_pgloader.png' /></a></center><p>After some basic performances checking as seen in my
<a href='https://github.com/dimitri/sudoku'>Common Lisp Sudoku Solver</a> project where I did get up to 
<em><strong>ten times faster</strong></em>
code when compared to python, it felt like the amazing set of features of
the language could be put to good use here.
</p><h2>So, what about performances after rewrite?</h2><p>The main reason why I'm now writing this blog post is receiving emails from
pgloader users with strange feelings about the speedup. Let's see at the
numbers one user gave me, for some data point:
</p><pre><code>&#xA0;select rows, v2, v3,
        round((  extract(epoch from v2)
               / extract(epoch from v3))::numeric, 2) as speedup
   from timing;
        
  rows   |        v2         |       v3        | speedup 
---------+-------------------+-----------------+---------
 4768765 | @ 37 mins 10.878  | @ 1 min 26.917  |   25.67
 3115880 | @ 36 mins 5.881   | @ 1 min 10.994  |   30.51
 3865750 | @ 33 mins 40.233  | @ 1 min 15.33   |   26.82
 3994483 | @ 29 mins 30.028  | @ 1 min 18.484  |   22.55
(4 rows)
</code></pre><center><em>The raw numbers have been loaded into a PostgreSQL table</em></center><p>So what we see in this quite typical 
<a href='http://pgloader.io/howto/csv.html'>CSV Loading</a> test case is a best case of
<em><strong>30 times faster</strong></em> import. Which brings some questions on the table, of course.
</p><h2>Wait, you're still using <code>COPY</code> right?</h2><p>The 
<a href='http://www.postgresql.org/docs/9.3/interactive/index.html'>PostgreSQL</a> database system provides a really neat 
<a href='http://www.postgresql.org/docs/9.3/interactive/sql-copy.html'>COPY</a> command, which in
turn is only exposing the 
<a href='http://www.postgresql.org/docs/9.3/static/protocol-flow.html#PROTOCOL-COPY'>COPY Streaming Protocol</a>, that pgloader is using.
</p><p>So yes, 
<a href='http://pgloader.io/'>pgloader</a> is still using 
<code>COPY</code>. This time the protocol implementation
is to be found in the Common Lisp 
<a href='http://marijnhaverbeke.nl/postmodern/'>Postmodern</a> driver, which is really great.
Before that, back when pgloader was python code, it was using the very good
<a href='http://initd.org/psycopg/'>psycopg</a> driver, which also exposes the COPY protocol.
</p><h2>So, what did happen here?</h2><p>Well it happens that pgloader is now built using Common Lisp technologies,
and those are really great, powerful and fast!
</p><p>Not only is Common Lisp code compiled to 
<em>machine code</em> when using most
<a href='http://cliki.net/Common%20Lisp%20implementation'>Common Lisp Implementations</a> such as 
<a href='http://sbcl.org/'>SBCL</a> or 
<a href='http://ccl.clozure.com/'>Clozure Common Lisp</a>; it's also
possible to actually benefit from 
<em>parallel computing</em> and 
<em>threads</em> in Common
Lisp.
</p><center><img src='http://tapoueh.org/images/speedup.jpg' /></center><center><em>That's not how I did it!</em></center><p>In the 
<a href='http://pgloader.io/'>pgloader</a> case I've been using the 
<a href='http://lparallel.org/'>lparallel</a> utilities, in particular
its 
<a href='http://lparallel.org/api/queues/'>queuing facility</a> to be able to implement 
<em>asynchronous IOs</em> where a thread
reads the source data and preprocess it, fills up a batch at a time in a
buffer that is then pushed down to the writer thread, that handles the 
<code>COPY</code>
protocol and operations.
</p><p>So my current analysis is that the new thread based architecture used with a
very powerful compiler for the Common Lisp high-level language are allowing
pgloader to enter a whole new field of 
<em>data loading performances</em>.
</p><h2>Conclusion</h2><p>Not only is pgloader so much faster now, it's also full of new capabilities
and supports several sources of data such as 
<a href='http://pgloader.io/howto/dBase.html'>dBase files</a>,
<a href='http://pgloader.io/howto/sqlite.html'>SQLite database files</a> or even 
<a href='http://pgloader.io/howto/mysql.html'>MySQL live connections</a>.
</p><p>Rather than a configuration file, the way to use the new pgloader is using a
<em>command language</em> that has been designed to look as much like SQL as possible
in the pgloader context, to make it easy for its users. Implementation wise,
it should now be trivial enough to implement compatibility with other 
<em>data
load</em> software that some 
<a href='http://www.postgresql.org/'>PostgreSQL</a> competitor products do have.
</p><p>Also, the new code base and feature set seems to attract way more users than
the previous implementation ever did, despite using a less popular
programming language.
</p><p>You can already 
<a href='http://pgloader.io/download.html'>download pgloader binary packages</a> for 
<em>debian</em> based
distributions and 
<em>centos</em> based ones too, and you will even find a 
<em>Mac OS X</em>
package file (
<code>.pkg</code>) that will make 
<code>/usr/local/bin/pgloader</code> available for you
on the command line. If you need a windows binary, drop me an email.
</p><p>The first stable release of the new 
<a href='http://pgloader.io/'>pgloader</a> utility is scheduled to be
named 
<code>3.1.0</code> and to happen quite soon. We are hard at work on packaging the
dependencies for 
<em>debian</em>, and you can have a look at the 
<a href='https://github.com/dimitri/ql-to-deb'>Quicklisp to debian</a>
project if you want to help us get there!
</p>]]>
  </description>
  <author>dim@tapoueh.org (Dimitri Fontaine)
  </author><pubDate>Wed, 14 May 2014 14:59:00 +0200</pubDate><guid isPermaLink="true">http://tapoueh.org/blog/2014/05/14-pgloader-got-faster.html</guid>
</item>

<item>
  <title>Import fixed width data with pgloader
  </title>
  <link>http://tapoueh.org/blog/2013/11/18-pgloader-fixed-width-data.html
  </link>
  <description><![CDATA[<p>A long time ago we talked about how to
<a href='http://tapoueh.org/blog/2010/04/27-import-fixed-width-data-with-pgloader.html'>Import fixed width data with pgloader</a>, following up on other stories still
online at 
<a href='http://www.postgresonline.com/journal/index.php?/archives/157-Import-fixed-width-data-into-PostgreSQL-with-just-PSQL.html'>Postgres OnLine Journal</a> and on 
<a href='http://people.planetpostgresql.org/dfetter/index.php?/archives/58-psql,-Paste,-Perl-Pefficiency!.html'>David Fetter's blog</a>. Back then, I
showed that using pgloader made it easier to import the data, but also
showed quite poor performances characteristics due to using the 
<code>debug</code> mode
in the timings. Let's update that article with current 
<a href='https://github.com/dimitri/pgloader'>pgloader</a> wonders!
</p><center><img src='http://tapoueh.org/images/fixed_text_read.png' /></center><h2>Redoing the python based test</h2><p>Let's be fair, hardware did evolve in those past 3 years, and the test that
ran in 14 seconds was done with debug information level, which is the wrong
way to do tests.
</p><pre><code>$ ~/dev/pgloader-v2/pgloader.py -R. -Tsc census.conf 

Table name        |    duration |    size |  copy rows |     errors 
====================================================================
fixed             |      1.834s |       - |      25375 |          0
</code></pre><p>I got timings anywhere betseen 
<em><strong>1.5 seconds</strong></em> and 
<em><strong>1.834 seconds</strong></em> here.
</p><h2>The new pgloader</h2><p>Now with the current version of pgloader, what do we get:
</p><pre><code>$ pgloader.exe census-place.load
2013-11-18T12:02:35.001000+01:00 LOG Starting pgloader, log system is ready.
2013-11-18T12:02:35.003000+01:00 LOG Parsing commands from file &quot;/Users/dim/dev/pgloader/test/census-places.load&quot;

                    table name       read   imported     errors            time
------------------------------  ---------  ---------  ---------  --------------
                      download          0          0          0          1.587s
                       extract          0          0          0          1.010s
                   before load          0          0          0          0.014s
------------------------------  ---------  ---------  ---------  --------------
                        places      25375      25375          0          0.366s
------------------------------  ---------  ---------  ---------  --------------
             Total import time      25375      25375          0          2.977s
</code></pre><p>So the loading itself took as much as 
<em><strong>366 milliseconds</strong></em> to run. To be fair
that's kind of a best run, with run times varying between that and 
<em><strong>700
milliseconds</strong></em>.
</p><center><img src='http://tapoueh.org/images/mph.jpg' /></center><p>So the new version is about 
<em><strong>3 to 9 times faster</strong></em> depending on the story you
want to tell. Let's stick with 
<em>much faster</em> for the sake of this article.
</p><h2>The command</h2><p>The new loader takes a full command as its input, rather than a
configuration file. Here's what the command I've used this time looks like:
</p><pre><code>LOAD ARCHIVE
   FROM http://www.census.gov/geo/maps-data/data/docs/gazetteer/places2k.zip
   INTO postgresql:///pgloader

   BEFORE LOAD DO
     $$ drop table if exists places; $$,
     $$ create table places
       (
          usps      char(2)  not null,
          fips      char(2)  not null,
          fips_code char(5),
          loc_name  varchar(64)
       );
     $$

   LOAD FIXED
        FROM FILENAME MATCHING ~/places2k.txt/
             WITH ENCODING latin-1
             (
             -- name   start  length
                usps       0  2,
                fips       2  2,
                fips_code  4  5,
                loc_name   9 64,
                p         73  9,
                h         82  9,
                land      91 14,
                water    105 14,
                ldm      119 14,
                wtm      131 14,
                lat      143 10,
                long     153 11
             )
        INTO postgresql:///pgloader?places
             (
	        usps, fips, fips_code,
                loc_name text using (right-trim loc_name)
             );
</code></pre><p>First, the URL used in the blog post of april 2010 is no longer valid. You
can find a list of other interesting data at the 
<a href='http://www.census.gov/geo/maps-data/data/gazetteer2000.html'>Census 2000 Gazetteer Files</a>
page, and of course you have more recent version of the data available. In
another format entirely, this time tab-based csv-like, so better for general
usage, but not for this test where I wanted to reuse the same data source as
3 years ago.
</p><p>What we can see in that command is that pgloader will actually download the
zip archive file from its http source URL, unzip it locally then work on the
filename from the archive matching the one we know about: we don't want to
hardcode in the command the name of the directory contained in the zip file.
</p><center><img src='http://tapoueh.org/images/lisp-locator-logo.jpg' /></center><p>Also, contrary to the previous version, it's quite easy to just trim the
<code>loc_name</code> column as we load the data. Here I've been adding a new function to
do that, because I wanted to play with optimizing it (adding type
declarations and inlining it), but the loading works about as well with just
the following (just timed 3 runs at 
<code>0.771s</code>, 
<code>0.654s</code> and 
<code>0.862s</code>) :
</p><pre><code>&#xA0;       INTO postgresql:///pgloader?places
             (
	        usps, fips, fips_code,
                loc_name text using (string-right-trim &quot; &quot; loc_name)
             );
</code></pre><p>The 
<a href='http://www.lispworks.com/documentation/HyperSpec/Body/f_stg_tr.htm'>string-right-trim</a> function is part of the Common Lisp Standard. The
optimisation here looks like:
</p><pre><code>(declaim (inline right-trim))

(defun right-trim (string)
  &quot;Remove whitespaces at end of STRING.&quot;
  (declare (type simple-string string))
  (string-right-trim &#039;(#\Space) string))
</code></pre><p>Note that you could be providing that definition in your own 
<code>trim.lisp</code> file
and provide it using the 
<code>--load trim.lisp</code> command line option, pgloader
would then compile that to machine code for you before processing your data
file.
</p><h2>Conclusion</h2><p>If you're already using pgloader, you will enjoy the new version of it! The
new version comes with a command line option to migrate the old
configuration file into a command string, making upgrades even easier.
</p><p>Of course, if you're interested, consider giving the release candidate a
try, it's available on the 
<a href='https://github.com/dimitri/pgloader'>pgloader</a> github repository already.
</p>]]>
  </description>
  <author>dim@tapoueh.org (Dimitri Fontaine)
  </author><pubDate>Mon, 18 Nov 2013 12:48:00 +0100</pubDate><guid isPermaLink="true">http://tapoueh.org/blog/2013/11/18-pgloader-fixed-width-data.html</guid>
</item>

<item>
  <title>A Worthwile Micro Optimisation
  </title>
  <link>http://tapoueh.org/blog/2013/10/03-micro-optimizing-int-to-ip-address.html
  </link>
  <description><![CDATA[<p>In our previous article about 
<a href='/blog/2013/10/01-loading-geolocation-data'>Loading Geolocation Data</a>, we did load some
data into PostgreSQL and saw the quite noticable impact of a user
transformation. As it happens, the function that did the integer to IP
representation was so naive as to scratch the micro optimisation itch of
some Common Lisp hackers: thanks a lot guys, in particular 
<a href='https://github.com/stassats'>stassats</a> who came
up with the solution we're seeing now.
</p><center><img src='http://tapoueh.org/images/make-computer-faster.jpg' /></center><p>The previous code was a straight rewrite of the provided documentation in
Common Lisp. See for yourself the docs as found at
<a href='http://dev.maxmind.com/geoip/legacy/csv/'>http://dev.maxmind.com/geoip/legacy/csv/</a>:
</p><pre><code>integer_ip = 2921648058

o1 = int ( ipnum / 16777216 ) % 256;
o2 = int ( ipnum / 65536    ) % 256;
o3 = int ( ipnum / 256      ) % 256;
o4 = int ( ipnum            ) % 256;

address = ( o1, o2, o3, o4 ).join(&#039;.&#039;)
</code></pre><p>And the code I came up with:
</p><pre><code>(defun ip-range (start-integer-string end-integer-string)
  &quot;Transform a couple of integers to an IP4R ip range notation.&quot;
  (declare (inline)
	   (optimize speed)
	   (type string start-integer-string end-integer-string))
  (flet ((integer-to-ip-string (int)
	   &quot;see http://dev.maxmind.com/geoip/legacy/csv/&quot;
	   (declare (inline) (optimize speed) (type fixnum int))
	   (format nil &quot;~a.~a.~a.~a&quot;
		   (mod (truncate int #. (expt 2 24)) 256)
		   (mod (truncate int #. (expt 2 16)) 256)
		   (mod (truncate int #. (expt 2 8)) 256)
		   (mod int 256))))
    (let ((ip-start (integer-to-ip-string (parse-integer start-integer-string)))
	  (ip-end   (integer-to-ip-string (parse-integer end-integer-string))))
      (format nil &quot;~a-~a&quot; ip-start ip-end))))
</code></pre><p>Quite a direct naive implementation. Which is good to show what you can
expect in a kind of a worst case, and that worst case was using 
<em>31.546
seconds</em> rather than 
<em>17.425 seconds</em> when not doing any conversion. Well of
course the python code was spending 
<em>78.979 seconds</em> for not doing any
conversion, but that's not the topic today.
</p><center><img src='http://tapoueh.org/images/ip-address.640.png' /></center><p>Let's now see one of the micro optimised solution, the one I picked among a
list of 8 different proposal, each a little more crazy than the previous
one:
</p><pre><code>(declaim (inline int-to-ip))
(defun int-to-ip (int)
  &quot;Transform an IP as integer into its dotted notation, optimised code from
   stassats.&quot;
  (declare (optimize speed)
           (type (unsigned-byte 32) int))
  (let ((table (load-time-value
                (let ((vec (make-array (+ 1 #xFFFF))))
                  (loop for i to #xFFFF
		     do (setf (aref vec i)
			      (coerce (format nil &quot;~a.~a&quot;
					      (ldb (byte 8 8) i)
					      (ldb (byte 8 0) i))
				      &#039;simple-base-string)))
                  vec)
                t)))
    (declare (type (simple-array simple-base-string (*)) table))
    (concatenate &#039;simple-base-string
		 (aref table (ldb (byte 16 16) int))
		 &quot;.&quot;
		 (aref table (ldb (byte 16 0) int)))))

(declaim (inline ip-range))
(defun ip-range (start-integer-string end-integer-string)
  &quot;Transform a couple of integers to an IP4R ip range notation.&quot;
  (declare (optimize speed)
	   (type string start-integer-string end-integer-string))
  (let ((ip-start (int-to-ip (parse-integer start-integer-string)))
	(ip-end   (int-to-ip (parse-integer end-integer-string))))
    (concatenate &#039;simple-base-string ip-start &quot;-&quot; ip-end)))
</code></pre><p>As usual the idea is to compute all you can in advance, here thanks to the
<a href='http://www.lispworks.com/documentation/HyperSpec/Body/s_ld_tim.htm'>load-time-value</a> special operator that's part of the Common Lisp Standard. So
we compute a table of all the dotted representation for a pair of two bytes,
and we do that computation at 
<em>load time</em>, which happens when you load the
<em>compiled code artifact</em> you generated from your sources. Then all we have to
do is take the upper and lower bytes, fetch their representation in our
table, and concatenate both with a middle dot.
</p><p>The reason why we only keep 2 bytes in the table is so that we don't require
about 
<em>64 GB</em> of memory to be able to transform ip addresses...
</p><center><img src='http://tapoueh.org/images/cpu-ram-disk.640.jpg' /></center><center><em>Even after all those years, either compute again or store in memory.</em></center><p>And what do we have now?
</p><pre><code>&#xA0;                   table name       read   imported     errors       time
------------------------------  ---------  ---------  ---------  ---------
                       extract          0          0          0       1.01
                   before load          0          0          0      0.077
------------------------------  ---------  ---------  ---------  ---------
              geolite.location     438386     438386          0     10.352
                geolite.blocks    1790461    1790461          0     18.045
------------------------------  ---------  ---------  ---------  ---------
                       finally          0          0          0     31.108
------------------------------  ---------  ---------  ---------  ---------
             Total import time    2228847    2228847          0   1m0.592s
</code></pre><p>Thanks to the optimisation, the 
<em>two bigint as text to iprange as text</em>
transformation now has an added cost of 
<code>620 ms</code> with our data set. The whole
file loading is now averaging to 
<code>10.07841 &#xB5;s</code> per row, or just a tad more
than 
<em><strong>10 microseconds</strong></em> per row, transformation included this time.
</p><p>Less than a second of added cost within a complete process taking around a
minute, that basically means that the transformation is now free.
</p><p>Despite what you might have read elsewhere, my experience with the Common
Lisp Community so far really is great!
</p>]]>
  </description>
  <author>dim@tapoueh.org (Dimitri Fontaine)
  </author><pubDate>Thu, 03 Oct 2013 22:10:00 +0200</pubDate><guid isPermaLink="true">http://tapoueh.org/blog/2013/10/03-micro-optimizing-int-to-ip-address.html</guid>
</item>

<item>
  <title>Loading Geolocation Data
  </title>
  <link>http://tapoueh.org/blog/2013/10/01-loading-geolocation-data.html
  </link>
  <description><![CDATA[<p>As I've been mentionning in the past already, I'm currently rewriting
pgloader from scratch in 
<a href='/tags/common-lisp'>Common Lisp</a>. In terms of 
<a href='http://en.wikipedia.org/wiki/Technical_debt'>technical debt</a> that's akin
to declaring 
<em>bankrupcy</em>, which is both sad news and good news as there's
suddenly new hope of doing it right this time.
</p><center><img src='http://tapoueh.org/images/lisp-python.png' /></center><center><em>Let's dive into the python to common lisp rewrite</em></center><h2>Why rewriting pgloader?</h2><p>Several problems hinted me into doing something other than maintaining the
code I had for 
<a href='/projects/pgloader'>pgloader</a>.
</p><p>First, the configuration file format was already used well beyond its
capacities: it's quite hard to express advanced fine grained options when
using the infamous 
<a href='http://en.wikipedia.org/wiki/INI_file'>INI file format</a>. Continuing with python I did consider
using the 
<a href='http://pyparsing.wikispaces.com/'>pyparsing</a> library and even went as far as writing a prototype
command language with it. While the lib was working great for that, at the
time at least it wasn't available for 
<em>Python 3</em> and not to be found in some
OS vendors. 
<em>pgloader</em> is available today for Debian and derivatives, RedHat
and derivatives, FreeBSD, OpenBSD, and some more. And you can easily enough
use it on Windows too, because it only depends on 
<a href='http://initd.org/psycopg/'>Psycopg</a>, basically, so
that I wanted to keep things simple for the end user. Basically, integrating
that lib was a problem.
</p><p>Then again, the performances were a bigger and bigger worry for me as the
size of files one want to load into its database grew bigger. The real
problem was not perceived as the current performance characteristics but
rather how to improve them, and I didn't see many ways forward. The most
obvious one was to rewrite critical parts in C, but if I need to switch away
from python I'd rather find a better compromse than writing python modules
in C.
</p><p>That's in short what lead me to consider alternatives. And as I had some
really good preliminary performances results with a programming language I
can actually like, I decided to go with Common Lisp.
</p><h2>A Test case</h2><p>The blog post I actually wanted to write would have been about the awesome
<a href='https://github.com/RhodiumToad/ip4r'>ip4r</a> extension, which is about indexing IP range searches as mostly used
nowadays in 
<em>geoip location</em>. The first step here is to find some data to use
and load them, and wanting to do that I realised we were lacking an
all-integrated tool easy enough to use.
</p><p>And that's exactly the reason why I'm working on 
<em>pgloader</em> in the first
place, to make it really easy to load data, converting it on the fly and all
the jazz. And that 
<em>ip4r</em> test case actually has been a very good excuse at
improving what I already had.
</p><center><img src='http://tapoueh.org/images/toy-loader.320.jpg' /></center><h2>The new pgloader command language</h2><p>Here's the current way of doing things with the new 
<em>pgloader</em> written in
Common-Lisp, then I'll tell you why using a 
<em>parser generator library</em> in that
language is less of a burden than doing the same thing in Python:
</p><pre><code>LOAD FROM ARCHIVE http://geolite.maxmind.com/download/geoip/database/GeoLiteCity_CSV/GeoLiteCity-latest.zip
   INTO postgresql://dim@localhost:54393/dim

BEFORE LOAD DO
   $$ create extension if not exists ip4r; $$,
   $$ create schema if not exists geolite; $$,
   $$ create table if not exists geolite.location
     (
        locid      integer primary key,
        country    text,
        region     text,
        city       text,
        postalcode text,
        location   point,
        metrocode  text,
        areacode   text
     );
   $$,
   $$ create table if not exists geolite.blocks
     (
        iprange    ip4r,
        locid      integer
     );
   $$,
   $$ drop index if exists geolite.blocks_ip4r_idx; $$,
   $$ truncate table geolite.blocks, geolite.location cascade; $$

    LOAD CSV FROM FILENAME MATCHING ~/GeoLiteCity-Location.csv/
             WITH ENCODING iso-8859-1
             (
                locId,
                country,
                region     null if blanks,
                city       null if blanks,
                postalCode null if blanks,
                latitude,
                longitude,
                metroCode  null if blanks,
                areaCode   null if blanks
             )
        INTO postgresql://dim@localhost:54393/ip4r?geolite.location
             (
                locid,country,region,city,postalCode,
                location point using (format nil &quot;(~a,~a)&quot; longitude latitude),
                metroCode,areaCode
             )
        WITH skip header = 2,
             fields optionally enclosed by &#039;&quot;&#039;,
             fields escaped by double-quote,
             fields terminated by &#039;,&#039;

AND LOAD CSV FROM FILENAME MATCHING ~/GeoLiteCity-Blocks.csv/
             WITH ENCODING iso-8859-1
             (
                startIpNum, endIpNum, locId
             )
        INTO postgresql://dim@localhost:54393/ip4r?geolite.blocks
             (
                iprange ip4r using (ip-range startIpNum endIpNum),
                locId
             )
        WITH skip header = 2,
             fields optionally enclosed by &#039;&quot;&#039;,
             fields escaped by double-quote,
             fields terminated by &#039;,&#039;

FINALLY DO
   $$
     create index blocks_ip4r_idx on geolite.blocks using gist(iprange);
   $$;
</code></pre><p>What you see here looks quite complex and heavy. That's because it's
addressing the full needs, let's see about that:
</p><ul><li>the source of the data loading is a <em>zip archive</em> that we have to download from a <em>http url</em>,</li></ul><ul><li>the <em>zip archive</em> contains a single directory entry wherein we find the data files we want to load, currently <code>GeoLiteCity_20130903</code>... as we want to be able to update the data without having to edit the command, you will note the support for matching the file name against a full path name: <code>FILENAME MATCHING ~/GeoLiteCity-Blocks.csv/</code> is applying a regular expression,</li></ul><ul><li>we want the loading process to take care about creating the database schema we're going to use, and even drop the current index if any before loading, because we know that loading the data in bulk and then creating the index is way faster than keeping the index around,</li></ul><ul><li>there's no out of band place where to register whether a field value is null in a plain text CSV file, even if technically that could be done, so we have to provide per field instructions on what a <code>NULL</code> will look like in the data itself,</li></ul><ul><li>the files we're using don't have the exact definition we want: rather than a pair of <em>double precision</em> fields for hosting the location we want a <a href='http://www.postgresql.org/docs/9.3/static/datatype-geometric.html#AEN6542'>PostgreSQL point</a> datatype column; and rather than a pair of <em>bigint</em> fields we want an <em>ip4r</em> column in PostgreSQL,</li></ul><ul><li>and of course as soon as the data loading is done we want to create the shiny <em>ip4r</em> <strong>GiST</strong> index.</li></ul><p>And as we are using an attribution licence for the data here, let me tell
you that: This article includes 
<strong>GeoLite</strong> data created by 
<strong>MaxMind</strong>, available
from 
<a href='http://www.maxmind.com'>http://www.maxmind.com</a>.
</p><h2>Transforming and Projecting</h2><p>The former 
<em>pgloader</em> was capable of using dynamic python code to reformat
fields on the fly, which was already quite good, but not up to the task
described just above. What we really want to be able to do here is 
<em>project</em>
any number of fields into a 
<em>possibly different</em> number of columns that are
dependant on the fields.
</p><center><img src='http://tapoueh.org/images/huge-full-outer-join.gif' /></center><center><em>Traditionnaly used for Full Outer Join, but I liked the symbol</em></center><p>In the simplest case possible, each field ends up into a column of the same
name, and that's still supported of course, it looks like this:
</p><pre><code>LOAD CSV FROM FILENAME MATCHING ~/GeoLiteCity-Blocks.csv/
         WITH ENCODING iso-8859-1
    INTO postgresql://dim@localhost:54393/ip4r?geonumip.blocks
    WITH skip header = 2,
         fields optionally enclosed by &#039;\&quot;&#039;,
         fields escaped by double-quote,
         fields terminated by &#039;,&#039;;
</code></pre><p>That's a very simple processing actually, with no added processing when
reading the file.
</p><p>In the previous example though, we're using a 
<em>transformation</em> function that I
included in 
<em>pgloader</em> to transform a couple of integers into an 
<em>iprange</em>.
Here's the Common Lisp sources for that function:
</p><pre><code>(defun ip-range (start-integer-string end-integer-string)
  &quot;Transform a couple of integers to an IP4R ip range notation.&quot;
  (declare (inline)
	   (optimize speed)
	   (type string start-integer-string end-integer-string))
  (flet ((integer-to-ip-string (int)
	   &quot;see http://dev.maxmind.com/geoip/legacy/csv/&quot;
	   (declare (inline) (optimize speed) (type fixnum int))
	   (format nil &quot;~a.~a.~a.~a&quot;
		   (mod (truncate int #. (expt 2 24)) 256)
		   (mod (truncate int #. (expt 2 16)) 256)
		   (mod (truncate int #. (expt 2 8)) 256)
		   (mod int 256))))
    (let ((ip-start (integer-to-ip-string (parse-integer start-integer-string)))
	  (ip-end   (integer-to-ip-string (parse-integer end-integer-string))))
      (format nil &quot;~a-~a&quot; ip-start ip-end))))
</code></pre><p>And here's how it's used:
</p><pre><code>INTO postgresql://dim@localhost:54393/ip4r?geolite.blocks
     (
        iprange ip4r using (ip-range startIpNum endIpNum),
        locId
     )
</code></pre><p>Internally, pgloader generates a 
<em>lambda</em> expression to process each row, in
that very case the expression looks like this:
</p><pre><code>(lambda (row)
  (destructuring-bind (startIpNum endIpNum locId) row
    (list (pgloader.transforms::ip-range startIpNum endIpNum)
          locId)))
</code></pre><p>That generated lambda expression is then 
<em><strong>compiled</strong></em> on the fly into machine
code that is then executed for each row input. That's the kind of things
allowing Common Lisp programmers not to resort to C.
</p><center><img src='http://tapoueh.org/images/lisplogo_fancy_256.png' /></center><p>Note that as in the previous 
<em>location</em> example you can also directly write
the Lisp expressions within the command itself:
</p><pre><code>INTO postgresql://dim@localhost:54393/ip4r?geolite.location
     (
        locid,country,region,city,postalCode,
        location point using (format nil &quot;(~a,~a)&quot; longitude latitude),
        metroCode,areaCode
     )
</code></pre><h2>Simple benchmarking</h2><p>Enough presenting the software already, let's run it!
</p><h3>Python version</h3><p>First, let's run the current python pgloader code against our dataset. Note
that you will have to download and unzip the data yourself here, then
prepare that kind of configuration file:
</p><pre><code>[pgsql]
base = pgloader

log_file            = /tmp/pgloader.log
log_min_messages    = INFO
client_min_messages = WARNING

client_encoding = &#039;latin1&#039;
lc_messages         = C
pg_option_standard_conforming_strings = on
; This setting has no effect other than allowing to check option precedence
pg_option_work_mem = 12MB

copy_every      = 25000
commit_every    = 25000
#copy_delimiter  = %

null         = &quot;&quot;
empty_string = &quot;\ &quot;

[tmpl]
template        = True
format          = csv
field_sep       = ,
skip_head_lines = 2

[location]
use_template    = tmpl
table           = location
filename        = /tmp/GeoLiteCity-latest/GeoLiteCity_20130903/GeoLiteCity-Location.csv
columns         = *

[blocks]
use_template    = tmpl
table           = blocks
filename        = /tmp/GeoLiteCity-latest/GeoLiteCity_20130903/GeoLiteCity-Blocks.csv
columns         = *
skip_head_lines = 2
</code></pre><p>And with that we can do:
</p><pre><code>./pgloader.py -R reformat -sTv -c ~/dev/temp/pgloader.geolite.conf
   ... edited ...
Table name        |    duration |    size |  copy rows |     errors 
====================================================================
blocks            |  01m18.979s |       - |    1790461 |          0
location          |     35.710s |       - |     438386 |          0
====================================================================
Total             |  01m54.713s |       - |    2228847 |          0
</code></pre><h3>Common Lisp version, no projection</h3><p>With a command doing the same amount of work as the previous one for the
<em>blocks</em> table (I still kept the longitude and latitude to point formating):
</p><pre><code>&#xA0;                   table name       read   imported     errors       time
------------------------------  ---------  ---------  ---------  ---------
                       extract          0          0          0      1.007
                   before load          0          0          0      0.024
------------------------------  ---------  ---------  ---------  ---------
             geonumip.location     438386     438386          0      8.868
               geonumip.blocks    1790461    1790461          0     17.425
------------------------------  ---------  ---------  ---------  ---------
             Total import time    2228847    2228847          0    27.324s
</code></pre><p>We can see new sections, as the command is now actually extracting the
archive (at the size and given how many tests I wanted to do, the fetching
step as been avoided in the tests here), then preparing the load with all
the 
<code>CREATE TABLE IF NOT EXISTS</code> steps, and then actually parsing the files
and loading the data.
</p><p>So for doing the same work, we went from nearly 
<em><strong>2 minutes</strong></em> with the previous
solution down to less than 
<em><strong>30 seconds</strong></em>, including the zip archive extraction
and the create table steps. Another way to look at it is that 
<em>pgloader</em> here
spent 
<code>9.73213 &#xB5;s</code> to process each row. Yeah, less than 10 
<em><strong>microseconds</strong></em> per
row.
</p><p>Declaring bankrupcy on the previous code base was maybe not the worst
decision I made this year, finally...
</p><p>I want to note here that the python importing code is actually using the 
<em>csv</em>
module for reading the files, and that module is already written in C in
fact. So we're actually comparing python and C on the one hand to Common
Lisp alone on the other hand.
</p><center><img src='http://tapoueh.org/images/made-with-lisp.png' /></center><h3>Common Lisp version with fields to columns projection</h3><p>Now, as we have some time budget left, let's have the loader actually do
more work for us and convert on the fly to the data representation we are
interested into:
</p><pre><code>&#xA0;                   table name       read   imported     errors       time
------------------------------  ---------  ---------  ---------  ---------
                       extract          0          0          0      1.008
                   before load          0          0          0      0.134
------------------------------  ---------  ---------  ---------  ---------
              geolite.location     438386     438386          0      9.521
                geolite.blocks    1790461    1790461          0     31.546
------------------------------  ---------  ---------  ---------  ---------
                       finally          0          0          0     31.134
------------------------------  ---------  ---------  ---------  ---------
             Total import time    2228847    2228847          0  1m13.343s
</code></pre><p>So this time with the extra work we're going from about 18 seconds up to
about 32 seconds. The projection of integers to ip ranges is costly, but we
are still so much ahead from the old solution than we have enough time to
run the command 
<code>create index blocks_ip4r_idx on geolite.blocks using
gist(iprange);</code> and still finish about 40 seconds earlier.
</p><p>This time 31,456,000 microseconds where spent loading 1,790,461 rows, and
that gives us 
<code>17.568659 &#xB5;s</code> per row in average, including the 
<em>two bigint as
text to iprange as text</em> transformation calls. Not too bad.
</p><h2>Conclusion</h2><p>Earlier in the article I said I would tell you why it's ok to use a 
<em>parser
generator library</em> such as 
<a href='http://nikodemus.github.io/esrap/'>esrap</a> here, whereas in the python case it was a
burden. The first part is that we don't have the major 
<em>platform</em> problem that
python has with version 2 against version 3: 
<a href='http://www.lispworks.com/documentation/HyperSpec/Front/'>the Common Lisp Specs</a> are a
really 
<em><strong>stable</strong></em> base to develop libs, and so even decade old untouched code
has a really good chance to work as-is.
</p><center><img src='http://tapoueh.org/images/morland_a_carriers_stable2.jpg' /></center><center><em>Another kind of </em><strong>stable</strong><em>, here</em></center><p>It's also possible and should be easy enough with 
<a href='http://www.xach.com/lisp/buildapp/'>cl-buildapp</a> to produce a
static all-included binary image of the application so that users don't have
to care about which programming language and libraires are used when
developping 
<em>pgloader</em>.
</p><p>And of course the proficiency and interactivity of using Common Lisp is so
much better than when using python than you almost wonder why, given the
performances characteristics we're seeing here and in almost any other test,
o why are so much people still using python?
</p><h2>Can I use that new software today?</h2><p>Yes you can, be aware that it's not for the faint of heart in the current
shape of things. Most of the development time has been spent on features
development and testing, integration with a 
<em>nice</em> enough command language and
things like that, and very few consideration has been made yet to ease of
use and advanced error management.
</p><p>If you're a Common Lisp user already, you can fetch the code and look
around, the 
<em>docstrings</em> and the code generated by the 
<em>parser</em> should help you
figure out the API and use it. Of course, any feedback is welcome!
</p><p>If you're not a Common Lisp user already, then the 
<em>README.md</em> file has
instructions to get you started with the software, and you can hack your way
around with the command language from examples in that blog post. Proper
documentation of the supported commands and their options is still on the
to-do list.
</p><p>The 
<em>pgloader</em> software is distributed under the same Licence as PostgreSQL
itself, a licence in between MIT and two clauses BSD ones.
</p><p>As I don't consider the software ready for the general public (mainly
because of the lack of proper docs), it's not yet published at the usual
place within 
<a href='http://github.com/dimitri'>my github repositories</a>, instead you can find it at its interm
place: 
<a href='http://git.tapoueh.org/?p=pgloader.git;a=summary'>http://git.tapoueh.org/?p=pgloader.git;a=summary</a>.
</p><p>Any contributions are welcome to help polish 
<em>pgloader</em>, have fun!
</p>]]>
  </description>
  <author>dim@tapoueh.org (Dimitri Fontaine)
  </author><pubDate>Tue, 01 Oct 2013 16:52:00 +0200</pubDate><guid isPermaLink="true">http://tapoueh.org/blog/2013/10/01-loading-geolocation-data.html</guid>
</item>

<item>
  <title>Emacs Muse meets Common Lisp
  </title>
  <link>http://tapoueh.org/blog/2013/07/08-Muse-blog-compiler.html
  </link>
  <description><![CDATA[<p>This blog of mine is written in the very good 
<a href='http://mwolson.org/projects/EmacsMuse.html'>Emacs Muse</a> format, that I find
much more friendly to writing articles than both 
<a href='http://orgmode.org/'>org-mode</a> and 
<a href='http://jblevins.org/projects/markdown-mode/'>markdown-mode</a>
that I both use in a regular basis too. The main think that I like in Muse
that those two others lack is the support for displaying images inline.
</p><center><img src='http://tapoueh.org/images/Emacs-Muse.png' /></center><center><em>Here's what it looks like to edit with Emacs Muse</em></center><h2>The Muse publishing system</h2><p>The idea is that you edit 
<code>.muse</code> files in Emacs then use a command to 
<em>publish</em>
your file to some format like HTML. You can also publish a whole project,
and then enjoy a fully static website that you can deploy on some URL.
</p><p>The drawback of using the Muse format here is to do with the associated
tooling. I didn't take time to study Muse Emacs Lisp sources and probably I
should have, as I found myself the need to add support for 
<em>tags</em> and per-tag
<em>rss</em> support. What I did then was using Muse provided 
<em>hooks</em> so that my code
gets run here and there.
</p><p>With my additions, though, publishing a single article took a painful time,
something around 30 seconds for the main page of it and then about as much
(sometimes more) to publish the project: previous and next articles
sections, tag files, rss files.
</p><h2>from Emacs Lisp to Common LIsp</h2><center><img src='http://tapoueh.org/images/lisp-is-different.jpg' /></center><center><em>and Common Lisp is different from Emacs Lisp</em></center><p>When I realized that my motivation to writing new blog articles was going so
low that I wasn't doing that anymore, I raised the priority of fixing my
blog setup enough so that I would actually do something about it. And then
decided it would actually be a good excuse for another 
<em>Common Lisp</em> project.
</p><p>It's available at 
<a href='http://git.tapoueh.org/?p=tapoueh.org.git;a=summary'>git.tapoueh.org</a> with my usual choice of licencing, the
<a href='http://www.wtfpl.net/'>WTFPL</a>.
</p><p>I've been using 
<a href='http://nikodemus.github.io/esrap/'>esrap</a> to write a Muse parser along with 
<a href='http://common-lisp.net/project/fiveam/'>5am</a> to test it
extensively. It turned out not an easy project to just parse the articles,
but thanks to 
<a href='http://weitz.de/cl-who/'>cl-who</a> (that stands for 
<code>with-html-output</code>) the output of the
parser is a very simple 
<em>nested list</em> data structure.
</p><p>In the 
<strong>Emacs Lisp</strong> Muse system there was a cool idea that you could embed
(compile time) dynamic sections of 
<code>&lt;lisp&gt;(code)&lt;/lisp&gt;</code> in the templates, so
I've kept that idea and implemented a 
<em>Server Side Include</em> facility.
</p><p>The idea has then been to build a dynamic website without any level of
caching at all so that anytime you reload a page its muse source file is
parsed and intermediate representation is produced. Then the 
<strong>SSI</strong> kicks with
adding the 
<em>header</em> and 
<em>footer</em> around the main article's body, and replacing
any embedded code calls with their results. Finally,
<code>with-html-output-to-string</code> is used to return an HTML string to the browser.
</p><p>With all that work happening at run-time, one would think that the
performances of the resulting solution would be awful. Well in fact not at
all, as you can see:
</p><pre><code>$ time curl -s http://localhost:8042/blog/2013/07/08-Muse-blog-compiler &gt; /dev/null
real	0m0.081s
</code></pre><p>And then some quick measurements of time spent to parse all the articles
I've ever published here:
</p><pre><code>TAPOUEH&gt; (time (length (find-blog-articles *blog-directory*
					   :parse-fn #&#039;muse-parse-article)))
Evaluation took:
  0.484 seconds of real time
  0.486381 seconds of total run time (0.415208 user, 0.071173 system)
  [ Run times consist of 0.089 seconds GC time, and 0.398 seconds non-GC time. ]
  100.41% CPU
  1,113,697,675 processor cycles
  206,356,848 bytes consed
  
181
</code></pre><p>So it takes about 
<code>80 ms</code> to render a full dynamic page. That's way better
than what I wanted to achieve!
</p><h2>the static compiler</h2><p>That said, we're talking about the ability to render about 
<code>12</code> pages per
second, which is not something acceptable as-is for production use. And
given the expected ratio of reads and writes, there's no good reason to
publish a dynamic website, so the next step here is to build a 
<em>static
website compiler</em>.
</p><p>And here's how it looks like now:
</p><pre><code>TAPOUEH&gt; (compile-articles)
parsed 199 docs in 0.627s
parsed chapeau of 172 blog articles in 0.114s
compiled the home page in 0.015s
compiled the tags cloud in 0.005s
compiled the blog archives page in 0.085s
compiled 199 documents in 12.333 secs
compiled 56 blog indexes in 1.721s
compiled 64 tag listings in 1.073s
compiled 6 rss feeds in 3.806s
compiled the sitemap in 0.021s
</code></pre><p>So it takes about 20 seconds to publish again the whole website. The reason
why it's ok for me not to optimize this yet is because I've also been
changing the CSS and HTML parts of the website, and each time the header or
the parser is changed, or even some SSI function's output, then I need to
compile the whole set of files anyway.
</p><h2>The <code>displaying-time</code> macro</h2><center><img src='http://tapoueh.org/images/lisplogo_fancy_256.png' /></center><p>While building this little Common Lisp project, I've added to my still very
little toolbelt a macro that I like. Here's how to use it:
</p><pre><code>(let* ((all-documents
	  (displaying-time (&quot;parsed ~d docs in ~ds~%&quot; (length result) timing)
	    (find-muse-documents)))
	 (blog-articles
	  (displaying-time (&quot;parsed chapeau of ~d blog articles in ~ds~%&quot;
			    (length result) timing)
	    (find-blog-articles *blog-directory*))))

    (displaying-time (&quot;compiled the home page in ~ds~%&quot; timing)
      (compile-home-page :documents blog-articles :verbose verbose))
    ...)
</code></pre><p>And here's the code I wrote to have this macro:
</p><pre><code>(defun elapsed-time-since (start)
  &quot;Return how many seconds ticked between START and now&quot;
  (/ (- (get-internal-real-time) start)
     internal-time-units-per-second))

(defmacro timing (&amp;body forms)
  &quot;return both how much real time was spend in body and its result&quot;
  (let ((start (gensym))
	(end (gensym))
	(result (gensym)))
    `(let* ((,start (get-internal-real-time))
	    (,result (progn ,@forms))
	    (,end (get-internal-real-time)))
       (values
	,result
	(float (/ (- ,end ,start) internal-time-units-per-second))))))

(defun replace-symbols-recursively (code symbols)
  &quot;Walk CODE to replace symbols as given in the SYMBOLS alist.&quot;
  (loop
     for s-exp in code
     when (listp s-exp) collect (replace-symbols-recursively s-exp symbols)
     else collect (if (symbolp s-exp)
		      (destructuring-bind (before . after)
			  (or (assoc s-exp symbols) (cons nil nil))
			(if before after s-exp))
		      s-exp)))

(defmacro displaying-time ((fmt &amp;rest bindings) &amp;body forms)
  &quot;display on *standard-output* how much time it took to execute body&quot;
  (let* ((result   (gensym))
	 (timing   (gensym))
	 (bindings
	  (replace-symbols-recursively bindings `((result . ,result)
						  (timing . ,timing)))))
    `(multiple-value-bind (,result ,timing)
	 (timing ,@forms)
       (format t ,fmt ,@bindings)
       ,result)))
</code></pre><p>Note that I already had the 
<code>timing</code> macro and just used it as is.
</p>]]>
  </description>
  <author>dim@tapoueh.org (Dimitri Fontaine)
  </author><pubDate>Mon, 08 Jul 2013 13:34:00 +0200</pubDate><guid isPermaLink="true">http://tapoueh.org/blog/2013/07/08-Muse-blog-compiler.html</guid>
</item>

<item>
  <title>from Parsing to Compiling
  </title>
  <link>http://tapoueh.org/blog/2013/05/13-from-parser-to-compiler.html
  </link>
  <description><![CDATA[<p>Last week came with two bank holidays in a row, and I took the opportunity
to design a 
<em>command language</em> for 
<a href='http://tapoueh.org/pgsql/pgloader.html'>pgloader</a>. While doing that, I unexpectedly
stumbled accross a very nice 
<em>AHAH!</em> moment, and I now want to share it with
you, dear reader.
</p><center><img src='http://tapoueh.org/images/lightbulb.gif' /></center><center><em>AHAH, you'll see!</em></center><p>The general approach I'm following code wise with that 
<em>command language</em> is
to first get a code API to expose the capabilities of the system, then
somehow plug the 
<em>command language</em> into that API thanks to a 
<em>parser</em>. It turns
out that doing so in 
<em>Common Lisp</em> is really easy, and that you can get a
<em>compiler</em> for free too, while at it. Let's see about that.
</p><h2>A very simple toy example</h2><p>In this newsgroup article 
<a href='https://groups.google.com/forum/?fromgroups=#!topic/comp.lang.lisp/JJxTBqf7scU'>What is symbolic compoutation?</a>, 
<a href='http://informatimago.com/'>Pascal Bourguignon</a>
did propose a very simple piece of code:
</p><pre><code>(defparameter *additive-color-graph*
  &#039;((red   (red white)   (green yellow) (blue magenta))
    (green (red yellow)  (green white)  (blue cyan))
    (blue  (red magenta) (green cyan)   (blue white))))

(defun symbolic-color-add (a b)
  (cadr (assoc a (cdr (assoc b *additive-color-graph*)))))
</code></pre><p>This is an example of 
<em>symbolic computation</em>, and we're going to build a
little 
<em>language</em> to express the data and the code. Not that we would need to
build one, mind you, more in order to have a really simple example leading
us to the 
<em>ahah</em> moment you're now waiting for.
</p><p>Before we dive into the main topic, you have to realize that the previous
code example actually works: it's defining some data, using an implicit data
structure composed by nesting lists together, and defines a function that
knows how to sort out the data in that anonymous data structure so as to
compound 2 colors together.
</p><pre><code>TOY-PARSER&gt; (symbolic-color-add &#039;red &#039;green)
YELLOW
</code></pre><h2>A command language and parser</h2><p>I decided to go with the following 
<em>language</em>:
</p><pre><code>color red   +red white    +green yellow  +blue magenta
color green +red yellow   +green white   +blue cyan
color blue  +red magenta  +green cyan    +blue white

mix red and green
</code></pre><p>And here's how some of the parser looks like, using the 
<a href='http://nikodemus.github.io/esrap/'>esrap</a> 
<em>packrat</em> lib:
</p><pre><code>(defrule color-name (and whitespaces (+ (alpha-char-p character)))
  (:destructure (ws name)
    (declare (ignore ws))		; ignore whitespaces
    ;; CL symbols default to upper case.
    (intern (string-upcase (coerce name &#039;string)) :toy-parser)))

;;; parse string &quot;+ red white&quot;
(defrule color-mix (and whitespaces &quot;+&quot; color-name color-name)
  (:destructure (ws plus color-added color-obtained)
    (declare (ignore ws plus))		; ignore whitespaces and keywords
    (list color-added color-obtained)))

;;; mix red and green
(defrule mix-two-colors (and kw-mix color-name kw-and color-name)
  (:destructure (mix c1 and c2)
    (declare (ignore mix and))		; ignore keywords
    (list c1 c2)))
</code></pre><p>Those 
<em>rules</em> are not the whole parser, go have a look at the project on
github if you want to see the whole code, it's called 
<a href='https://github.com/dimitri/toy-parser'>toy-parser</a> over there.
The main idea here is to show that when we parse a line from our little
language, we produce the simplest possible structured data: in lisp that's
<em>symbols</em> and 
<em>lists</em>.
</p><p>The reason why it makes sense doing that is the next rule:
</p><center><img src='http://tapoueh.org/images/the-one-ring.jpg' /></center><center><em>The one grammar rule to bind them all</em></center><pre><code>(defrule program (and colors mix-two-colors)
  (:destructure (graph (c1 c2))
    `(lambda ()
       (let ((*additive-color-graph* &#039;,graph))
	 (symbolic-color-add &#039;,c1 &#039;,c2)))))
</code></pre><p>This rule is the complex one to bind them all. It's using a 
<em>quasiquote</em>, a
basic lisp syntax element allowing the programmer to very easily produce
data that looks exactly like code. Let's see how it goes with a very simple
example:
</p><pre><code>TOY-PARSER&gt; (pprint (parse &#039;program
                           &quot;color red +green yellow mix green and red&quot;))

(LAMBDA NIL
  (LET ((*ADDITIVE-COLOR-GRAPH* &#039;((RED (GREEN YELLOW)))))
    (SYMBOLIC-COLOR-ADD &#039;RED &#039;GREEN)))
; No value
</code></pre><p>The parser is producing structure (nested) data that really looks like lisp
code, right? So maybe we can just run that code...
</p><h2>What about a compiler now?</h2><center><img src='http://tapoueh.org/images/aha.jpg' /></center><center><em>Here is my AHAH moment!</em></center><p>Let's see about actually running the code:
</p><pre><code>TOY-PARSER&gt; (let* ((code &quot;color red +green yellow mix green and red&quot;)
		   (program (parse &#039;program code)))
	      (compile nil program))
#&lt;Anonymous Function #x3020027CF0EF&gt;
NIL
NIL
TOY-PARSER&gt; (let* ((code &quot;color red +green yellow mix green and red&quot;)
		   (program (parse &#039;program code)))
	      (funcall (compile nil program)))
YELLOW
</code></pre><p>So we have a string reprensing code in our very little language, and a
parser that knows how to produce a nested list of atoms that looks like lisp
code. And as we have lisp, we can actually compile that code at run-time
with the same compiler that we used to produce our parser, and we can then
<code>funcall</code> that function we just built.
</p><p>Oh and the function is actually compiled down to native code, of course:
</p><pre><code>TOY-PARSER&gt; (let* ((code &quot;color red +green yellow mix red and green&quot;)
		   (program (parse &#039;program code))
		   (func    (compile nil program)))
	      (time (loop repeat 1000 do (funcall func))))

(LOOP REPEAT 1000 DO (FUNCALL FUNC))
took 108 microseconds (0.000108 seconds) to run.
During that period, and with 4 available CPU cores,
     105 microseconds (0.000105 seconds) were spent in user mode
      13 microseconds (0.000013 seconds) were spent in system mode
NIL
</code></pre><p>Yeah, it took the whole of 
<code>108 microseconds</code> to actually run the code
generated by our own 
<em>parser</em> 
<strong>a thousand times</strong>, on my laptop. I can believe
it's been compiled to native code, that seems like the right ballpark.
</p><h2>Conclusion</h2><p>The 
<a href='https://github.com/dimitri/toy-parser'>toy-parser</a> code is there on 
<em>GitHub</em> and you can actually load it using
<a href='http://www.quicklisp.org/'>Quicklisp</a>: clone the repository in 
<code>~/quicklisp/local-projects/</code> then
<code>(ql:quickload &quot;toy-parser&quot;)</code>, and play with it in 
<code>(in-package :toy-parser)</code>.
</p><p>The only thing I still want to say here is this: can your programming
language of choice make it that easy?
</p>]]>
  </description>
  <author>dim@tapoueh.org (Dimitri Fontaine)
  </author><pubDate>Mon, 13 May 2013 11:08:00 +0200</pubDate><guid isPermaLink="true">http://tapoueh.org/blog/2013/05/13-from-parser-to-compiler.html</guid>
</item>

<item>
  <title>Emacs Conference
  </title>
  <link>http://tapoueh.org/blog/2013/03/04-Emacs-Conference.html
  </link>
  <description><![CDATA[<p>The 
<a href='http://emacsconf.herokuapp.com/'>Emacs Conference</a> is happening, it's real, and it will take place at the
end of this month in London. Check it out, and register at
<a href='http://emacsconf.eventbrite.co.uk/'>Emacs Conference Event Brite</a>. It's free and there's still some availability.
</p><center><img src='http://tapoueh.org/images/emacs-rocks-logo.png' /></center><center><em>It's all about Emacs, and it rocks!</em></center><p>We have a great line-up for this conference, which makes me proud to be able
to be there. If you've ever been paying attention when using 
<a href='http://www.gnu.org/software/emacs/'>Emacs</a> then
you've already heard those names: 
<a href='http://sachachua.com/blog/'>Sacha Chua</a> is frequently blogging about
how she manages to improve her workflow thanks to 
<a href='http://www.gnu.org/software/emacs/emacs-lisp-intro/'>Emacs Lisp</a>, 
<a href='https://github.com/jwiegley'>John Wiegley</a>
is a proficient Emacs contributor maybe best known for his 
<a href='https://github.com/ledger/ledger'>ledger</a> 
<em>Emacs
Mode</em>, then we have 
<a href='http://www.lukego.com/'>Luke Gorrie</a> who hacked up 
<a href='http://wingolog.org/archives/2006/01/02/slime'>SLIME</a> among other things, we
also have 
<a href='http://nic.ferrier.me.uk/'>Nic Ferrier</a> who is starting a revolution in how to use 
<em>Emacs Lisp</em>
with 
<a href='http://elnode.org/'>elnode</a>. And more! Including 
<a href='http://en.wikipedia.org/wiki/Steve_Yegge'>Steve Yegge</a>!
</p><center>See you there in London.</center>]]>
  </description>
  <author>dim@tapoueh.org (Dimitri Fontaine)
  </author><pubDate>Mon, 04 Mar 2013 13:58:00 +0100</pubDate><guid isPermaLink="true">http://tapoueh.org/blog/2013/03/04-Emacs-Conference.html</guid>
</item>

<item>
  <title>Playing with pgloader
  </title>
  <link>http://tapoueh.org/blog/2013/02/12-playing-with-pgloader.html
  </link>
  <description><![CDATA[<p>While making progress with both 
<a href='http://wiki.postgresql.org/wiki/Event_Triggers'>Event Triggers</a> and 
<a href='http://tapoueh.org/blog/2013/01/08-Extensions-Templates.html'>Extension Templates</a>, I
needed to make a little break. My current keeping sane mental exercise seems
to mainly involve using 
<em>Common Lisp</em>, a programming language that ships with
about all the building blocks you need.
</p><center><img src='http://tapoueh.org/images/made-with-lisp.png' /></center><center><em>Yes, that old language brings so much on the table</em></center><p>When using 
<em>Common Lisp</em>, you have an awesome interactive development
environment where you can redefine function and objects 
<em>while testing them</em>.
That means you don't have to quit the interpreter, reload the new version of
the code and put the interactive test case together all over again after a
change. Just evaluate the change in the interactive environement: functions
are compiled incrementally over their previous definition, objects whose
classes have changed are migrated live.
</p><p>See, I just said 
<em>objects</em> and 
<em>classes</em>. 
<em>Common Lisp</em> comes with some advanced
<em>Object Oriented Programming</em> facilities named 
<a href='http://www.aiai.ed.ac.uk/~jeff/clos-guide.html'>CLOS</a> and 
<a href='http://www.alu.org/mop/index.html'>MOP</a> where the 
<em>Java</em> and
<em>Python</em> and 
<em>C++</em> object models are just a subset of what you're being offered.
Hint, those don't have 
<a href='http://en.wikipedia.org/wiki/Multiple_dispatch'>Multiple Dispatch</a>.
</p><p>And you have a very sophisticated 
<a href='http://www.gigamonkeys.com/book/beyond-exception-handling-conditions-and-restarts.html'>Condition System</a> where 
<em>Exceptions</em> are just
a subset of what you can do (hint: have a look a 
<a href='http://www.gigamonkeys.com/book/beyond-exception-handling-conditions-and-restarts.html#restarts'>restarts</a> and tell me you
didn't wish your programming language of choice had them). And it continues
that way for about any basic building bloc you might want to be using.
</p><h2>Loading data</h2><p>Back to 
<a href='http://tapoueh.org/pgsql/pgloader.html'>pgloader</a> will you tell me. Right. I've been spending a couple of
evening on hacking on the new version of pgloader in 
<em>Common Lisp</em>, and wanted
to share some preliminary results.
</p><center><img src='http://tapoueh.org/images/toy-loader.320.jpg' /></center><center><em>Playing with the loader</em></center><p>The current status of the new 
<em>pgloader</em> still is pretty rough, if you're not
used to develop in Common Lisp you might not find it ready for use yet. I'm
still working on the internal APIs and trying to make something clean and
easy to use for a developer, and then I will provide some external ways to
play with it, user oriented. I missed that step once with the 
<em>Python</em> based
version of the tool, I don't want to do the same errors again this time.
</p><p>So here's a test run with the current 
<em>pgloader</em>, on a small enough data set
of 
<code>226 MB</code> of 
<code>CSV</code> files.
</p><pre><code>time python pgloader.py -R.. --summary -Tc ../pgloader.dbname.conf

Table name        |    duration |    size |  copy rows |     errors
====================================================================
aaaaaaaaaa_aaaa   |      2.148s |       - |      24595 |          0
bbbbbbbbbb_bbbb...|      0.609s |       - |        326 |          0
cccccccccc_cccc...|      2.868s |       - |      25126 |          0
dddddddddd_dddd...|      0.638s |       - |          8 |          0
eeeeeeeeee_eeee...|      2.874s |       - |      36825 |          0
ffffffffff_ffffff |      0.667s |       - |        624 |          0
gggggggggg_gggg...|      0.847s |       - |       5638 |          0
hhh_hhhhhhh       |      9.907s |       - |     120159 |          0
iii_iiiiiiiiiiiii |      0.574s |       - |        661 |          0
jjjjjjj           |      6.647s |       - |      30027 |          0
kkk_kkkkkkkkk     |      0.439s |       - |         12 |          0
lll_llllll        |      0.308s |       - |          4 |          0
mmmm_mmm          |      2.139s |       - |      29669 |          0
nnnn_nnnnnn       |      8.555s |       - |     100197 |          0
oooo_ooooo        |     13.781s |       - |      93555 |          0
pppp_ppppppp      |      8.275s |       - |      76457 |          0
qqqq_qqqqqqqqqqqq |      8.568s |       - |     126159 |          0
====================================================================
Total             |  01m09.902s |       - |     670042 |          0
</code></pre><h2>Streaming data</h2><p>With the new code in 
<em>Common Lisp</em>, I could benefit from real multi threading
and higher level abstraction to make it easy to use: 
<a href='http://lparallel.org/'>lparallel</a> is a lib
providing exactly what I need here, with 
<em>workers</em> and 
<em>queues</em> to communicate
data in between them.
</p><p>What I'm doing is that two threads are separated, one is reading the data
from either a 
<code>CSV</code> file or a 
<em>MySQL</em> database directly, and pushing that data
in the queue; while the other thread is pulling data from the queue and
writing it into our 
<a href='http://www.postgresql.org/'>PostgreSQL</a> database.
</p><pre><code>CL-USER&gt; (pgloader.csv:import-database &quot;dbname&quot;
            :csv-path-root &quot;/path/to/csv/&quot;
            :separator #\Tab
            :quote #\&quot;
            :escape &quot;\&quot;\&quot;&quot;
            :null-as &quot;:null:&quot;)
                    table name       read   imported     errors       time
------------------------------  ---------  ---------  ---------  ---------
               aaaaaaaaaa_aaaa      24595      24595          0     0.995s
          bbbbbbbbbb_bbbbbbbbb        326        326          0     0.570s
       cccccccccc_cccccccccccc      25126      25126          0     1.461s
      dddddddddd_dddddddddd_dd          8          8          0     0.650s
eeeeeeeeee_eeeeeeeeee_eeeeeeee      36825      36825          0     1.664s
             ffffffffff_ffffff        624        624          0     0.707s
     gggggggggg_ggggg_gggggggg       5638       5638          0     0.655s
                   hhh_hhhhhhh     120159     120159          0     3.415s
             iii_iiiiiiiiiiiii        661        661          0     0.420s
                       jjjjjjj      30027      30027          0     2.743s
                 kkk_kkkkkkkkk         12         12          0     0.327s
                    lll_llllll          4          4          0     0.315s
                      mmmm_mmm      29669      29669          0     1.182s
                   nnnn_nnnnnn     100197     100197          0     2.206s
                    oooo_ooooo      93555      93555          0     9.683s
                  pppp_ppppppp      76457      76457          0     5.349s
             qqqq_qqqqqqqqqqqq     126159     126159          0     2.495s
------------------------------  ---------  ---------  ---------  ---------
             Total import time     670042     670042          0    34.836s
NIL
</code></pre><p>As you can see the control is still made for interactive developer usage,
which is fine for now but will have to change down the road, when the APIs
stabilize.
</p><p>Now, let's compare to reading directly from 
<em>MySQL</em>:
</p><pre><code>CL-USER&gt; (pgloader.mysql:stream-database &quot;dbname&quot;)
                    table name       read   imported     errors       time
------------------------------  ---------  ---------  ---------  ---------
               aaaaaaaaaa_aaaa      24595      24595          0     0.887s
          bbbbbbbbbb_bbbbbbbbb        326        326          0     0.617s
       cccccccccc_cccccccccccc      25126      25126          0     1.497s
      dddddddddd_dddddddddd_dd          8          8          0     0.582s
eeeeeeeeee_eeeeeeeeee_eeeeeeee      36825      36825          0     1.697s
             ffffffffff_ffffff        624        624          0     0.748s
     gggggggggg_ggggg_gggggggg       5638       5638          0     0.923s
                   hhh_hhhhhhh     120159     120159          0     3.525s
             iii_iiiiiiiiiiiii        661        661          0     0.449s
                       jjjjjjj      30027      30027          0     2.546s
                 kkk_kkkkkkkkk         12         12          0     0.330s
                    lll_llllll          4          4          0     0.323s
                      mmmm_mmm      29669      29669          0     1.227s
                   nnnn_nnnnnn     100197     100197          0     2.489s
                    oooo_ooooo      93555      93555          0     9.148s
                  pppp_ppppppp      76457      76457          0     6.713s
             qqqq_qqqqqqqqqqqq     126159     126159          0     4.571s
------------------------------  ---------  ---------  ---------  ---------
          Total streaming time     670042     670042          0    38.272s
NIL
</code></pre><p>The 
<em>streaming</em> here is a tad slower than the 
<em>importing</em> from files. Now if you
want to be fair when comparing those, you would have to take into account
the time it takes to 
<em>export</em> the data out from its source. When doing that
<em>export/import</em> dance, a quick test shows a timing of 
<code>1m4.745s</code>. Now, if we do
an 
<em>export only</em> test, it runs in 
<code>31.822s</code>. So yes streaming is a good thing to
have here.
</p><h2>Conclusion</h2><p>We just got twice as fast as the python version.
</p><p>Some will say that I'm not comparing fairly to the 
<em>Python</em> version of
pgloader here, because I could have implemented the streaming facility in
<em>Python</em> too. Well actually I did, the option are called 
<a href='http://tapoueh.org/pgsql/pgloader.html#sec13'>section_threads</a> and
<a href='http://tapoueh.org/pgsql/pgloader.html#sec15'>split_file_reading</a>, that you can set so that a reader is pushing data into a
set of queues and several workers are feeding each from its own queue. It
didn't help with performances at all. Once again, read about the infamous
<a href='http://docs.python.org/3/c-api/init.html#threads'>Global Interpreter Lock</a> to understand why not.
</p><center><img src='http://tapoueh.org/images/lisplogo_flag_128.png' /></center><p>So actually it's a fair comparison here where the new code is twice as fast
as the previous one, with only some hours of hacking and before spending any
time on optimisation. Well, apart from using a 
<em>producer</em>, a 
<em>consumer</em> and a
<em>queue</em>, which I almost had to have for streaming in between two database
connections anyways.
</p>]]>
  </description>
  <author>dim@tapoueh.org (Dimitri Fontaine)
  </author><pubDate>Tue, 12 Feb 2013 11:17:00 +0100</pubDate><guid isPermaLink="true">http://tapoueh.org/blog/2013/02/12-playing-with-pgloader.html</guid>
</item>

<item>
  <title>pgloader: what&#039;s next?
  </title>
  <link>http://tapoueh.org/blog/2013/01/28-pgloader-future.html
  </link>
  <description><![CDATA[<p><a href='http://tapoueh.org/pgsql/pgloader.html'>pgloader</a> is a tool to help loading data into 
<a href='http://www.postgresql.org/'>PostgreSQL</a>, adding some error
management to the 
<a href='http://www.postgresql.org/docs/9.2/interactive/sql-copy.html'>COPY</a> command. 
<code>COPY</code> is the fast way of loading data into
PostgreSQL and is transaction safe. That means that if a single error
appears within your bulk of data, you will have loaded none of it. 
<code>pgloader</code>
will submit the data again in smaller chunks until it's able to isolate the
bad from the good, and then the good is loaded in.
</p><center><img src='http://tapoueh.org/images/PDL_Adapter-250.png' /></center><center><em>Not quite this kind of data loader</em></center><p>In a recent migration project where we freed data from MySQL into
PostgreSQL, we used 
<code>pgloader</code> again. But the loading time was not fast enough
for the service downtime window that we had here. Indeed 
<a href='http://www.python.org/'>Python</a> is not known
for being the fastest solution around. It's easy to use and to ship to
production, but sometimes you not only want to be able to be efficient when
writing code, you also need the code to actually run fast too.
</p><h2>Faster data loading</h2><p>So I began writing a little dedicated tool for that migration in 
<a href='http://cliki.net/'>Common Lisp</a>
which is growing on me as my personal answer to the burning question: 
<em>python
2 or python 3</em>? I find 
<em>Common Lisp</em> to offer an even more dynamic programming
environment, an easier language to use, and the result often has
performances characteristics way beyond what I can get with python. Between
<a href='http://tapoueh.org/blog/2012/07/10-solving-sudoku.html'>5 times faster</a> and 
<a href='http://tapoueh.org/blog/2012/08/20-performance-the-easiest-way.html'>121 times faster</a> in some quite stupid benchmark.
</p><p>Here, with real data, my one shot attempt has been running more than 
<em>twice
as fast</em> as the python version, after about a day of programming.
</p><center><img src='http://tapoueh.org/images/lisp-python.png' /></center><center><em>See what's happening now?</em></center><p>The other thing here is that I've tempted to get 
<code>pgloader</code> work in parallel,
but at the time I didn't know about the 
<a href='http://docs.python.org/3/c-api/init.html#threads'>Global Interpreter Lock</a> that they
didn't find how to remove in Python 3 still, by the way. So my threading
attempts at making 
<code>pgloader</code> work in parallel are pretty useless.
</p><p>Whereas in 
<em>Common Lisp</em> I can just use the 
<a href='http://lparallel.org/'>lparallel</a> lib, which exposes
threading facilities and some 
<em>queueing</em> facilities as a mean to communicate
data in between workers, and have my code easily work in parallel for real.
</p><h2>Compatibility</h2><p>The only drawback that I can see here is that if you've been writing your
own 
<em>reformating modules</em> in python for 
<code>pgloader</code> (yes you can
<a href='http://tapoueh.org/pgsql/pgloader.html#sec21'>implement your own reformating module for pgloader</a>), then you would have to
port it to 
<em>Common Lisp</em>. Shout me an email if that's your case.
</p><h2>Next version</h2><p>So, I think we're going to have a 
<em>pgloader 3</em> someday, that will be way
faster than the current one, and bundle some more features: real parallel
behavior, ability to fetch non local data (connecting to MySQL directly, or
HTTP, S3, etc); and I'm thinking about offering a 
<code>COPY</code> like syntax to drive
the loading too, while at it. Also, the ability to discover the set of data
to load all by itself when you want to load a whole database: think of it as
a special 
<em>Migration</em> mode of operations.
</p><p>Some feature requests can't be solved easily when keeping the old 
<code>.INI</code>
syntax cruft, so it's high time to implement some kind of a real command
language. I have several ideas about those, in between the 
<code>COPY</code> syntax and
the 
<code>SQL*Loader</code> configuration format, which is both clunky and quite
powerful, too.
</p><p>After a beginning in 
<code>TCL</code> and a complete rewrite in python in 
<code>2005</code>, it looks
like 
<code>2013</code> is going to be the year of 
<em>pgloader 3</em>, in 
<em>Common Lisp</em>!
</p>]]>
  </description>
  <author>dim@tapoueh.org (Dimitri Fontaine)
  </author><pubDate>Mon, 28 Jan 2013 10:48:00 +0100</pubDate><guid isPermaLink="true">http://tapoueh.org/blog/2013/01/28-pgloader-future.html</guid>
</item>

<item>
  <title>Lost in scope
  </title>
  <link>http://tapoueh.org/blog/2013/01/09-Lost-in-scope.html
  </link>
  <description><![CDATA[<p>Thanks to 
<a href='https://twitter.com/mickael/status/288795520179240962'>Mickael</a> on 
<em>twitter</em> I got to read an article about loosing scope
with some common programming languages. As the blog article 
<a href='https://my.smeuh.org/al/blog/lost-in-scope'>Lost in scope</a>
references 
<em>functional programming languages</em> and plays with both 
<em>Javascript</em>
and 
<em>Erlang</em>, I though I had to try it out with 
<em>Common Lisp</em> too.
</p><center><img src='http://tapoueh.org/images/lambda.png' /></center><center><em>Let's have fun with lambda!</em></center><p>So, here we go with a simple Common Lisp attempt. The 
<em>Lost in scope</em> article
begins with defining a very simple function returning a boolean value, only
true when it's not 
<code>monday</code>.
</p><h2>Monday is special</h2><p>Keep in mind that the following example has been choosen to be simple yet
offer a case of 
<em>lexical binding shadowing</em>. It looks convoluted. Focus on the
<code>day</code> binding.
</p><pre><code>(defparameter *days*
  &#039;(monday tuesday wednesday thursday friday saturday sunday)
  &quot;List of days in the week&quot;)

(defun any-day-but-monday? (day)
  &quot;Returns a generalized boolean, true unless DAY is &#039;monday&quot;
  (member day (remove-if (lambda (day) (eq day &#039;monday)) *days*)))
</code></pre><p>So as you can see, in 
<em>Common Lisp</em> we just get away with a list of symbols
rather than a string that we split to have a list of strings, or an array of
strings, as in the examples with 
<em>python</em> and 
<em>ruby</em>.
</p><p>Now, the 
<em>generalized boolean</em> is either 
<code>nil</code> to mean false, or anything else
to mean 
<code>true</code>, and in that example the return value of 
<a href='http://www.lispworks.com/documentation/HyperSpec/Body/a_member.htm'>member</a> is a sub-list
that begins where the 
<em>member</em> was found:
</p><pre><code>CL-USER&gt; (any-day-but-monday? &#039;monday)
NIL

CL-USER&gt; (any-day-but-monday? &#039;tuesday)
(TUESDAY WEDNESDAY THURSDAY FRIDAY SATURDAY SUNDAY)
</code></pre><p>Oh, and as we work with 
<em>Common Lisp</em>, we're having a real 
<a href='http://www.gigamonkeys.com/book/lather-rinse-repeat-a-tour-of-the-repl.html'>REPL</a> where to play
directly with our code, no need to add 
<em>interactive</em> stanzas in the main
program text file just to be able to play with it. In 
<a href='http://common-lisp.net/project/slime/'>Emacs Slime</a> we just
use 
<code>C-M-x</code> on a 
<em>form</em> to have it available in the 
<em>REPL</em>, or 
<code>C-c C-l</code> to load the
whole file we're working on.
</p><p>So, we see that 
<em>Common Lisp</em> scoping rules are silently doing the right thing
here. Within the 
<a href='http://www.lispworks.com/documentation/HyperSpec/Body/f_rm_rm.htm'>remove-if</a> call we define a 
<em>lambda</em> function taking a single
parameter called 
<em>day</em>. It so happens that this parameter is shadowing the
<em>any-day-but-monday?</em> function parameter, and that shadowing only happens in
the 
<em>lexical scope</em> of the 
<em>lambda</em> we are creating. For a detailed discussion
about that concept, I would refer you to the 
<a href='http://www.cs.cmu.edu/Groups/AI/html/cltl/clm/node43.html'>Scope and Extent</a> chapter of
<em>Common Lisp the Language, 2nd Edition</em>.
</p><p>In 
<em>Common Lisp</em> we have both 
<em>lexical scope</em> and 
<em>dynamic extents</em>, and a
variable defined with 
<em>defparameter</em> or 
<em>defvar</em> or that you otherwise 
<a href='http://www.lispworks.com/documentation/HyperSpec/Body/s_declar.htm'>declare</a>
<em>special</em> will have a 
<em>dynamic extent</em>. Hence this section title.
</p><h2>Closures</h2><p>Now, the 
<a href='https://my.smeuh.org/al/blog/lost-in-scope'>lost in scope</a> article tries some more at finding a solution around
the scoping rules of the 
<em>python</em> and 
<em>ruby</em> languages, where the developer can
not easily instruct the language about the scoping rules he wants to be
using in a case by case way, as far as I can see.
</p><p>First, let's reproduce the problem by using a single variable that we bind
in all the closures. Those are called 
<em>callbacks</em> in the original article, so
I've kept using that name here.
</p><center><img src='http://tapoueh.org/images/callback.jpg' /></center><pre><code>(defparameter *callbacks-all-sunday*
    (loop
       for day in *days*
       collect (lambda () day))
  &quot;loop binds DAY only once&quot;)
</code></pre><p>In that example, there's only a single variable day that we reuse throughout
the 
<em>loop</em> construct, so that when the loop ends, we have a list of closures
all refering to the same variable, and this variable, by the end of the
loop, has 
<code>sunday</code> as its value.
</p><pre><code>CL-USER&gt; (mapcar #&#039;funcall *callbacks-all-sunday*)
(SUNDAY SUNDAY SUNDAY SUNDAY SUNDAY SUNDAY SUNDAY)
</code></pre><h2>Closures, take 2</h2><p>Now, the way to have what we want here, that is a list of closures each
having its own variable.
</p><pre><code>(defparameter *callbacks*
  (mapcar (lambda (day)
	    ;; for each day, produce a separate closure
	    ;; around its own lexical variable day
	    (lambda () day))
	  *days*)
  &quot;A list of callbacks to return the current day...&quot;)
</code></pre><p>And there we go:
</p><pre><code>CL-USER&gt; (mapcar #&#039;funcall *callbacks*)
(MONDAY TUESDAY WEDNESDAY THURSDAY FRIDAY SATURDAY SUNDAY)
</code></pre><h2>Conclusion</h2><p>Scoping rules are very important in any programming language, functional or
not, and must be well understood by programmers. I find that once again,
that topic has received a very deep thinking in 
<em>Common Lisp</em>, and the
language is giving all the options to its developers.
</p><center><img src='http://tapoueh.org/images/scope.png' /></center><center><em>What are your language of choice scoping rules?</em></center><p>I want to stress that in 
<em>Common Lisp</em> the scope rules are very clearly
defined in the standard documentation of the language. For instance, 
<em>defun</em>
and 
<em>let</em> both introduce a lexical binding, 
<em>defvar</em> and 
<em>defparameter</em> introduce
a 
<em>dynamic variable</em>.
</p><p>Also, as a user of the language you have the ability to 
<em>declare</em> any variable
as being 
<em>special</em> in order to introduce yourself a 
<em>dynamic variable</em>. In 
<code>C</code> you
can declare some variables as being 
<em>static</em>, which is something else and
frown with a very different set of problems.
</p>]]>
  </description>
  <author>dim@tapoueh.org (Dimitri Fontaine)
  </author><pubDate>Wed, 09 Jan 2013 11:07:00 +0100</pubDate><guid isPermaLink="true">http://tapoueh.org/blog/2013/01/09-Lost-in-scope.html</guid>
</item>

<item>
  <title>CL Happy Numbers
  </title>
  <link>http://tapoueh.org/blog/2012/11/20-CL-Happy-Numbers.html
  </link>
  <description><![CDATA[<p>A while ago I stumbled upon 
<a href='http://tapoueh.org/blog/2010/08/30-happy-numbers.html'>Happy Numbers</a> as explained in
<a href='http://programmingpraxis.com/2010/07/23/happy-numbers/'>programming praxis</a>, and offered an implementation of them in 
<code>SQL</code> and in
<code>Emacs Lisp</code>. Yeah, I know. Why not, though?
</p><center><img src='http://tapoueh.org/images/happy-numbers.png' /></center><p>Today I'm back on that topic and as I'm toying with 
<em>Common Lisp</em> I though it
would be a good excuse to learn me some new tricks. As you can see from the
earlier blog entry, last time I did attack the 
<em>digits</em> problem quite lightly.
Let's try a better approach now.
</p><pre><code>(defun digits (n)
  &quot;return the list of the digits of N&quot;
  (nreverse
   (loop for x = n then r
      for (r d) = (multiple-value-list (truncate x 10))
      collect d
      until (zerop r))))
</code></pre><p>As you can see I wanted to use that facility I like very much, the 
<code>for
x = n then r</code> way to handle first loop iteration differently from the
next ones. But I've been hinted on 
<code>#lisp</code> that there's a much better way to
write same code:
</p><pre><code>(defun integer-digits (integer)
  &quot;stassats version&quot;
  (nreverse
   (loop with remainder
      do (setf (values integer remainder) (truncate integer 10))
      collect remainder
      until (zerop integer))))
</code></pre><p>That code runs about twice as fast as the previous one and is easier to
reason about. It's using 
<code>setf</code> and the form 
<a href='http://www.lispworks.com/documentation/lw51/CLHS/Body/f_values.htm'>setf values</a>, something nice to
discover as it seems to be quite powerful. Let's see how to use it, even if
it's really simple:
</p><pre><code>CL-USER&gt; (integer-digits 12304501)
(1 2 3 0 4 5 0 1)
</code></pre><p>Let's move on to solving the 
<em>Happy Numbers</em> problem though:
</p><pre><code>(defun sum-of-squares-of-digits (integer)
  (loop with remainder
     do (setf (values integer remainder) (truncate integer 10))
     sum (* remainder remainder)
     until (zerop integer)))

(defun happy? (n &amp;optional seen)
  &quot;return true when n is a happy number&quot;
  (let* ((happiness (sum-of-squares-of-digits n)))
    (cond ((eq 1 happiness)      t)
	  ((memq happiness seen) nil)
	  (t
	   (happy? happiness (push happiness seen))))))

(defun find-happy-numbers (limit)
  &quot;find all happy numbers from 1 to limit&quot;
  (loop for n from 1 to limit when (happy? n) collect n))
</code></pre><p>And here's how it goes:
</p><pre><code>CL-USER&gt; (find-happy-numbers 100)
(1 7 10 13 19 23 28 31 32 44 49 68 70 79 82 86 91 94 97 100)

CL-USER&gt; (time (length (find-happy-numbers 1000000)))
(LENGTH (FIND-HAPPY-NUMBERS 1000000))
took 1,621,413 microseconds (1.621413 seconds) to run.
       116,474 microseconds (0.116474 seconds, 7.18%) of which was spent in GC.
During that period, and with 4 available CPU cores,
     1,431,332 microseconds (1.431332 seconds) were spent in user mode
       145,941 microseconds (0.145941 seconds) were spent in system mode
 185,438,208 bytes of memory allocated.
 1 minor page faults, 0 major page faults, 0 swaps.
143071
</code></pre><p>Of course that code is much faster than the one I wrote before both in 
<code>SQL</code>
and 
<em>Emacs Lisp</em>, the reason being that instead of writing the number into a
<em>string</em> with 
<code>(format t &quot;~d&quot; number)</code> then 
<a href='http://www.lispworks.com/documentation/HyperSpec/Body/f_subseq.htm'>subseq</a> to get them one after the
other, we're now using 
<a href='http://www.lispworks.com/documentation/HyperSpec/Body/f_floorc.htm'>truncate</a>.
</p><p>Happy hacking!
</p><h2>Update</h2><p>It turns out that to solve math related problem, some maths hindsight is
helping. Who would have believed that? So if you want to easily get some
more performances out of the previous code, just try that solution:
</p><pre><code>(defvar *depressed-squares* &#039;(0 4 16 20 37 42 58 89 145)
  &quot;see http://oeis.org/A039943&quot;)

(defun undepressed? (n)
  &quot;same as happy?, using a static list of unhappy sums&quot;
  (cond ((eq 1 n) t)
	((member n *depressed-squares*) nil)
	(t
	 (let ((h (sum-of-squares-of-digits n)))
	   (undepressed? h)))))

(defun find-undepressed-numbers (limit)
  &quot;find all happy numbers from 1 to limit&quot;
  (loop for n from 1 to limit when (undepressed? n) collect n))
</code></pre><p>Time to compare:
</p><pre><code>CL-USER&gt; (time (length (find-happy-numbers 1000000)))
(LENGTH (FIND-HAPPY-NUMBERS 1000000))
took 1,938,048 microseconds (1.938048 seconds) to run.
       290,902 microseconds (0.290902 seconds, 15.01%) of which was spent in GC.
During that period, and with 4 available CPU cores,
     1,778,021 microseconds (1.778021 seconds) were spent in user mode
       140,862 microseconds (0.140862 seconds) were spent in system mode
 185,438,208 bytes of memory allocated.
 3,320 minor page faults, 0 major page faults, 0 swaps.
143071

CL-USER&gt; (time (length (find-undepressed-numbers 1000000)))
(LENGTH (FIND-UNDEPRESSED-NUMBERS 1000000))
took 1,036,847 microseconds (1.036847 seconds) to run.
         5,372 microseconds (0.005372 seconds, 0.52%) of which was spent in GC.
During that period, and with 4 available CPU cores,
     1,018,708 microseconds (1.018708 seconds) were spent in user mode
        16,982 microseconds (0.016982 seconds) were spent in system mode
 2,289,152 bytes of memory allocated.
143071
CL-USER&gt; 
</code></pre>]]>
  </description>
  <author>dim@tapoueh.org (Dimitri Fontaine)
  </author><pubDate>Tue, 20 Nov 2012 18:20:00 +0100</pubDate><guid isPermaLink="true">http://tapoueh.org/blog/2012/11/20-CL-Happy-Numbers.html</guid>
</item>

<item>
  <title>Concurrent Hello
  </title>
  <link>http://tapoueh.org/blog/2012/11/04-Concurrent-Hello.html
  </link>
  <description><![CDATA[<p>Thanks to 
<a href='https://twitter.com/mickael/status/265191809100181504'>Mickael</a> on 
<em>twitter</em> I ran into that article about implementing a
very basic 
<em>Hello World!</em> program as a way to get into a new concurrent
language or facility. The original article, titled
<a href='http://himmele.blogspot.de/2012/11/concurrent-hello-world-in-go-erlang.html'>Concurrent Hello World in Go, Erlang and C++</a> is all about getting to know
<a href='http://golang.org/'>The Go Programming Language</a> better.
</p><p>To quote the article:
</p><blockquote><p>The first thing I always do when playing around with a new
software platform is to write a concurrent "Hello World" program. The
program works as follows: One active entity (e.g. thread, Erlang process,
Goroutine) has to print "Hello " and another one "World!\n" with the two
active entities synchronizing with each other so that the output always is
"Hello World!\n".
</p></blockquote><p>Here's my try in 
<a href='http://cliki.net/'>Common Lisp</a> using 
<a href='http://lparallel.org/'>lparallel</a> and some 
<em>local nicknames</em>, the
whole 
<code>23</code> lines of it:
</p><pre><code>(defun say-hello (helloq worldq n)
  (dotimes (i n)
    (format t &quot;Hello &quot;)
    (lq:push-queue :say-world worldq)
    (lq:pop-queue helloq))
  (lq:push-queue :quit worldq))

(defun say-world (helloq worldq)
  (when (eq (lq:pop-queue worldq) :say-world)
    (format t &quot;World!~%&quot;)
    (lq:push-queue :say-hello helloq)
    (say-world helloq worldq)))

(defun hello-world (n)
  (let* ((lp:*kernel*  (lp:make-kernel 2)) ; a new one each time, as we end it
	 (channel      (lp:make-channel))
	 (helloq       (lq:make-queue))
	 (worldq       (lq:make-queue)))
    (lp:submit-task channel #&#039;say-world helloq worldq)
    (lp:submit-task channel #&#039;say-hello helloq worldq n)
    (lp:receive-result channel)
    (lp:receive-result channel)
    (lp:end-kernel)))
</code></pre><p>If you want to play locally with that code, I've been updating it to a
<em>github</em> project named 
<a href='https://github.com/dimitri/go-hello-world'>go-hello-world</a>, even if it's coded in 
<em>CL</em>. See the
<code>package.lisp</code> in there for how I did enable the 
<em>local nicknames</em> 
<code>lp</code> and 
<code>lq</code> for
the 
<em>lparallel</em> packages.
</p><h2>Beware of the REPL</h2><p>In a previous version of this very article, I said that sometimes I get an
extra line feed in the output and I didn't understand why. Some great Common
Lisp folks did hint me about that: it's the 
<em>REPL</em> output that get
intermingled with the program output, and that's because the 
<code>hello-world</code>
main function was returning before the thing is over.
</p><p>I've added a 
<code>receive-result</code> call in it per worker so that it waits until the
end of the program before returning to the 
<em>REPL</em>, and that indeed fixes it. A
way to assert that is using the 
<code>time</code> macro, which was always intermingled
with the output before. It's fixed now:
</p><pre><code>CL-USER&gt; (time (go-hello-world:hello-world 1000))
Hello World!
...
Hello World!
(GO-HELLO-WORLD:HELLO-WORLD 1000)
took 27,886 microseconds (0.027886 seconds) to run.
      1,593 microseconds (0.001593 seconds, 5.71%) of which was spent in GC.
During that period, and with 4 available CPU cores,
     23,246 microseconds (0.023246 seconds) were spent in user mode
     14,427 microseconds (0.014427 seconds) were spent in system mode
 4,272 bytes of memory allocated.
 10 minor page faults, 0 major page faults, 0 swaps.
(#&lt;PROCESS lparallel kernel shutdown manager(62) [Reset] #x30200109F65D&gt; ...)
CL-USER&gt; 
</code></pre><h2>Conclusion</h2><p>While 
<em>Go</em> language seems to bring very interesting things on the table, such
as better compilation units and tools, I still think that the concurrency
primitives at the core of it are easy to find in other places. Which is a
good thing, as it means we know they work.
</p><p>That also means that we don't need to accept 
<em>Go</em> syntax as the only way to
properly solve that 
<em>concurrency</em> problem, I much prefer doing so with 
<em>Common
Lisp</em> (lack of?) syntax myself.
</p><h2>Update</h2><p>A previous version of this article was finished and published too quickly,
and the conclusion was made from a buggy version of the program. It's all
fixed now. Thanks a lot to people who contributed comments so that I could
fix it, and thanks again to 
<em>James M. Lawrence</em> for 
<a href='http://lparallel.org/'>lparallel</a>!
</p>]]>
  </description>
  <author>dim@tapoueh.org (Dimitri Fontaine)
  </author><pubDate>Sun, 04 Nov 2012 23:04:00 +0100</pubDate><guid isPermaLink="true">http://tapoueh.org/blog/2012/11/04-Concurrent-Hello.html</guid>
</item>

<item>
  <title>Fast and stupid?
  </title>
  <link>http://tapoueh.org/blog/2012/08/20-performance-the-easiest-way.html
  </link>
  <description><![CDATA[<p>I stumbled onto an interesting article about performance when using python,
called 
<a href='http://jiaaro.com/python-performance-the-easyish-way'>Python performance the easy(ish) way</a>, where the author tries to get
the bet available performances out of the dumbiest possible python code,
trying to solve a very simple and stupid problem.
</p><p>With so many 
<em>smart</em> qualifiers you can only guess that I did love the
challenge. The idea is to write the simplest code possible and see how
smarter you need to be when you need perfs. Let's have a try!
</p><h2>local python results</h2><p>Here's the code I did use to benchmark the python solution:
</p><pre><code>def sumrange(arg):
    return sum(xrange(arg))

def sumrange2(arg):
    x = i = 0
    while i &lt; arg:
        x += i
        i += 1
    return x


import ctypes
ct_sumrange = ctypes.CDLL(&#039;/Users/dim/dev/CL/jiaroo/sumrange.so&#039;)

def sumrange_ctypes(arg):
    return ct_sumrange.sumrange(arg)

if __name__ == &quot;__main__&quot;:
    import timeit
    t1 = timeit.Timer(&#039;import jiaroo; jiaroo.sumrange(10**10)&#039;)
    t2 = timeit.Timer(&#039;import jiaroo; jiaroo.sumrange2(10**10)&#039;)
    ct = timeit.Timer(&#039;import jiaroo; jiaroo.sumrange_ctypes(10**10)&#039;)

    print &#039;timing python sumrange(10**10)&#039;
    print &#039;xrange: %5fs&#039; % t1.timeit(1)
    print &#039;while:  %5fs&#039; % t2.timeit(1)
    print &#039;ctypes: %5fs&#039; % ct.timeit(1)
</code></pre><p>Oh. And the C code too, sorry about that.
</p><pre><code>#include &lt;stdio.h&gt;

int sumrange(int arg)
{
    int i, x;
    x = 0;

    for (i = 0; i &lt; arg; i++) {
        x = x + i;
    }
    return x;
}
</code></pre><p>And here's how I did compile it. The author of the inspiring article
insisted on stupid optimisation targets, I did follow him:
</p><pre><code>gcc -shared -Wl,-install_name,sumrange.so -o sumrange.so -fPIC sumrange.c -O0
</code></pre><p>And here's the result I did get out of it:
</p><pre><code>python jiaroo.py
timing python sumrange(10**10)
xrange: 927.039917s
while:  2377.291237s
ctypes: 5.297124s
</code></pre><p>Let's be fair, with 
<code>-O2</code> we get much better results:
</p><pre><code>timing python sumrange(10**10)
ctypes: 1.065684s
</code></pre><h2>Common Lisp to the rescue</h2><p>So let's have a try in Common Lisp, will you ask me, right?
</p><p>Here's the code I did use, you can see three different tries:
</p><pre><code>;;;; jiaroo.lisp
;;;
;;; See http://jiaaro.com/python-performance-the-easyish-way
;;;
;;; The goal here is to find out if CL needs to resort to C for very simple
;;; optimisation tricks like python apparently needs too, unless using pypy
;;; (to some extend).

(in-package #:jiaroo)

;;; &quot;jiaroo&quot; goes here. Hacks and glory await!

(defun sumrange-loop (max)
  &quot;return the sum of numbers from 1 to MAX&quot;
  (let ((sum 0))
    (declare (type (and unsigned-byte fixnum) max sum)
	     (optimize speed))
    (loop for i fixnum from 1 to max do (incf sum i))))

(defun sumrange-dotimes (max)
  &quot;return the sum of numbers from 1 to MAX&quot;
  (let ((sum 0))
    (declare (type (and unsigned-byte fixnum) max sum)
	     (optimize speed))
    (dotimes (i max sum)
      (incf sum i))))

(defun pk-sumrange (max)
  (declare (type (and unsigned-byte fixnum) max)
	   (optimize speed))
  (let ((sum 0))
    (declare (type (and fixnum unsigned-byte) sum))
    (dotimes (i max sum)
      (setf sum (logand (+ sum i) most-positive-fixnum)))))

(defmacro timing (&amp;body forms)
  &quot;return both how much real time was spend in body and its result&quot;
  (let ((start (gensym))
	(end (gensym))
	(result (gensym)))
    `(let* ((,start (get-internal-real-time))
	    (,result (progn ,@forms))
	    (,end (get-internal-real-time)))
       (values ,result (/ (- ,end ,start) internal-time-units-per-second)))))

(defun bench-sumrange (power)
  &quot;print execution time of both the previous functions&quot;
  (let* ((max (expt 10 power))
	 (lp-time (multiple-value-bind (r s) (timing (sumrange-loop max)) s))
	 (dt-time (multiple-value-bind (r s) (timing (sumrange-dotimes max)) s))
	 (pk-time (multiple-value-bind (r s) (timing (pk-sumrange max)) s)))
    (format t &quot;timing common lisp sumrange 10**~d~%&quot; power)
    (format t &quot;loop:       ~2,3fs ~%&quot; lp-time)
    (format t &quot;dotimes:    ~2,3fs ~%&quot; dt-time)
    (format t &quot;pk dotimes: ~2,3fs ~%&quot; pk-time)))
</code></pre><p>And here's the results:
</p><pre><code>CL-USER&gt; (bench-sumrange 10)
timing common lisp sumrange 10**10
loop:       11.213s 
dotimes:    7.642s 
pk dotimes: 22.185s 
NIL
</code></pre><h2>Discussion</h2><p>So python is very slow. C is pretty fast. And Common Lisp just in the
middle. Honnestly I expected better performances from my beloved Common Lisp
here, but I didn't try very hard, by using 
<a href='http://ccl.clozure.com/'>Clozure Common Lisp</a> which is not
the quicker Common Lisp implementation around. For this very benchmark, if
you're seeking speed use either 
<a href='http://sbcl.org/'>Steel Bank Common Lisp</a> or 
<a href='http://www.clisp.org/'>CLISP</a> which is
known to have a pretty fast bignums implementation (which you don't need in
64 bits in that game).
</p><p>On the other hand, I think that having to go write a C plugin and deal with
how to compile and deploy it in the middle of a python script is something
to avoid. When using Common Lisp you don't need to resort to that for the
<em>runtime</em> to get down from python 
<em>xrange</em> implementation at 
<code>927.039917s</code> down to
the 
<em>dotimes</em> implementation taking 
<code>7.642s</code>. That's about 
<code>121</code> times faster.
</p><p>So while 
<code>C</code> is even better, and while I would like a Common Lisp guru to show
me how to get a better speed here, I still very much appreciate the solution
here.
</p><p>Let's see the winning source code in 
<em>python</em> and 
<em>common lisp</em> to compare the
programmer side of things: how hard was it really to get 
<code>121</code> times faster?
</p><pre><code>def sumrange(arg):
    return sum(xrange(arg))
</code></pre><pre><code>(defun sumrange-dotimes (max)
  &quot;return the sum of numbers from 1 to MAX&quot;
  (let ((sum 0))
    (declare (type (and unsigned-byte fixnum) max sum)
	     (optimize speed))
    (dotimes (i max sum)
      (incf sum i))))
</code></pre><p>That's about it. Yes we can see some 
<em>manual</em> optimisation directives here,
which are optimisation 
<em>extra complexity</em>. Not to the same level as bringing a
compiled artifact that you need to build and deploy, though. Remember that
you will need to know the full path where to find the 
<code>sumrange.so</code> file on
the production system, in the optimised 
<em>python</em> case, so that's what we are
comparing against.
</p><p>Here's what happens without the optimisation, and with a smaller target:
</p><pre><code>CL-USER&gt; (time (jiaroo:sumrange-dotimes (expt 10 9)))
(JIAROO:SUMRANGE-DOTIMES (EXPT 10 9))
took 722,592 microseconds (0.722592 seconds) to run.
During that period, and with 2 available CPU cores,
     714,709 microseconds (0.714709 seconds) were spent in user mode
       1,183 microseconds (0.001183 seconds) were spent in system mode
499999999500000000

CL-USER&gt; (time (let ((sum 0)) (dotimes (i (expt 10 9) sum) (incf sum i))))
(LET ((SUM 0)) (DOTIMES (I (EXPT 10 9) SUM) (INCF SUM I)))
took 2,174,767 microseconds (2.174767 seconds) to run.
During that period, and with 2 available CPU cores,
     2,156,549 microseconds (2.156549 seconds) were spent in user mode
        10,225 microseconds (0.010225 seconds) were spent in system mode
499999999500000000
</code></pre><p>We get a 
<code>3</code> times speed-up from those 2 lines of lisp optimisation
directives, which is pretty good. And it's exponential as I didn't have the
patience to actually wait until the non optimised 
<code>10^10</code> run finished, I
killed it.
</p><h2>Conclusion</h2><p>That's a case here where I don't know how to reach 
<code>C</code> level of performances
with Common Lisp, which could just be because I don't know yet how to do.
</p><p>Still, getting a 
<code>121</code> times speedup when compared to the pure 
<em>python</em> version
of the code is pretty good and encourages me to continue diving into Common
Lisp.
</p>]]>
  </description>
  <author>dim@tapoueh.org (Dimitri Fontaine)
  </author><pubDate>Wed, 22 Aug 2012 16:05:00 +0200</pubDate><guid isPermaLink="true">http://tapoueh.org/blog/2012/08/20-performance-the-easiest-way.html</guid>
</item>

<item>
  <title>Solving Every Sudoku Puzzle
  </title>
  <link>http://tapoueh.org/blog/2012/07/10-solving-sudoku.html
  </link>
  <description><![CDATA[<p><a href='http://norvig.com/'>Peter Norvig</a> published a while ago a very nice article titled
<a href='http://norvig.com/sudoku.html'>Solving Every Sudoku Puzzle</a> wherein he presents a programmatic approach to
solving that puzzle game.
</p><center><a href='http://en.wikipedia.org/wiki/Sudoku'><img src='http://tapoueh.org/images/sudoku.png' /></a></center><p>The article is very well written and makes it easy to think that coming up
with the code for such a solver is a very easy task, you apply some basic
problem search principles and there you are. Which is partly true, in fact.
Also, he uses 
<code>python</code>, and that means that a lot of trivial programming
activities are not a concern anymore, such as memory management.
</p><p>As I've been teaching myself 
<a href='http://www.cliki.net/Common%20Lisp'>Common Lisp</a> for some weeks now I though I would
like to read a lisp version of his code, and the article even has a section
titled 
<em>Translations</em>. Unfortunately, no lisp version is available there. One
might argue that 
<a href='http://clojure.org/'>Clojure</a> is a decent enough lisp, but my current quest is
all about 
<em>Common Lisp</em> really. So I had to write one myself.
</p><pre><code>CL-USER&gt; (sudoku:print-puzzle
	  (sudoku:solve-grid
&quot;530070000600195000098000060800060003400803001700020006060000280000419005000080079&quot;))
5 3 4 | 6 7 8 | 9 1 2 
6 7 2 | 1 9 5 | 3 4 8 
1 9 8 | 3 4 2 | 5 6 7 
------+-------+------
8 5 9 | 7 6 1 | 4 2 3 
4 2 6 | 8 5 3 | 7 9 1 
7 1 3 | 9 2 4 | 8 5 6 
------+-------+------
9 6 1 | 5 3 7 | 2 8 4 
2 8 7 | 4 1 9 | 6 3 5 
3 4 5 | 2 8 6 | 1 7 9 
took 1,974 microseconds (0.001974 seconds) to run.
During that period, and with 2 available CPU cores,
     1,894 microseconds (0.001894 seconds) were spent in user mode
        88 microseconds (0.000088 seconds) were spent in system mode
 174,320 bytes of memory allocated.
#&lt;SUDOKU::PUZZLE #x3020023BB9FD&gt;
</code></pre><h2>Comments on the python version</h2><p>Norvig's article is very well written, I think. By that I mean that by
reading it you're confident that you've understood the problem and how the
solution is articulated, so you almost think you don't need to really try to
understand the code, it's just an illustration of the text.
</p><p>Well, not so much. When you want to port the exact same algorithm you have
to understand exactly what the code is doing so that you're not implementing
something else. All the more when, as I did, you want to use some other data
structure.
</p><p>My goal was not to rewrite the code as-is, but to try and come up with
<em>idiomatic</em> lisp code implementing Norvig's solution. So rather than using
<em>strings</em> and 
<em>dictionaries</em> (in lisp, they still call them a 
<a href='http://www.lispworks.com/documentation/lw50/CLHS/Body/f_mk_has.htm'>hash table</a>) I've
been using more natural data structures.
</p><p>The 
<em>python</em> code is really not that easy to follow, full of functional
programming veteran tricks. I mean avoiding 
<em>exceptions</em> and simply
returning 
<code>False</code> whenever there's a problem, and using functions such as
<code>all</code> and 
<code>some</code> to manage that. It's certainly working, it's not making the
code any easier to read.
</p><p>To summarize, that code looks like it's been written by someone smart who
didn't want to spend more than a couple of hours on it, and did take all
known trustworthy shortcuts he could to achieve that goal. Quality and
readability certainly weren't the key motive. I've been quite deceived after
reading a very good article.
</p><h2>Comments on the common lisp version</h2><p>Keep in mind that I'm just a 
<em>Common Lisp</em> newbie. I've been told some good
pieces of advice by knowledgeable people though, so with some luck my
implementation is somewhat 
<em>lispy</em> enough.
</p><p>So we start by defining some data structures and low-level functions to
build up the more complex one, so that it's easier to read and debug. The
<em>sudoku</em> puzzle is then a grid of digits and a grid of possible values in
places where the digits are yet unknown.
</p><p>The way to represent that 9x9 grid is with using 
<a href='http://www.lispworks.com/documentation/lw51/CLHS/Body/f_mk_ar.htm'>make-array</a>:
</p><pre><code>(make-array &#039;(9 9)
	    :element-type &#039;(integer 0 9)
	    :initial-element 0)
</code></pre><p>Then the possible values. I though about using a 
<code>bit-vector</code> (and actually I
did implement it that way), then I've been told that the 
<em>Common Lisp</em> way to
approach that is using 
<a href='http://psg.com/~dlamkins/sl/chapter18.html'>2-complement integer representation</a>, as we have
plenty of functions to operate numbers that way. I wouldn't believe that
would make the code simpler, but in fact it really did, see:
</p><pre><code>CL-USER&gt; #b111111111
511
CL-USER&gt; (logcount #b111111111)
9
CL-USER&gt; (logcount 511)
9
CL-USER&gt; (logbitp 3 #b100100100)
NIL
CL-USER&gt; (logbitp 2 #b100100100)
T
CL-USER&gt; (format nil &quot;~2r&quot; (logxor #b111111111 (ash 1 4)))
&quot;111101111&quot;
CL-USER&gt; (logbitp 4 (logxor #b111111111 (ash 1 4)))
NIL
</code></pre><p>With that in mind, we can write the following code:
</p><pre><code>(defun count-remaining-possible-values (possible-values)
  &quot;How many possible values are left in there?&quot;
  ;; we could raise an empty-values condition if we get 0...
  (logcount possible-values))

(defun first-set-value (possible-values)
  &quot;Return the index of the first set value in POSSIBLE-VALUES.&quot;
  (+ 1 (floor (log possible-values 2))))

(defun only-possible-value-is? (possible-values value)
  &quot;Return a generalized boolean which is true when the only value found in
   POSSIBLE-VALUES is VALUE&quot;
  (and (logbitp (- value 1) possible-values)
       (= 1 (logcount possible-values))))

(defun list-all-possible-values (possible-values)
  &quot;Return a list of all possible values to explore&quot;
  (loop for i from 1 to 9
     when (logbitp (- i 1) possible-values)
     collect i))

(defun value-is-set? (possible-values value)
  &quot;Return a generalized boolean which is true when given VALUE is possible
   in POSSIBLE-VALUES&quot;
  (logbitp (- value 1) possible-values))

(defun unset-possible-value (possible-values value)
  &quot;return an integer representing POSSIBLE-VALUES with VALUE unset&quot;
  (logxor possible-values (ash 1 (- value 1))))
</code></pre><p>You can see here that I was also under the influence of a recent reading
about 
<a href='http://gar1t.com/blog/2012/06/10/solving-embarrassingly-obvious-problems-in-erlang/'>making it obvious</a>, or so called 
<a href='http://dieswaytoofast.blogspot.fr/2012/07/erlang-why-so-many-seemingly-identical.html'>intentional programming</a>, following
what 
<a href='http://armstrongonsoftware.blogspot.fr/'>Joe Armstrong</a> has to say about it:
</p><blockquote><p>  Intentional programming is a name I give to a style of programming where
  the reader of a program can easily see what the programmer intended by
  their code. The intention of the code should be obvious from the names of
  the functions involved and not be inferred by analysing the structure of
  the code. (Reading the code should) precisely expresses the intention of
  the programmerhere no guesswork or program analysis is involved, we
  clearly read what was intended.
</p><small>Joe Armstrong in <cite title='Intentional Programming'>Intentional Programming</cite></small></blockquote><p>So there we go with function names such as 
<code>count-remaining-possible-values</code>,
that will help when reading some more complex code, as in the following, the
meat of the solution:
</p><pre><code>(defmethod eliminate ((puzzle puzzle) row col value)
  &quot;Eliminate given VALUE from possible values in cell ROWxCOL of PUZZLE, and
   propagate when needed&quot;
  (with-slots (grid values) puzzle
    ;; if already unset, work is already done
    (when (value-is-set? (aref values row col) value)
      ;; eliminate the value from the set of possible values
      (let* ((possible-values
	      (unset-possible-value (aref values row col) value)))
	(setf (aref values row col) possible-values)

	;; now if we&#039;re left with a single possible value
	(when (= 1 (count-remaining-possible-values possible-values))
	  (let ((found-value (first-set-value possible-values)))
	    ;; update the main grid
	    (setf (aref grid row col) found-value)

	    ;; eliminate that value we just found in all peers
	    (eliminate-value-in-peers puzzle row col found-value)))

	;; now check if any unit has a single possible place for that value
	(loop
	   for (r . c)
	   in (list-places-with-single-unit-solution puzzle row col value)
	   do (assign puzzle r c value))))))
</code></pre><p>So that lisp code is quite verbose and at 389 lines almost doubles the 201
lines Norvig had. When clarity is part of the goal, that's hard to avoid, I
hope I made a good case that this is not due to lisp being overly verbose by
itself.
</p><h2>Comments on the development environment</h2><p>Or why I even considered 
<em>Common Lisp</em> as an interesting language for that
kind of exercise, and some more. 
<em>I'll have to tell about re-sharding data
live with 16 threads and 256 databases, all in CL, someday</em>.
</p><p>So I've been doing some 
<em>Emacs Lisp</em> development for a while now, and the part
that makes that so much fun is the instant reward. You write some code in
your editor, type a key chord (usually, that's 
<code>C-M-x runs the command
eval-defun</code>) and your code is loaded up, ready to be tested. In 
<em>Emacs Lisp</em>
the test can be simply using your editor and watching the new behavior
taking place, or playing in the 
<code>M-x ielm</code> console. When the code is not
ready, it crashes, and you're left in the interactive debugger, where you
can use 
<code>C-x C-e runs the command eval-last-sexp</code> to evaluate any expression
in your source and see its value in the current 
<em>debug frame</em>.
</p><p>That way of working is a huge productivity boost, that I've been missing
much when getting back to writing C code for PostgreSQL. I can't 
<code>C-M-x</code> the
current function and go write some 
<code>SQL</code> to test it right away, I have to
<em>compile</em> the whole source tree, then 
<em>install</em> the new binaries, then 
<em>restart</em>
the test server and then open up a 
<em>psql</em> console to interact with the new
code. Of course I could just 
<code>make check</code> and watch the results, but then if I
attach a 
<em>debugger</em> it complains that the code on-disk is more recent than the
code in the 
<em>core dump</em>.
</p><p>What if you want 
<em>Emacs Lisp</em> integrated facilities and something made for
general programming rather than suited to building a text editor? Don't get
me wrong, you can probably find more production ready code in 
<em>elisp</em> than in
many other languages, just because Emacs has been there for about 35 years.
Editor targeted production code, though.
</p><p>This integrated development cycle is all the same when you're using 
<em>Common
Lisp</em>. The awesome 
<a href='http://common-lisp.net/project/slime/'>Superior Lisp Interaction Mode for Emacs</a> is providing
exactly that experience. Just run 
<code>M-x slime</code> and then as you define your code
you can 
<code>C-M-x</code> the function at point, see the compilation errors and warnings
if any in the associated 
<em>REPL</em>, and just try your code. I tend to mostly play
in the command line, it's possible to just use 
<code>C-x C-e</code> while typing too.
</p><h2>Performances</h2><p>Of course we do care! After all the original article came with a quite
detailed performance analysis with graphs and all. I won't be reproducing
that, sorry. I'll just show you what penalty you get for using an older
language specification, much more dynamic and with more features than
python, and with a great, scratch that, awesome development environment.
</p><p>Oh wait, that's the other way round, no penalty, it's actually so much
faster!
</p><h3>Python version perfs</h3><p>The results I got on my desktop machine are about twice as fast as in the
original article, I guess newer machines and newer python have something to
say for that:
</p><pre><code>dim ~/dev/CL/sudoku python sudoku.dim.py 
  All tests pass.
  Solved 50 of 50 easy puzzles (avg 0.01 secs (151 Hz), max 0.01 secs).
  Solved 95 of 95 hard puzzles (avg 0.02 secs (42 Hz), max 0.12 secs).
  Solved 11 of 11 hardest puzzles (avg 0.01 secs (115 Hz), max 0.01 secs).
</code></pre><p>That makes an average of 
<code>(50*151 + 95*42 + 11*115) / (50+95+11) =
82Hz</code>.
</p><p>That seems pretty good, let's continue.
</p><p>As you can see I've cut away the 
<em>random puzzle</em> part, that's because I was
too lazy to implement that part, which didn't seem all that interesting to
me. If you think that's a problem and need solving, I accept patches.
</p><h3>Common lisp version perfs</h3><p>When using 
<a href='http://sbcl.org/'>SBCL</a> on the same machine, what I got was:
</p><pre><code>(sudoku:solve-example-grids)
  Solved 50 of 50 easy puzzles (avg .0021 sec (471.7 Hz), max 0.015 secs).
  Solved 95 of 95 hard puzzles (avg .0022 sec (446.0 Hz), max 0.008 secs).
  Solved 11 of 11 hardest puzzles (avg .0018 sec (550.0 Hz), max 0.003 secs).
</code></pre><p>With the same way to compute the average, we now have 
<code>461.6Hz</code>.
</p><p>Now, that's between 3 times and more than 
<strong>10 times faster</strong> than the python
version (taken collection per collection), for a comparable effort, a much
better development environment, and the same all dynamic no explicit
compiling approach.
</p><h2>Conclusion</h2><p>I guess I'm fond of 
<em>Common Lisp</em>, which I already saw coming (so did you,
right?), and now I have some public article and code to share about why :)
</p><p>The code is hosted at 
<a href='https://github.com/dimitri/sudoku'>https://github.com/dimitri/sudoku</a> if you're
interested, with the necessary files to reproduce, some docs, etc.
</p><p>Also, apart from using 
<em>integers</em> as 
<em>bitfields</em>, which I did more for being
lispy than for performances, I did very little effort for optimizing the
code. It's quite naive in this respect, yet allow me an average of 
<code>461.6Hz</code>
rather than 
<code>82Hz</code>, that's 
<em><strong>5.6 times faster</strong></em> average.
</p><p>So yes, I will continue to invest some precious time in 
<em>Common Lisp</em> as a
very good interactive scripting language, and maybe more than that.
</p>]]>
  </description>
  <author>dim@tapoueh.org (Dimitri Fontaine)
  </author><pubDate>Tue, 10 Jul 2012 20:37:00 +0200</pubDate><guid isPermaLink="true">http://tapoueh.org/blog/2012/07/10-solving-sudoku.html</guid>
</item>
 </channel>
</rss>