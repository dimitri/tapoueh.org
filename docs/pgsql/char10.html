<!DOCTYPE HTML>
<html>
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta charset=utf-8 /> 
    <meta name=viewport content="width=device-width" /> 
    <meta name="generator" content="Emacs Muse Blog" /> 
    <meta description="blog de Dimitri Fontaine, Expert PostgreSQL - 2ndQuadrant France" />
    <meta property="og:title" content="tapoueh.org"/>
    <meta property="og:type" content="non_profit"/>
    <meta property="og:url" content="http://tapoueh.org/pgsql/char10.html"/>
    <meta property="og:image" content="http://tapoueh.org/static/2ndQuadrant-cross.png"/>
    <meta property="og:site_name" content="Tapoueh"/>
    <meta property="fb:admins" content="545062436"/>

    <title>Expertise PostgreSQL</title>
    <link rel='stylesheet' type='text/css' media='all' href='../static/styles.css' />
    <link rel='stylesheet' type='text/css' medial='all' href='../static/bootstrap/css/bootstrap.min.css' />
    <link rel='stylesheet' type='text/css' medial='all' href='../static/FontAwesome/css/font-awesome.min.css' />

    <link rel='stylesheet' type='text/css' medial='all' href='../static/highlight.js/styles/sunburst.css' />
    <script type='text/javascript' src='../static/highlight.js/highlight.pack.js'></script>
    <script>hljs.initHighlightingOnLoad();</script>

    <script type='text/javascript' src='../static/jquery-1.9.1.min.js'></script>
    <link rel='stylesheet' type='text/css' medial='all' href='../static/jqcloud.css' />
    <script type='text/javascript' src='../static/jqcloud-1.0.4.min.js'></script>

    <!-- <link href="/static/bootstrap/css/bootstrap.min.css" rel="stylesheet"/>
    <link rel="stylesheet" href="/static/FontAwesome/css/font-awesome.min.css"/>
    <link rel="stylesheet" href="/static/highlight.js/styles/sunburst.css">
    <script src="/static/highlight.js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    <script src="/static/jquery-1.9.1.min.js"></script>
    <link rel="stylesheet" type="text/css" href="/static/jqcloud.css" />
    <script type="text/javascript" src="/static/jqcloud-1.0.4.min.js"></script>
    -->

    <script type="text/javascript" src="http://apis.google.com/js/plusone.js">{lang: 'fr'}</script>
    
  </head>
  <body>

   <div id="wrap">

    <div class="navbar navbar-inverse navbar-fixed-top">
      <div class="nav-collapse collapse">
	<p class="pull-right">
	  <a href="http://2ndquadrant.com">
	    <img alt="logo" src="/static/2ndQuadrant-cross.png"
		 style="margin-right: 8px; margin-top: 4px; height: 32px;"/>
	  </a>
	</p>
	<div class="navbar-inner">
	 <a class="brand" style="margin-left: 0.2em; margin-top: -4px;"
	    href="http://expert-postgresql.fr/">
	  <img src="/static/expert-postgresql.png"></a>
	 <ul class="nav">
	   <li><a href="../blog/index.html">
               <i class="icon-book"></i> blog</a>
           </li>
	   <li><a href="../projects.html">
               <i class="icon-suitcase"></i> projects</a>
           </li>
	   <li><a href="../conferences.html">
               <i class="icon-plane"></i> confs</a>
           </li>
	   <li><a href="../about.html">
               <i class="icon-beer"></i> about</a>
           </li>
	   <li><a href="../rss/tapoueh.xml">
               <i class="icon-rss"></i> rss</a>
           </li>
	 </ul>
	</div>
      </div>
    </div>

    <div class="container">
      <div class="row">
	<div class="span4">
	  <div></div>
	  <div class='date'>Saturday, July 03 2010 <i class='icon-calendar'></i></div>
	  <div style='text-align: right;'><span></span> </div>
	  

	  <div style="margin-top: 1em;">
	    <div class='span2 pull-left'><a class='thumbnail' href='../blog/archives.html'><img class='img-polaroid' style='width: 160px; height: 120px;' src='../thumbnails/article2.gif' /></a></div>
	    <div id='social' class='span1 pull-right social'><ul><li><g:plusone size='tall' href='http://tapoueh.org/pgsql/char10'></g:plusone></li><li><a href='http://twitter.com/share' class='twitter-share-button' data-via='tapoueh' data-count='vertical'>Tweet</a><script type='text/javascript' src='http://platform.twitter.com/widgets.js'></script></li></ul></div>
	  </div>

	  <div id="prevnext">
	    
	    <div></div>
	    <div></div>
	  </div>
	  
	  <div>
	    <div id="left-tag-cloud" class="jqcloud"
		 style="width: 300px; height: 650px; position: absolute; top: 475px;">
	    </div>
            <script>
            var word_list;
            
            $.ajax({
                type: 'GET',
                url: '/cloud.json',
                dataType: 'json',
                success: function(response) { word_list = response; },
                data: {},
                async: false
            });
             
            $(document).ready(function() {
               $("#left-tag-cloud").jQCloud(word_list,
	                {shape: "rectangular",
	                 removeOverflowing: false});
            });
            </script>
	  </div>

	</div>

	<div id="title" class="span8">
	  <h1>Next Generation PostgreSQL </h1>
	</div>
	
	<div id="content" class="span8">
<p><a href='http://char10.org/'>CHAR(10)</a> was a great venue with great people. The conference itself and the
"Hall's Talks" have been a huge source of ideas popping up. Again...
</p><center><img src='../images/oriel_college_first_quad.jpg' /></center><p>As I've been talking to several people about those ideas over the time and
past conferences, I figured I should try to put them down now. And the new
ones too. So in this article I'll try to organise and share those ideas that
I'm growing in my head, in the hope that some people will think that maybe
the ideas are worthwile enough to think about them, make them theirs, then
why not work on them!
</p><p>Organising ideas is all about putting them togother in boxes and attach to
them the more meaningfull label you can think of. Here, I think the labels
are MVCC in the Cloud, and How to Further Optimise PostgreSQL.
</p><h2>DISCLAIMER</h2><p>Those ideas that I try to organise and present in this documents are
expressed like if they were good designs. I realise that may well be not
true, but that's the best way I can think of to share them. So not only that
is a 
<em>Work In Progress</em> but you're welcome to find this collection nothing but
a waste of time. Apologies if that's the case.
</p><p>Also those are all 
<em>bird's eye view</em> and lots of implementation problems are
certainly hidden all around, but my current knowlegde of the internals of
the system won't allow me to be the first to catch them. Hint, hint.
</p><h2>MVCC in the Cloud</h2><p>Nowadays all the rage is in the virtualised environments, dynamic sizing of
resources and pay-only-what-you-use. They even found a funny name to sell
this, welcome to the Cloud!
</p><p>The problem is that while it's quite easy to understand how to apply such a
reasonning to stateless services, such as webservers, it's a lot more
difficult to apply the same to persistent storage services, like, say, a
traditional relational database. Let's not forget that the main services of
an RDBMS ain't interpreting SQL, but offering transactions and durability. 
</p><p>I think forgetting about that would be awful and could lead to strange
solutions appearing, where you play with highly dynamic non-structured
non-normalized caches that people would be happy to use as persistent
storage services. Oh and of course only provide APIs, so that the
application has to know all about the storage and data location, rather than
having to describe what data they need. Yeah, 
<a href='http://en.wikipedia.org/wiki/Declarative_programming'>declarative languages</a> are not
cool anymore, we don't want no Lisp, no Haskell, no RegExp, no CSS, no XML,
no XSLT, NoSQL!
</p><p>So, well, is it possible to apply those Cloud concepts to ACID persistent
storage services such as PostgreSQL, and get some benefits out of that? I
think so, and will try to explain how. If you're a web indexer robot of some
kind, please consider attaching that to your 
<em>distributed computing</em> label,
would you have one.
</p><h3>Transparent cluster</h3><p>The first problem I'd try to have an idea about is how to offer a
transparent service to users in the presence of replication. Say you're
running a master slave 
<em>Hot Standby</em> setup, using 
<em>Streaming Replication</em> for
transfering the data. Now the application code will either connect to the
master or to the slave, and depending on this choice will not be able to
benefit from the same 
<em>service</em>.
</p><p>So the idea would be for the 
<em>service</em> offered by both the 
<em>nodes</em> of this
<em>cluster</em> to be the same, as far as the user is concerned. This simplification
could come in two steps. First, you need 
<em>XID</em> feedback from the slave to the
master. That means that the older snapshot still in use in any slave should
be known by the master, so that it will refrain from cleaning up the house
while the party's still ongoing. It's rude to do that, even more so to
friends.
</p><p>Once you can trust that any snapshot on the slave is still valid on the
master, the second step would be for the slave to forward a transaction to
its master (
<code>primary_coninfo</code>) as soon as it can't serve it itself. I think
this is as soon as the transaction would require the 
<em>Virtual XID</em> to turn
into a real one. Oh, and as you know the current transaction's snapshot is
still valid on the master, the only thing you need is being able to open a
transaction on this very 
<em>snapshot</em> over there.
</p><p>And it so happens that being able to connect to a system and ask for a given
snapshot you know about already would be useful for other projects (parallel
dumps and distributing query processing). I think some hackers are already
contemplating on how to offer such a facility. Thanks guys!
</p><h3>But I want my ticket to the cloud!</h3><p>The previous idea goes a long way to help offering a global service to users
but does nothing to help solve the real complex 
<em>Cloud</em> challenges. Those are
related to the flexibility you want from the 
<em>Cloud</em>, the marketing name of it
being 
<em>Elasticity</em>. To be a 
<em>Cloud</em> service, you need to be able to add and
remove nodes to your service and stay online.
</p><p>So, let's continue thinking, but focusing a little more.
</p><p>Lots of efforts are ongoing in the domains of replications and remote acces
to data. I'm thinking about 
<a href='http://postgres-xc.sourceforge.net/'>Postgres-XC</a>, 
<a href='http://www.postgres-r.org/'>Postgres-R</a> and 
<a href='http://wiki.postgresql.org/wiki/SQL/MED'>SQL/MED</a> related
work. I like those projects and architecture but I don't think getting them
to the 
<em>Cloud</em> will be practical, except for 
<code>SQL/MED</code>.
</p><p>If you missed it, 
<code>SQL/MED</code> is the part of the 
<a href='http://www.wiscorp.com/SQLStandards.html'>standard</a> that allows a database
server to expose objects from the 
<em>outer space</em>. Meaning anything from a local
text file to another database server, of the same or another
technology. It's very powerful and valuable, but the limitation is that you
can't reach outer space within your transaction boundaries. That's a pretty
good 
<em>limit</em>, though, because it means you now get autonomous transactions
(you embed a transaction into another and their 
<code>COMMIT/ROLLBACK</code> statuses are
independant, or, say, 
<em>autonomous</em>).
</p><p>To provide elastic clustering in the 
<em>Cloud</em>, with some kind of transparency
for the user, you don't want to lose MVCC, that thing that allows the
database to respect the 
<em>ACID</em> guarantees. Maintaining MVCC in a cluster is,
on the other hand, exactly what 
<em>Postgres-XC</em>, 
<em>Postgres-R</em> and 
<a href='http://portal.acm.org/citation.cfm?id=1113574.1113576'>Middle-R</a> are
working on.
</p><p>The problem there is that either you have full support of PostgreSQL
features, that's 
<em>Postgres-R</em>, or you have data distribution, that's
<em>Postgres-XC</em> (which does not yet provide node fault-tolerance, by the way).
</p><p>In the latter case, it's even a big problem to offer all the PostgreSQL
features (user defined functions, triggers, rules, etc) because for
parallelizing query processing (you 
<em>have</em> to do that if you're distributing
the data) the vast majority of the work they're doing is in the planner and
optimiser. I don't even want to think how they will make it possible for a
trigger to touch a non-local table in a distributed cluster.
</p><p>And one of the thing 
<em><strong>Markus</strong></em> showed at 
<a href='http://char10.org/talk-schedule-details#talk13'>CHAR(10)</a> is that what's killing
replicated systems really is the communication overhead. In this light,
really, the worst case would be 
<a href='http://en.wikipedia.org/wiki/Serializability#Common_mechanism_-_SS2PL'>Two Phase Commit</a>, but it appears that
<em>Postgres-XC</em> might not be that far away. The real big winner is 
<em>Postgres-R</em>
which shows very low communication overhead, detailed next.
</p><h3>Remote tablespace</h3><p>The idea here would be to keep the global MVCC facilities that those project
provide. I'll confess that I like the 
<em>Postgres-R</em> implementation of it, even
if it relies on a 
<a href='http://en.wikipedia.org/wiki/Group_communication_system'>Group Communication System</a> to deliver a total 
<em>commit</em>
ordering of transactions in the cluster. But the advantage is that the
design has been made with the idea of supporting dynamic cluster
configuration: you can accept new nodes and lose some others online.
</p><p>Now the 
<code>GCS</code> is a hard-sell in our community, because finding a good
implementation of one ain't an easy task, and it seems that some of
PostgreSQL developers either burnt themselves on a 
<em>work-in-progress</em>
implementation, or just don't trust the theory. So I'd like to find out some
trustworthy and light way to derive a distributed sequence that would be a
reference for XID ordering in the cluster. I'm reading docs on 
<a href='http://en.wikipedia.org/wiki/Vector_clock'>Vector Clock</a>
and 
<a href='http://en.wikipedia.org/wiki/Paxos_algorithm'>Paxos algorithm</a> now.
</p><p>So, say we have a global MVCC we can trust. Then, the data distribution
would be easier to address at the 
<em>executor</em> level I think. What you need to
be able to say is that some data are stored on another node. You need the
catalogs to exists on all the nodes, and I think that the physical location
property of the data is already well defined in the notion of a
<em>tablespace</em>. So we should extend this notion to remote PostgreSQL nodes that
happen to share the MVCC details with us.
</p><p>That certainly already means that any and all 
<code>DDL</code> targeting the remote
tablespace needs to happen using 
<em>2 Phase Commit</em>, or maybe an asynchronous
model would fit here, I'm not sure. But I don't really see how. The data
would exist only on the master node for any given tablespace, but the
catalogs should certainly be in-sync, unless you want to accept a whole new
error domain, but which could be made to look like a serialisation failure
after all: if the 
<code>DDL</code> changes are asynchronous, then your problem is the
lack of remote 
<em>exclusive lock</em>, I guess.
</p><p>That something I've been thinking about for a long time, you can find the
<a href='http://wiki.postgresql.org/wiki/Distributing_PostgreSQL'>distributing PostgreSQL wiki page</a> and see its history for the details. If
you see so you'll notice that the page needs some refreshing (at least the
ideas behind the wiki page got some refresh here and there, the most recent
and important one being 
<a href='http://char10.org'>CHAR(10)</a>.
</p><p>Then the planner would work as it's doing now, but certain parts of the plan
would have to be handled to another PostgreSQL instance. And as we're
working with a global MVCC idea, we keep transactional behavior.
</p><p>This does not provide data distributing off itself, for that to happen you
would use current 
<a href='http://www.postgresql.org/docs/8.4/static/ddl-partitioning.html'>ddl partitioning</a> facilities and have the partitions sit on
different 
<em>remote</em> tablespaces. As soon as you do that, you can distribute the
processing by having the network IO happen asynchronously, which I think
would be required to achieve any kind of performances. Plus we already have
something in this spirit with 
<code>effective_io_concurrency</code>.
</p><h3>Mirrored tablespace</h3><p>Now of course we all like to have a choice. Here the choice would be about
data locality. Certain systems already exists (distributed file systems,
including 
<a href='http://www.gluster.org/'>GlusterFS</a> or to some extend 
<a href='http://www.dragonflybsd.org/hammer/'>HAMMER</a>) that propose to mix and match
data replication and data distribution. Or simply think about 
<code>RAID-10</code>. You
have both 
<em>striping</em> and 
<em>mirroring</em> there, so that's somewhat expected to want
to have that in your database product too...
</p><p>That leads me to think we should offer 
<em>mirror tablespaces</em> while at
it. Implementing that can be as complex as optimising 
<em>2 Phase Commit</em> or as
simple (ahah) as having a 
<em>per tablespace Streaming Replication</em> solution.
</p><p>Now of course you have to explain data locality in the network to the
planner, so that it can compare the cost of running a 
<code>JOIN</code> over there then
retrieving the result set against doing that locally. It seems clean what's
best until you think that while the other node is hashing your fact table
you're free to locally execute another part of the same query plan...
</p><h3>It's so cloudy now I can feel the rain coming</h3><p>And as all the rage is about offering 
<em>elastic</em> capabilities to your
clustering solution, the next step after that is having some smart agent in
the system that will notice that we're doing this or that data transfers to
solve user queries so often that we should setup a mirror of this remote
tablespace now.
</p><p>Of course, you also want newcomers to be able to extend your elasticity,
like "hey guys I'm new there, how can I help?".
</p><p>While at it, your smart cluster manager program should be able to arrange
things so that the loss of any node at any time is just an optimisation
problem, not a data loss one. Nothing more critical than a 
<em>serialisation
error</em>, but maybe we should invent a new 
<code>SQLERR</code> here, I'd like to propose the
following error message to the transactions you need to abort in case of
some node unexpected disapearing: 
<em>the elastic just shrinked</em>.
</p><h2>How to Further Optimise PostgreSQL</h2><p>It seems harder and harder to find good ideas that look effective and that
are not too complex to understand. It's simply because about all of them
already have been implemented into the product, if you ask me. But that does
not mean there isn't a way to go, that means finding it is getting tough,
and that when pursuing the ideas used to be some 1-man work for some
<em>consolidated</em> weeks, it's getting to collaborative working of several
talented people sharing a common goal and able to dedicate 
<strong>months</strong> of their
time.
</p><p>I hope it does not mean we should be happy with what we have but we rather
continue improving our prefered product.
</p><h3>On the usage of indexes as column store</h3><p>When you're handling very big tables, there's a very good chance that some
column's value will get repeated all over the place. But if you index that
column, you'll see the value only once and a list of pointers to the rows
that are hosting it. Column storage is about compressing that data set in a
way that you store the column value only once, and also about avoiding to
pass the data around in the 
<em>executor</em>.
</p><p>The idea here would need to first have the ability to use the index in an
authoritative way, without refering the main table storage to confirm the
visibility. That's already a work in progress.
</p><p>When we have that, then we could think about how we're using the indexes
now. Their only purpose in life is to help solve restrictive queries by
accessing directly to the rows of interrest, avoiding to 
<em>scan</em> all those data
that won't fit in main memory, so that you rely on your slow drives to get
access to it.
</p><p>Another idea here would be to be able to use the index to solve queries in
other cases than just applying the restrictions and filtering they
require. By that I mean that we could use our indexes to retrieve some
column's value, rather than the main table's data, when we have the idea
that the columns cardinality is low enough.
</p><p>I guess that would mean we have optional column storage, in the case all the
values you need from the table are in the available index(es). 
</p><p><em><strong>Gavin</strong></em> reports that this idea may not provide the bangs for the bucks, but
warns that it could be he's too much used to thinking on datawarehouse
terms. Also that before engaging into such an effort, it might be good to
have an idea of the benefit you're running after, which is the classic
<a href='http://en.wikipedia.org/wiki/Amdahl%27s_law'>Amdahl's law</a> or the old saying that you should always profile before to
spend time on optimizing.
</p><p>So it's impossible without some preliminary work to assess how useful this
kind of 
<em>index as a column store</em> would be in the general case, but that
wouldn't stop me to share the idea, see...
</p><h3>The Executor as a Virtual Machine</h3><p>I've been talking a lot with 
<em><strong>Gavin</strong></em>, and his mind is full of datawarehouse
optimisation problems. One of them is how to speed up the data retrieval
pipeline from disk. Given what are modern 
<code>CPU</code>, we should be saturating read
capabilities of any hard drive without a sweat, and we're not there yet. It
happens that 
<a href='http://monetdb.cwi.nl/'>MonetDB</a> is about there, with their 
<a href='http://monetdb.cwi.nl/MonetDB/Documentation/MAL-Synopsis.html'>MonetDB Assembly Language</a>,
further described on the 
<a href='http://monetdb.cwi.nl/MonetDB/Documentation/MAL-Reference.html'>MAL reference</a> page.
</p><p>The big difference in architecture between them and us is how they execute
the queries, with code path that contain very little branches, loops, or
function calls. That alone would be responsible for an incredible
performance benefit, like, a factor of 
<code>25</code> to 
<code>50</code> times what we have now, have
I been hinted.
</p><p>If you think about it in the 
<em>right</em> angle, to be able to suppress all the
looping and branching and function calling we currently have into the
executor code, it could be that the simplest solution (ahah) would be to
expose the executor capabilities as 
<em>opcodes</em> or 
<em>assembly</em> and have the planner
and optimiser be 
<em>just in time compilers</em> for this new Virtual Machine.
</p><p>And while at it, we then might benefit from some intermediate representation
of the plan tree, which could start out as what the planner works with in
term of data structure. Out of that, the optimiser job would be to generate
the 
<em>executor code</em>, and I think it should be possible to define an
optimisation effort target at this point, akin to 
<code>-O0</code> to 
<code>-O3</code> option of
<code>gcc</code>. This is another much wanted feature, being able to set the amount of
effort to put into finding the best plan possible, and this 
<em>executor virtual
machine</em> or 
<em>executor assembly</em> idea may not be the most simple way to get
there, but I see it as a conceptually simple fallout.
</p><p>Another thing we might want to look at is how we batch the disk level
reads. The current executor will fetch a row at a time, and fetching
something like 
<code>2 MB</code> at a time (maybe only for 
<em>seqscans</em>) might offer some
great speed benefits, that should even balance out the waste in case you
filter out some of those data.
</p><h3>Automatic use of Materialized VIEWs</h3><p>The initial title of this section was 
<strong>Matching a query against some
"template" at runtime</strong>, which is the hard part of the problem, I think.
</p><p>The goal is simple and easy to understand, when you maintain some
materialized views with triggers, you go a long way to ensure that the
content is trustworthy in a transactional way. That means using that table
or the normalized ones would not change any query result. So the best would
be for the relational engine we love and trust to 
<em>simply</em> (from a user
perspective) be able to use them when having to solve a query that could
benefit.
</p><p>As of course the dynamic query will not always be written the exact same way
the 
<code>VIEW</code> is, you can't just 
<em>hash</em> the query text and compare with some
index. Also, the user queries will not embed the 
<code>VIEW</code> anywhere in their
definition, potentially, so you're not searching for a 
<em>substring match</em>
either. It's more like matching a 
<em>subplan</em> of the query. Then I guess the
planner and optimiser should consider, when evaluating the 
<em>cost</em> of any plan,
if there's a materialized view somewhere that could help running it.
</p><p>In case we have manually refreshed materialized views, I guess we could
still benefit from the same mecanisms, but we would need a way to invalidate
the plans when the data is not fresh enough, so that we decouple the
refreshing interval to the acceptable lag in usage. Some 
<em>metadata</em> and
<em>auto-analyze</em> support would certainly do.
</p><h3>Analyzing VIEWs as a correlated-statistics solution</h3><p>This one is tricky. Oh well you may find all the ideas here as tricky, more
often than not tricky enough that they don't need pursuing. But the other
ones I've been thinking about them for long enough that they just feel
<em>natural</em> for my brain. Aha.
</p><p>So one of the major problem of PostgreSQL currently is related to how its
planner and optimiser 
<em>heavily</em> depend on 
<code>ANALYZE</code> statistics. The problem is
not so much having fresh ones nowadays, thanks to 
<code>autovacuum</code>, but to their
quality. And the first quality problem we have is tied to 
<em>correlated</em> data.
</p><p>What that means is that if you have a couple of colums 
<code>a</code> and 
<code>b</code> set in a way
that each time a is not null, 
<code>b = 2 * a</code>, then PostgreSQL has no
way to realize that. So if you have a query 
<code>WHERE a=1 AND b=2</code>
then it will think having the 
<code>AND</code> in there means you double the output
restrictivity, and it's not the case.
</p><p>There was a 
<a href='http://archives.postgresql.org/pgsql-performance/2009-06/msg00107.php'>thread</a> about exactly this problem on the mailing lists, where
<em><strong>Simon</strong></em> proposed that we implement 
<code>ANALYZE foo [WHERE .... ]</code> in order to
address this problem. I think a generalisation of this proposal is to be
able to 
<code>ANALYSE</code> a 
<code>VIEW</code>, as I 
<a href='http://archives.postgresql.org/pgsql-performance/2009-06/msg00118.php'>said here</a>.
</p><p>So if we ever get to implement a way to match a user query dynamically
against some materialized view existing in the system, I suppose we could
also benefit from this view's statistics in the case it's not materialized.
</p><h3>Planner costs and system usage statistics, or admission control</h3><p><em><strong>Kevin Grittner</strong></em> on the mailing lists 
<a href='http://www.mail-archive.com/pgsql-hackers@postgresql.org/msg142629.html'>proposed</a> that we read section 2.4 of the
<a href='http://db.cs.berkeley.edu/papers/fntdb07-architecture.pdf'>Architecture of a Database System</a> by 
<em>Joseph M. Hellerstein</em>, 
<em>Michael
Stonebraker</em> and 
<em>James Hamilton</em>. The idea revolves around accepting queries
depending on the estimated resource usage they should need to execute, and
the current availability of those on the system.
</p><p>Now, it could be that a way to approach that would be to have some
OS-specific 
<a href='http://www.postgresql.org/docs/8.4/static/monitoring-stats.html'>statistic views</a> in PostgreSQL, we already have a lot of those
now. So we could query PostgreSQL for the current system 
<em>load average</em>, 
<em>io
wait</em>, 
<em>main memory usage</em> and whatnot. The usual answer to that is that this
is in no way the job of a database engine to care about such things, so
please try and find the right tool for the right job, thank you very much.
</p><p>Well, the idea here would be that the planner would now have access to those
information, by calling a function. And we could expose some 
<code>GUC</code> thresholds
not to overcome in the planning stage.
</p><p>That means that the plan itself should store the information of the 
<code>GUC</code> as
they were at the very moment of the planning, so that you possibly get plan
invalidation to kick in and force a re-plan when you 
<code>EXECUTE</code> a 
<code>PREPARE
STATEMENT</code> in the case the current resource usage changed enough.
</p><p>Now the remaining issue is that you're using the system's resources usage
indicators at planning time, to be able to prune some plans, but for complex
queries there can be a meaningfull delay between the planning and the
execution, so you only get illusions about resource availability.
</p><p>Well, unless and until you get the possibility of issuing more than one plan
for any given query and then have the executor pick one. In fact you want to
apply this idea recursively, so that at any given part of the plan the
executor has some way to check between what environment the planner was
running with and what the executor is running with. Having such a feature
would also greatly enhance data locality access patterns, that also would
just be a choice of 
<em>subplan</em> left to executor to make.
</p><p>Oh and should we get distributed query processing on a single node, then
again we would benefit here by having the planner propose plans using the
capability or bypassing it, and the executor check whether it makes sense to
use it now given the estimated costs. At all plan levels.
</p><p>The main drawback against such a line of thought is the major overhaul in
executor design this represent, and in the case of 
<em>only</em> providing
distributed sorting, e.g., then the 
<em>cost / benefit</em> ratio ain't foreseen as
good enough to attack it.
</p><p>In the context of this document though, it's some more reasons to consider
the benefits of architecting the current executor code into a 
<em>Virtual
Machine</em> set of instructions. Then some of the instructions would allow for
checking the current environment and branching in the plan, following the
best compromise as of current execution time. Sure, that's not symplifying
the implementation. But that's a very good reason to have the use case in
mind before making any low-level choice.
</p><h2>And some other thoughts</h2><p>You know the famous joke saying there are 
<a href='http://www.gnu.org/fun/jokes/10-kinds-of-people.html'>10 Kinds of People in the World</a>,
those who understand binary and those who don't. You though that counting to
3 in decimal was so much easier, didn't you? (Hint, I told about organizing
this list of ideas under two big labels, of which this is the third)
</p><h3>Supervized guest deamons, with an API please</h3><p>There are several projects that could benefit from being 
<em>integrated</em>
alongside 
<strong>core</strong> processes. By that I mean being part of the start, reload,
stop and restart procedures. The current list includes 
<em>autovacuum</em>, a 
<em>pgagent</em>
scheduler, and a ticker. And 
<em>helper backgrounds</em> like PostgreSQL had in some
<code>6.x</code> branch, and that 
<em><strong>Markus</strong></em> implemented in 
<a href='http://postgres-r.org'>Postgres-R</a>. There's even an
argument about including a connection pool into the mix, but 
<em><strong>Jan</strong></em> won that
over a beer in Ottawa this year, so I won't insist. You'll notice there's a
lucky winner here, we needed 
<em>autovacuum</em> so bad that it's thankfully already
in 
<em>core</em>.
</p><p>Providing an API that would allow to register user defined 
<em>deamon</em> processes
would allow for including those other projects, and maybe some more, in a
very easy way for the user.
</p><p>My current thinking about that would be to steal as much as meaningfull from
the Erlang/OTP supervisor processes API, including the 
<code>MaxR</code> and 
<code>MaxT</code>
variables to protect the main system to suffer from user deamons
mis-behaviors. 
</p><p>It would even make sense to only provide support for 
<code>gen_fsm</code> kind of hosted
processes, meaning the API is to register a global unique name per process,
a state and code entry points (transitions). Now that we have a mecanism to
send signals to backends, with a payload, it's called 
<code>LISTEN/NOTIFY</code>. We
could certainly reuse that to send messages to the hosted state machines:
that would be the events that trigger te transitions --- the event name
would match the code function.
</p><p>In the case of a pgagent scheduler, we would need to be able to produce
events internally, without user interaction, but my understanding of
<code>SIGALARM</code> is that it's made for that. It's not clear what the best design
would be here, but maybe registering a 
<em>pgagent clock service</em> that would
<code>NOTIFY</code> the 
<em>pgagent launcher service</em> would do. It would also have to 
<code>LISTEN</code>
for changes on its underlying job scheduling tables, I guess.
</p><p>And for querying the database, we'd use 
<code>SPI</code>, I guess.
</p><h3>Logs analysis</h3><p>Nowadays to analyze logs and provide insights, the more common tool to use
is 
<a href='http://pgfouine.projects.postgresql.org/'>pgfouine</a>, which does an excellent job. But there has been some
improvements in logs capabilities that we're not benefiting from yet, and
I'm thinking about the 
<code>CSV</code> log format.
</p><p>So the idea would be to turn 
<em>pgfouine</em> into a set of 
<code>SQL</code> queries against the
logs themselves once imported into the database. Wait. What about having our
next PostgreSQL version, which is meant to include CSV support in 
<em>SQL/MED</em>,
to directly expose its logs as a system view?
</p><p>A good thing would be to expose that as a ddl-partitioned table following
the log rotation scheme as setup in 
<code>postgresql.conf</code>, or maybe given in some
sort of a setup, in order to support 
<code>logrotate</code> users. At least some
facilities to do that would be welcome, and I'm not sure plain 
<em>SQL/MED</em> is
that when it comes to 
<em>source</em> partitioning.
</p><p>Then all that remains to be done is a set of 
<code>SQL</code> queries and some static or
dynamic application to derive reports from there.
</p><h1>Conclusion</h1><p>I hope some of those ideas are viable and interresting to some people, and
should that be the case, seeing progress made on those would be awesome!
Meanwhile, thanks for reading.
</p>  </div>
  </div>
  </div>
  <div id="push"></div>
  </div>

<footer>
  <p>
    <small>Unless otherwise specified, the contents of
    <a href=../index.html>this website</a> are Copyright
    &#169;&#160; 2008-2013 <a href=/about>Dimitri Fontaine</a>, and are
    licensed for use under
    <a href=http://creativecommons.org/licenses/by-sa/3.0/
    rel=license><abbr title="Creative
    Commons">CC</abbr>&#160;<abbr title=Attribution>BY</abbr>-<abbr title=Share-Alike>SA</abbr>&#160;3.0</a>.</small>
  </p> 
</footer> 

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-47059482-1', 'tapoueh.org');
  ga('send', 'pageview');

</script>

</body>
</html>
