#author Dimitri Fontaine
#title tail -f /dev/dim
#desc dim's PostgreSQL blog

* 20090414 Skytools 3.0 reaches alpha1

#20090414
#%20Skytools%203%2E0%20reaches%20alpha1

It's time for [[http://wiki.postgresql.org/wiki/Skytools][Skytools]] news again! First, we did improve documentation of
current stable branch with hosting high level presentations and [[http://wiki.postgresql.org/wiki/Londiste_Tutorial][tutorials]] on
the [[http://wiki.postgresql.org/][PostgreSQL wiki]]. Do check out the [[http://wiki.postgresql.org/wiki/Londiste_Tutorial][Londiste Tutorial]], it seems that's
what people hesitating to try out londiste were missing the most.

The other things people miss out a lot in current stable Skytools (version
=2.1.9= currently) are cascading replication (which allows for *switchover* and
*failover*) and =DDL= support. The new incarnation of skytools, version =3.0=
[[http://lists.pgfoundry.org/pipermail/skytools-users/2009-April/001029.html][reaches alpha1]] today. It comes with full support for *cascading* and *DDL*, so
you might want to give it a try.

It's a rough release, documentation is still to get written for a large part
of it, and bugs are still to get fixed. But it's all in the Skytools spirit:
simple and efficient concepts, easy to use and maintain. Think about this
release as a *developer preview* and join us :)

* 20090210 Prefix GiST index now in 8.1

#20090210
#%20Prefix%20GiST%20index%20now%20in%208%2E1

The [[http://blog.tapoueh.org/prefix.html][prefix]] project is about matching a *literal* against *prefixes* in your
table, the typical example being a telecom routing table. Thanks to the
excellent work around *generic* indexes in PostgreSQL with [[http://www.postgresql.org/docs/current/static/gist-intro.html][GiST]], indexing
prefix matches is easy to support in an external module. Which is what
the [[http://prefix.projects.postgresql.org/][prefix]] extension is all about.

Maybe you didn't come across this project before, so here's the typical
query you want to run to benefit from the special indexing, where the =@>=
operator is read *contains* or *is a prefix of*:

<src lang="sql">
  SELECT * FROM prefixes WHERE prefix @> '0123456789';
</src>

Now, a user asked about an =8.1= version of the module, as it's what some
distributions ship (here, Red Hat Enterprise Linux 5.2). It turned out it
was easy to support =8.1= when you already support =8.2=, so the =CVS= now hosts
[[http://cvs.pgfoundry.org/cgi-bin/cvsweb.cgi/prefix/prefix/][8.1 support code]]. And here's what the user asking about the feature has to
say:

<quote>
It's works like a charm now with 3ms queries over 200,000+ rows.  The speed
also stays less than 4ms when doing complex queries designed for fallback,
priority shuffling, and having multiple carriers.
</quote>

* 20090205 Importing XML content from file

#20090205
#%20Importing%20XML%20content%20from%20file

The problem was raised this week on [[http://www.postgresql.org/community/irc][IRC]] and this time again I felt it would
be a good occasion for a blog entry: how to load an =XML= file content into a
single field?

The usual tool used to import files is [[http://www.postgresql.org/docs/current/interactive/sql-copy.html][COPY]], but it'll want each line of the
file to host a text representation of a database tuple, so it doesn't apply
to the case at hand. [[http://blog.rhodiumtoad.org.uk/][RhodiumToad]] was online and offered the following code
to solve the problem:

<src lang="sql">
create or replace function xml_import(filename text)
  returns xml
  volatile
  language plpgsql as
$f$
    declare
        content bytea;
        loid oid;
        lfd integer;
        lsize integer;
    begin
        loid := lo_import(filename);
        lfd := lo_open(loid,262144);
        lsize := lo_lseek(lfd,0,2);
        perform lo_lseek(lfd,0,0);
        content := loread(lfd,lsize);
        perform lo_close(lfd);
        perform lo_unlink(loid);
 
        return xmlparse(document convert_from(content,'UTF8'));
    end;
$f$;
</src>

As you can see, the trick here is to use the [[http://www.postgresql.org/docs/current/interactive/largeobjects.html][large objects]] API to load the
file content into memory (=content= variable), then to parse it knowing it's
an =UTF8= encoded =XML= file and return an [[http://www.postgresql.org/docs/current/interactive/datatype-xml.html][XML]] datatype object.

* 20090204 Asko Oja talks about Skype architecture

#20090204
#%20Asko%20Oja%20talks%20about%20Skype%20architecture

In this [[http://postgresqlrussia.org/articles/view/131][russian page]] you'll see a nice presentation of Skype databases
architectures by Asko Oja himself. It's the talk at Russian PostgreSQL
Community meeting, October 2008, Moscow, and it's a good read.

		       [[http://postgresqlrussia.org/articles/view/131][../images/Moskva_DB_Tools.v3.png]]

The presentation page is in russian but the slides are in English, so have a
nice read!

* 20090203 Skytools ticker daemon and londiste

#20090203
#20090203%20Skytools%20ticker%20daemon%20and%20londiste

One of the difficulties in getting to understand and configure =londiste=
reside in the relation between the =ticker= and the replication. This question
was raised once more on IRC yesterday, so I made a new FAQ entry about it:
[[http://blog.tapoueh.org/skytools.html#ticker][How do this ticker thing relates to londiste?]]

* 20090131 Comparing Londiste and Slony

#20090131
#%20Skytools%20ticker%20daemon%20and%20londiste

In the page about [[skytools.html][Skytools]] I've encouraged people to ask some more questions
in order for me to be able to try and answer them. That just happened, as
usual on the =#postgresql= IRC, and the question is
[[skytools.html#slony][What does londiste lack that slony has?]]

* 20090128 Controling HOT usage in 8.3

#20090128
#%20Controling%20HOT%20usage%20in%208%2E3

As it happens, I've got some environments where I want to make sure =HOT= (*aka
Heap Only Tuples*) is in use. Because we're doing so much updates a second
that I want to get sure it's not killing my database server. I not only
wrote some checking view to see about it, but also made a [[http://www.postgresql.fr/support:trucs_et_astuces:controler_l_utilisation_de_hot_a_partir_de_la_8.3][quick article]]
about it in the [[http://postgresql.fr/][French PostgreSQL website]]. Handling around in =#postgresql=
means that I'm now bound to write about it in English too!

So =HOT= will get used each time you update a row without changing an indexed
value of it, and the benefit is skipping index maintenance, and as far as I
understand it, easying =vacuum= hard work too. To get the benefit, =HOT= will
need some place where to put new version of the =UPDATEd= tuple in the same
disk page, which means you'll probably want to set your table [[http://www.postgresql.org/docs/8.3/static/sql-createtable.html#SQL-CREATETABLE-STORAGE-PARAMETERS][fillfactor]] to
something much less than =100=.

Now, here's how to check you're benefitting from =HOT=:

<src lang="sql">
SELECT schemaname, relname,
       n_tup_upd,n_tup_hot_upd,
       case when n_tup_upd > 0
            then ((n_tup_hot_upd::numeric/n_tup_upd::numeric)*100.0)::numeric(5,2) 
            else NULL
       end AS hot_ratio
 
 FROM pg_stat_all_tables;
 
 schemaname | relname | n_tup_upd | n_tup_hot_upd | hot_ratio
------------+---------+-----------+---------------+-----------
 public     | table1  |         6 |             6 |    100.00
 public     | table2  |   2551200 |       2549474 |     99.93
</src>

Here's even an extended version of the same request, displaying the
=fillfactor= option value for the tables you're inquiring about. This comes
separated from the first example because you get the =fillfactor= of a
relation into the =pg_class= catalog =reloptions= field, and to filter against a
schema qualified table name, you want to join against =pg_namespace= too.

<src lang="sql">
SELECT t.schemaname, t.relname, c.reloptions, 
       t.n_tup_upd, t.n_tup_hot_upd, 
       case when n_tup_upd > 0 
            then ((n_tup_hot_upd::numeric/n_tup_upd::numeric)*100.0)::numeric(5,2)
            else NULL
        end AS hot_ratio
FROM pg_stat_all_tables t 
      JOIN (pg_class c JOIN pg_namespace n ON c.relnamespace = n.oid) 
        ON n.nspname = t.schemaname AND c.relname = t.relname
 
 schemaname | relname |   reloptions    | n_tup_upd | n_tup_hot_upd | hot_ratio
------------+---------+-----------------+-----------+---------------+-----------
 public     | table1  | {fillfactor=50} |   1585920 |       1585246 |     99.96
 public     | table2  | {fillfactor=50} |   2504880 |       2503154 |     99.93
</src>

Don't let the =HOT= question affect your sleeping no more!

* 20090121 Londiste Trick

#20090121
#%20Londiste%20Trick

So, you're using =londiste= and the =ticker= has not been running all night
long, due to some restart glitch in your procedures, and the *on call* admin
didn't notice the restart failure. If you blindly restart the replication
daemon, it will load in memory all those events produced during the night,
at once, because you now have only one tick where to put them all.

The following query allows you to count how many events that represents,
with the magic tick numbers coming from =pgq.subscription= in columns
=sub_last_tick= and =sub_next_tick=.

<src lang="sql">
SELECT count(*) 
  FROM pgq.event_1, 
      (SELECT tick_snapshot
         FROM pgq.tick
        WHERE tick_id BETWEEN 5715138 AND 5715139
      ) as t(snapshots)
WHERE txid_visible_in_snapshot(ev_txid, snapshots);
</src>

In our case, this was more than *5 millions and 400 thousands* of events. With
this many events to care about, if you start londiste, it'll eat as many
memory as needed to have them all around, which might be more that what your
system is able to give it. So you want a way to tell =londiste= not to load
all events at once. Here's how: add the following knob to your *.ini*
configuration file before to restart the londiste daemon:

<src>
    pgq_lazy_fetch = 500
</src>

Now, =londiste= will lazyly fetch =500= events at once or less, even if a single
=batch= (which contains all *events* between two *ticks*) contains a huge number
of events. This number seems a good choice as it's the default =PGQ= setting
of number of events in a single *batch*. This number is only outgrown when the
ticker is not running or when you're producing more *events* than that in a
single transaction.

Hope you'll find the tip useful!


* 20081204 Fake entry

#20081204
#20081204%20Fake%20entry

This is a test of a fake entry to see how muse will manage this.

With some =SQL= inside:

<quote>
<src lang="sql">
SELECT * FROM planet.postgresql.org WHERE author = "dim";
</src>
</quote>
