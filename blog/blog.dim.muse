#author Dimitri Fontaine
#title tail -f /dev/dim
#desc dim's PostgreSQL blog

* 20110629-09:50 Multi-Version support for Extensions

We still have this problem to solve with extensions and their packaging.
How to best organize things so that your extension is compatible with before
=9.1= and =9.1= and following releases of [[http://www.postgresql.org/][PostgreSQL]]?

Well, I had to do it for the [[http://pgfoundry.org/projects/ip4r/][ip4r]] contribution, and I wanted the following
to happen:

<src>
dpkg-deb: building package `postgresql-8.3-ip4r' ...
dpkg-deb: building package `postgresql-8.4-ip4r' ...
dpkg-deb: building package `postgresql-9.0-ip4r' ...
dpkg-deb: building package `postgresql-9.1-ip4r' ...
</src>

And here's a simple enough way to achieve that.  First, you have to get your
packaging ready the usual way, and to install the build dependencies.  Then
realizing that =/usr/share/postgresql-common/supported-versions= from the
latest =postgresql-common= package will only return =8.3= in =lenny= (yes, I'm
doing some *backporting* here), we have to tweak it.

<src>
$ dpkg -l postgresql-server-dev-* | awk '/^ii/ {print $2}'
postgresql-server-dev-8.3
postgresql-server-dev-8.4
postgresql-server-dev-9.0
postgresql-server-dev-9.1
postgresql-server-dev-all

$ sudo dpkg-divert \
--divert /usr/share/postgresql-common/supported-versions.distrib \
--rename /usr/share/postgresql-common/supported-versions

$ cat /usr/share/postgresql-common/supported-versions
#! /bin/bash

dpkg -l postgresql-server-dev-* \
| awk -F '[ -]' '/^ii/ && ! /server-dev-all/ {print $6}'
</src>

Now we are allowed to build our extension for all those versions, so we add
=9.1= to the =debian/pgversions= file.  And =debuild= will do the right thing now,
thanks to [[http://manpages.debian.net/cgi-bin/man.cgi?query=pg_buildext][pg_buildext]] from [[http://packages.debian.org/sid/postgresql-server-dev-all][postgresql-server-dev-all]].

The problem we face is that the built is not an [[http://www.postgresql.org/docs/9.1/static/extend-extensions.html][extension]] as in =9.1=, so
things like =\dx= in =psql= and [[http://www.postgresql.org/docs/9.1/static/sql-createextension.html][CREATE EXTENSION]] will not work out of the box.
First, we need a control file.  Then we need to remove the transaction
control from the install script (here, =ip4r.sql=), and finally, this script
needs to be called =ip4r--1.05.sql=.  Here's how I did it:

<src>
$ cat ip4r.control
comment = 'IPv4 and IPv4 range index types'
default_version = '1.05'
relocatable = yes

$ cat debian/postgresql-9.1-ip4r.install
debian/ip4r-9.1/ip4r.so usr/lib/postgresql/9.1/lib
ip4r.control usr/share/postgresql/9.1/extension
debian/ip4r-9.1/ip4r.sql usr/share/postgresql/9.1/extension

$ cat debian/postgresql-9.1-ip4r.links
usr/share/postgresql/9.1/extension/ip4r.sql usr/share/postgresql/9.1/extension/ip4r--1.05.sql
</src>

Be careful not to forget to remove any and all =BEGIN;= and =COMMIT;= lines from
the =ip4r.sql= file, which meant that I also removed support for *Rtree*, which
is not relevant for modern versions of PostgreSQL saith the script (post
=8.2=).  That means I'm not publishing this very work yet, but I wanted to
share the =debian/postgresql-9.1-extension.links= idea.

Notice that I didn't change anything about the =.sql.in= make rule, so I
didn't have to use the support for =module_pathname= in the control file.

Now, after the usual =debuild= step, I can just =sudo debi= to install all the
just build packages and =CREATE EXTENSION= will run fine.  And in =9.0= you get
the old way to install it, but it still works:

<src>
$ psql -U postgres --cluster 9.0/main -1 \
-f /usr/share/postgresql/9.0/contrib/ip4r.sql
<lots of chatter>

$ psql -U postgres --cluster 9.1/main -c 'create extension ip4r;'
CREATE EXTENSION
</src>

That's it :)

* 20110530-11:00 Back from Ottawa, preparing for Cambridge

While [[http://blog.hagander.net/][Magnus]] is all about [[http://2011.pgconf.eu/][PG Conf EU]] already, you have to realize we're just
landed back from [[http://www.pgcon.org/2011/][PG Con]] in Ottawa.  My next stop in the annual conferences
is [[http://char11.org/][CHAR 11]], the *Clustering, High Availability and Replication* conference in
Cambridge, 11-12 July.  Yes, on the old continent this time.

This year's *pgcon* hot topics, for me, have been centered around a better
grasp at [[http://www.postgresql.org/docs/9.1/static/transaction-iso.html#XACT-SERIALIZABLE][SSI]] and *DDL Triggers*.  Having those beasts in [[http://www.postgresql.org/][PostgreSQL]] would
allow for auditing, finer privileges management and some more automated
replication facilities.  Imagine that =ALTER TABLE= is able to fire a *trigger*,
provided by *Londiste* or *Slony*, that will do what's needed on the cluster by
itself.  That would be awesome, wouldn't it?

At *CHAR 11* I'll be talking about [[http://wiki.postgresql.org/wiki/SkyTools][Skytools 3]].  You know I've been working on
its *debian* packaging, now is the time to review the documentation and make
there something as good looking as the monitoring system are...

Well, expect some news and a nice big picture diagram overview soon, if work
schedule leaves me anytime that's what I want to be working on now.

* 20110512-10:30 Preparing for PGCON

It's this time of the year again, the main international
[[http://www.pgcon.org/2011/][PostgreSQL Conference]] is next week in Ottawa, Canada.  If previous years are
any indication, this will be great event where to meet with a lot of the
members of your community.  The core team will be there, developers will be
there, and we will meet with users and their challenging use cases.

This is a very good time to review both what you did in the project those
last 12 months, and what you plan to do next year.  To help with that,
several *meeting* events are organized.  They're like a whole-day round table
with a kind of an agenda, with a limited number of invited people in, and
very intense on-topic discussions about how to organize ourselves for
another great year of innovation in PostgreSQL.

Then we have two days full of talks where I usually learn some new aspect of
the project or of the product, and where ideas tend to just pop-up in a
continuous race.  Being away from home and with people you see only once a
year (some of them more than that of course, hi European fellows!) seems to
allow for some broader thinking.

The talks I want to go to include
[[http://www.pgcon.org/2011/schedule/events/361.en.html][Database Scalability Patterns: Sharding for Unlimited Growth]] by
[[http://www.pgcon.org/2011/schedule/speakers/20.en.html][Robert Treat]], [[http://www.pgcon.org/2011/schedule/events/366.en.html][Maintaining Terabytes]] by [[http://www.pgcon.org/2011/schedule/speakers/112.en.html][Selena Deckelmann]], [[http://www.pgcon.org/2011/schedule/events/307.en.html][NTT’s Case Report]]
by [[http://www.pgcon.org/2011/schedule/speakers/192.en.html][Tetsuo Sakata]], [[http://www.pgcon.org/2011/schedule/events/350.en.html][Hacking the Query Planner]] by [[http://www.pgcon.org/2011/schedule/speakers/202.en.html][Tom Lane]].  That's for a first
day, right?

Then, on the second day, I notice [[http://www.pgcon.org/2011/schedule/events/311.en.html][Range Types]] by [[http://www.pgcon.org/2011/schedule/speakers/83.en.html][Jeff Davis]],
[[http://www.pgcon.org/2011/schedule/events/309.en.html][SP-GiST - a new indexing infrastructure for PostgreSQL]] by [[http://www.pgcon.org/2011/schedule/speakers/29.en.html][Oleg]] and [[http://www.pgcon.org/2011/schedule/speakers/33.en.html][Teodor]],
[[http://www.pgcon.org/2011/schedule/events/337.en.html][The Write Stuff]] by [[http://www.pgcon.org/2011/schedule/speakers/110.en.html][Greg Smith]] (a colleague at [[http://www.2ndquadrant.fr/][2ndQuadrant]]).

I will miss [[http://www.pgcon.org/2011/schedule/events/333.en.html][Serializable Snapshot Isolation in Postgres]] by [[http://www.pgcon.org/2011/schedule/speakers/113.en.html][Kevin Grittner]]
and [[http://www.pgcon.org/2011/schedule/speakers/197.en.html][Dan Ports]], unfortunately, because I'll be talking about
[[http://www.pgcon.org/2011/schedule/events/280.en.html][Extensions Development]] at the same time.

Well of course this list is just a first selection, hallway tracks are often
what guides me through talks or make me skip some.

See you there!

* 20110504-11:45 Tables and Views dependencies

Let's say you need to =ALTER TABLE foo ALTER COLUMN bar TYPE bigint;=, and
[[http://postgresql.org][PostgreSQL]] is helpfully telling you that no you can't because such and such
*views* depend on the column.  The basic way to deal with that is to copy
paste from the error message the names of the views involved, then prepare a
script wherein you first =DROP VIEW ...;= then =ALTER TABLE= and finally =CREATE
VIEW= again, all in the same transaction.

So you have to copy paste also the view definitions.  With large view
definitions, it quickly gets cumbersome to do so.  Well when you're working
on operations, you have to bear in mind that cumbersome is a synonym for
*error prone*, in fact — so you want another solution if possible.

Oh, and the other drawback of this solution is that =ALTER TABLE= will first
take a =LOCK= on the table, locking out any activity.  And more than that, the
lock acquisition will queue behind current activity on the table, which
means waiting for a fairly long time and damaging the service quality on a
moderately loaded server.

It's possible to abuse the [[http://www.postgresql.org/docs/current/static/catalogs.html][system catalogs]] in order to find all *views* that
depend on a given table, too.  For that, you have to play with =pg_depend= and
you have to know that internally, a *view* is in fact a *rewrite rule*.  Then
here's how to produce the two scripts that we need:

<src lang="sql">
=# \t
Showing only tuples.

=# \o /tmp/drop.sql
=# select 'DROP VIEW ' || views || ';'
     from (select distinct(r.ev_class::regclass) as views
            from pg_depend d join pg_rewrite r on r.oid = d.objid 
           where refclassid = 'pg_class'::regclass
             and refobjid = 'SCHEMA.TABLENAME'::regclass
             and classid = 'pg_rewrite'::regclass 
             and pg_get_viewdef(r.ev_class, true) ~ 'COLUMN_NAME') as x;

=# \o /tmp/create.sql
=# select 'CREATE VIEW ' || views || E' AS \n'
       || pg_get_viewdef(views, true) || ';' 
     from (select distinct(r.ev_class::regclass) as views 
          from pg_depend d join pg_rewrite r on r.oid = d.objid
         where refclassid = 'pg_class'::regclass
           and refobjid = 'SCHEMA.TABLENAME'::regclass
           and classid = 'pg_rewrite'::regclass
           and pg_get_viewdef(r.ev_class, true) ~ 'COLUMN_NAME') as x;

=# \o
</src>

Replace =SCHEMA.TABLENAME= and =COLUMN_NAME= with your targets here and the
first query should give you one row per candidate view.  Well if you're not
using the =\o= trick, that is — if you do, check out the generated file
instead, with =\! cat /tmp/drop.sql= for example.

Please note that this catalog query is not accurate, as it will select as a
candidate any view that will by chance both depend on the target table and
contain the =column_name= in its text definition.  So either filter out the
candidates properly (by proper proof reading then another =WHERE= clause), or
just accept that you might =DROP= then =CREATE= again more *views* than need be.

If you need some more details about the =\t \o= sequence you might be
interested in this older article about [[http://tapoueh.org/articles/blog/_Resetting_sequences._All_of_them,_please!.html][resetting sequences]].

* 20110502-17:30 Extension =module_pathname= and =.sql.in=

While currently too busy at work to deliver much Open Source contributions,
let's debunk an old habit of [[http://www.postgresql.org/][PostgreSQL]] extension authors.  It's all down to
copy pasting from *contrib*, and there's no reason to continue doing =$libdir=
this way ever since =7.4= days.

Let's take an example here, with the [[https://github.com/dimitri/prefix][prefix]] extension.  This one too will
need some love, but is still behind on my spare time todo list, sorry about
that.  So, in the =prefix.sql.in= we read

<src lang="sql">
  CREATE OR REPLACE FUNCTION prefix_range_in(cstring)
  RETURNS prefix_range
  AS 'MODULE_PATHNAME'
  LANGUAGE 'C' IMMUTABLE STRICT;
</src>

Two things are to change here.  First, the PostgreSQL *backend* will
understand just fine if you just say =AS '$libdir/prefix'=.  So you have to
know in the =sql= script the name of the shared object library, but if you do,
you can maintain directly a =prefix.sql= script instead.

The advantage is that you now can avoid a compatibility problem when you
want to support PostgreSQL from =8.2= to =9.1= in your extension (older than
that and it's [[http://wiki.postgresql.org/wiki/PostgreSQL_Release_Support_Policy][no longer supported]]).  You directly ship your script.

For compatibility, you could also use the [[http://developer.postgresql.org/pgdocs/postgres/extend-extensions.html][control file]] =module_pathname=
property.  But for =9.1= you then have to add a implicit =Make= rule so that the
script is derived from your =.sql.in=. And as you are managing several scripts
— so that you can handle *versioning* and *upgrades* — it can get hairy (*hint*,
you need to copy =prefix.sql= as =prefix--1.1.1.sql=, then change its name at
next revision, and think about *upgrade* scripts too).  The =module_pathname=
facility is better to keep for when managing more than a single extension in
the same directory, like the [[http://git.postgresql.org/gitweb?p=postgresql.git;a=blob;f=contrib/spi/Makefile;h=0c11bfcbbd47b0c3ed002874bfefd9e2022cf5ac;hb=HEAD][SPI contrib]] is doing.

Sure, maintaining an extension that targets both antique releases of
PostgreSQL and [[http://developer.postgresql.org/pgdocs/postgres/sql-createextension.html][CREATE EXTENSION]] super-powered one(s) (not yet released) is a
little more involved than that.  We'll get back to that, as some people are
still pioneering the movement.

On my side, I'm working with some [[http://www.debian.org/][debian]] [[http://qa.debian.org/developer.php?login=myon][developer]] on how to best manage the
packaging of those extensions, and this work could end up as a specialized
*policy* document and a coordinated *team* of maintainers for all things
PostgreSQL in =debian=.  This will also give some more steam to the PostgreSQL
effort for debian packages: the idea is to maintain packages for all
supported version (from =8.2= up to soon =9.1=), something =debian= itself can not
commit to.

* 20110423-10:30 Emacs and PostgreSQL, PL line numbering

A while ago I've been fixing and publishing [[https://github.com/dimitri/pgsql-linum-format][pgsql-linum-format]] separately.
That allows to number =PL/whatever= code lines when editing from [[http://www.gnu.org/software/emacs/][Emacs]], and
it's something very useful to turn on when debugging.

	    [[../images/emacs-pgsql-linum.png]]


The carrets on the *fringe* in the emacs window are the result of
<code>(setq-default indicate-buffer-boundaries 'left)</code> and here it's
just overloading the image somehow.  But the idea is to just =M-x linum-mode=
when you need it, at least that's my usage of it.

You can use [[https://github.com/dimitri/el-get][el-get]] to easily get (then update) this little =Emacs= extension.


* 20110411-11:30 Some notes about Skytools3

I've been working on [[http://github.com/markokr/skytools][skytools3]] packaging lately.  I've been pushing quite a
lot of work into it, in order to have exactly what I needed out of the box,
after some 3 years of production and experiences with the products.  Plural,
yes, because even if [[http://wiki.postgresql.org/wiki/PgBouncer][pgbouncer]] and [[http://wiki.postgresql.org/wiki/PL/Proxy][plproxy]] are siblings to the projets (same
developers team, separate life cycle and releases), then =skytools= still
includes several sub-projects.

Here's what the =skytools3= packaging is going to look like:

<src>
skytools3              Skytool's replication and queuing
python-pgq3            Skytool's PGQ python library
python-skytools3       python scripts framework for skytools
skytools-ticker3       PGQ ticker daemon service
skytools-walmgr3       high-availability archive and restore commands
postgresql-8.4-pgq3    PGQ server-side code (C module for PostgreSQL)
postgresql-9.0-pgq3    PGQ server-side code (C module for PostgreSQL)
</src>

This split is needed so that you can install your *daemons* (we call them
*consumers*) on separate machines than where you run [[http://postgresql.org][PostgreSQL]].  But for the
=walmgr= part, it makes no sense to install it if you don't have a local
PostgreSQL service, as it's providing =archive= and =restore= commands.  Then
the *ticker*, you're free to run it on any machine really, so just package it
this way (in =skytools3= the *ticker* is written in =C= and does not depend on the
python framework any more).

What you can't see here yet is the new goodies that wraps it as a quality
=debian= package.  A new =skytools= user is created for you when you install the
=skytools3= package (which contains the services), along with a skeleton file
=/etc/skytools.ini= and a user directory =/etc/skytools/=.  Put in there your
services configuration file, and register those service in the
=/etc/skytools.ini= file itself.  Then they will get cared about in the =init=
sequence at startup and shutdown of your server.

The services will run under the =skytools= system user, and will default to
put their log into =/var/log/skytools/=.  The =pidfile= will get into
=/var/run/skytools/=.  All integrated, automated.

Next big *TODO* is about documentation, reviewing it and polishing it, and I
think that =skytools3= will then get ready for public release.  Yes, you read
it right, it's happening this very year!  I'm very excited about it, and
have several architectures that will greatly benefit from the switch to
=skytools3=.  More on that later, though!  (Yes, my *to blog later* list is
getting quite long now).


* 20110329-15:30 towards pg_staging 1.0

If you don't remember about what [[pgstaging.html][pg_staging]] is all about, it's a central
console from where to control all your [[http://www.postgresql.org/][PostgreSQL]] databases.  Typically you
use it to manage your development and pre-production setup, where developers
ask you pretty often to install them some newer dump from the production,
and you want that operation streamlined and easy.

	    [[../images/pg_staging.png]]


** Usage 

The typical session would be something like this:

<src>
pg_staging> databases foodb.dev
                    foodb      foodb_20100824 :5432
           foodb_20100209      foodb_20100209 :5432
           foodb_20100824      foodb_20100824 :5432
                pgbouncer           pgbouncer :6432
                 postgres            postgres :5432

pg_staging> dbsizes foodb.dev
foodb.dev
           foodb_20100209: -1
           foodb_20100824: 104 GB
Total                    = 104 GB

pg_staging> restore foodb.dev
...
pg_staging> switch foodb.dev today
</src>

The list of supported commands is quite long now, and documented too (it
comes with two man pages).  The =restore= one is the most important and will
create the database, add it to the =pgbouncer= setup, fetch the backup named
=dbname.`date -I`.dump=, prepare a filtered object list (more on that), load
*pre* =SQL= scripts, launch =pg_restore=, =VACUUM ANALYZE= the database when
configured to do so, load the *post* =SQL= scripts then optionaly *switch* the
=pgbouncer= setup to default to this new database.

** Filtering

The newer option is called =tablename_nodata_regexp=, and here's its documentation in full:

<quote>
     List of table names regexp (comma separated) to restore without
     content. The =pg_restore= catalog =TABLE DATA= sections will get
     filtered out.  The regexp is applied against =schemaname.tablename=
     and non-anchored by default.
</quote>

This comes to supplement the =schemas= and =schemas_nodata= options, that allows
to only restore objects from a given set of *schemas* (filtering out triggers
that will calls function that are in the excluded schemas, like
e.g. [[http://wiki.postgresql.org/wiki/Skytools][Londiste]] ones) or to restore only the =TABLE= definitions while skipping
the =TABLE DATA= entries.

** Setup

To setup your environment for *pg_staging*, you need to take some steps.  It's
not complex but it's fairly involved.  The benefit is this amazingly useful
central unique console to control as many databases as you need.

You need a =pg_staging.ini= file where to describe your environment.  I
typically name the sessions in there by the name of the database to restore
followed by a =dev= or =preprod= extension.

You need to have all your backups available through =HTTP=, and as of now,
served by the famous *apache* =mod_dir= directory listing.  It's easy to add
support to other methods, but is has not been done yet.  You also need to
have a cluster wide =--globals-only= backup available somewhere so that you
can easily create the users etc you need from =pg_staging=.

You also need to run a =pgbouncer= daemon on each database server, allowing
you to bypass editing connection strings when you =switch= a new database
version live.

You also need to install the *client* script, have a local =pgstaging= system
user and allow it to run the client script as root, so that it's able to
control some services and edit =pgbouncer.ini= for you.

** Status

I'm still using it a lot (several times a week) to manage a whole
development and pre-production environment set, so the very low
[[https://github.com/dimitri/pg_staging][code activity]] of the project is telling that it's pretty stable (last series
of *commits* are all bug fixes and round corners).

Given that, I'm thinking in terms of =pg_staging 1.0= soon!  Now is a pretty
good time to try it and see how it can help you.

* 20110301-16:30 Extensions in 9.1

If you've not been following closely you might have missed out on extensions
integration.  Well, [[http://en.wikipedia.org/wiki/Tom_Lane_(computer_scientist)][Tom]] spent some time on the patches I've been preparing
for the last 4 months.  And not only did he commit most of the work but he
also enhanced some parts of the code (better factoring) and basically
finished it.

At the [[http://wiki.postgresql.org/wiki/PgCon_2010_Developer_Meeting][previous developer meeting]] his advice was to avoid putting too much
into the very first version of the patch for it to stand its chances of
being integrated, and while in the review process more than one major
[[http://www.postgresql.org/][PostgreSQL]] contributor expressed worries about the size of the patch and the
number of features proposed.  Which is the usual process.

Then what happened is that ***Tom*** finally took a similar reasoning as mine
while working on the feature.  To maximize the benefits, once you have the
infrastructure in place, it's not that much more work to provide the really
interesting features.  What's complex is agreeing on what exactly are their
specifications.  And in the *little* time window we got on this commit fest
(well, we hijacked about 2 full weeks there), we managed to get there.

So in the end the result is quite amazing, and you can see that on the
documentation chapter about it:
[[http://developer.postgresql.org/pgdocs/postgres/extend-extensions.html][35.15. Packaging Related Objects into an Extension]].

All the *contrib* modules that are installing =SQL= objects into databases for
you to use them are now converted to ***Extensions*** too, and will get released
in =9.1= with an upgrade script that allows you to *upgrade from unpackaged*.
That means that once you've upgraded from a past PostgreSQL release up to
=9.1=, it will be a command away for you to register *extensions* as such.  I
expect third party *extension* authors (from [[http://pgfoundry.org/projects/ip4r/][ip4r]] to [[http://pgfoundry.org/projects/temporal][temporal]]) to release a
*upgrade-from-unpackaged* version of their work too.

Of course, a big use case of the *extensions* is also in-house =PL= code, and
having version number and multi-stage upgrade scripts there will be
fantastic too, I can't wait to work with such a tool set myself.  Some later
blog post will detail the benefits and usage.  I'm already trying to think
how much of this version and upgrade facility could be expanded to classic
=DDL= objects…

So expect some more blog posts from me on this subject, I will have to talk
about *debian packaging* an extension (it's getting damn easy with
[[http://packages.debian.org/squeeze/postgresql-server-dev-all][postgresql-server-dev-all]] — yes it has received some planing ahead), and
about how to package your own extension, manage upgrades, turn your current
=pre-9.1= extension into a *full blown extension*, and maybe how to stop
worrying about extension when you're a DBA.

If you have some features you would want to discuss for next releases,
please do contact me!

Meanwhile, I'm very happy that this project of mine finally made it to *core*,
it's been long in the making.  Some years to talk about it and then finally
4 months of coding that I'll remember as a marathon.  Many Thanks go to all
who helped here, from [[http://www.2ndquadrant.com/][2ndQuadrant]] to early reviewers to people I talked to
over beers at conferences… lots of people really.

To an extended PostgreSQL (and beyond) :)

* 20110207-11:10 Back from FOSDEM

This year we were in the main building of the conference, and apparently the
booth went very well, solding lots of [[http://postgresqleu.spreadshirt.net/][PostgreSQL merchandise]] etc.  I had the
pleasure to once again meet with the community, but being there only 1 day I
didn't spend as much time as I would have liked with some of the people there.

In case you're wondering, my [[http://fosdem.org/2011/schedule/event/pg_extension1][extension's talk]] went quite well, and several
people were kind enough to tell me they appreciated it!  There was video
recording of it, so we will soon have proofs showing how bad it really was
and how *polite* those people really are :)

I will soon be able to write an article series detailing what's an Extension
and how you deal with them, either as a user or an author.  Well in fact the
goal is for any user to easily become an extension author, as I think lots
of people are already maintaining server side code but missing tools to
manage it properly.  But that will begin once the patch is in, so that I
present *the real stuff* rather than what I proposed to the community… Stay
tuned!


* 20110201-13:35 Going to FOSDEM

A quick blog entry to say that yes:

	    [[../images/going-to-fosdem-2011.png]]


And I will even do my [[http://fosdem.org/2011/schedule/event/pg_extension1][Extension's talk]] which had a [[http://blog.hagander.net/archives/183-Feedback-from-PGDay.EU-the-speakers.html][success at pgday.eu]].  The
talk will be updated to include the last developments of the extension's
feature, as some of it changed already in between, and to detail the plan
for the =ALTER EXTENSION ... UPGRADE= feature that I'd like to see included as
soon as =9.1=, but time is running so fast.

In fact the design for the =UPGRADE= has been done and reviewed already, but
there's yet to reach consensus on how to setup which is the upgrade file to
use when upgrading from a given version to another.  I've solved it in my
patch, of course, by adding properties into the extension's *control
file*. That's the best place to have that setup I think, it allows lots of
flexibility, leave the extension's author in charge, and avoids any hard
coding of any kind of assumptions about file naming or whatever.

Next days and reviews will tell us more about how the design is received.
Meanwhile, we're working on finalizing the main extension's patch, offering
=pg_dump= support.

See you at [[http://fosdem.org/2011/][FOSDEM]]!

* 20101124-16:45 Dynamic Triggers in PLpgSQL

You certainly know that implementing *dynamic* triggers in =PLpgSQL= is
impossible. But I had a very bad night, being up from as soon as 3:30 am
today, so that when a developer asked me about reusing the same trigger
function code from more than one table and for a dynamic column name, I
didn't remember about it being impossible.

Here's what happens in such cases, after a long time on the problem (yes,
overall, that's a slow day). Note that I'm abusing the =(record_literal).*=
notation a lot in there, and even the =(record_literal).column_name= too.

<src lang="sql">
CREATE OR REPLACE FUNCTION public.update_timestamp()
 RETURNS TRIGGER
 LANGUAGE plpgsql
AS $f$
DECLARE
    ts_column varchar;
    old_timestamp timestamptz;
    attname name;
    n text;
    v text;
BEGIN
    IF TG_NARGS != 1
    THEN
        RAISE EXCEPTION 'Trigger public.update_timestamp() called with % args',
                         TG_NARGS;
    END IF;

    ts_column := TG_ARGV[0];

    EXECUTE 'SELECT n.' || ts_column
         || ' FROM (SELECT (' 
         || quote_literal(OLD) || '::' || TG_RELID::regclass
         || ').*) as n'
       INTO old_timestamp;

    -- build NEW record text
    n := '(';
    FOR attname IN
      EXECUTE 'SELECT attname '
           || '  FROM pg_class c left join pg_attribute a on a.attrelid = c.oid'
           || ' WHERE c.oid = $1 and attnum > 0 order by attnum'
       USING TG_RELID
    LOOP
        EXECUTE 'SELECT (' || quote_literal(NEW) || '::' || TG_RELID::regclass || ').' || attname INTO v;

        IF n != '(' THEN n := n || ','; END IF;

        IF attname = ts_column 
           AND v::timestamptz IS NOT DISTINCT FROM old_timestamp
        THEN
                n := n || now();
        ELSE
                n := n || COALESCE(v, '');              
        END IF;
    END LOOP;
    n := n || ')';

    EXECUTE 'SELECT ($1::' || TG_RELID::regclass || ').*'
      INTO NEW
     USING n;

    RETURN NEW;
END;
$f$;
</src>

It's not pretty, and not fast. It's about =2 ms= per call on a table with =15=
columns, in some preliminary tests. But it sure was a nice challenge!

* 20101107-13:45 pg_basebackup

[[http://2ndquadrant.com/about/#krosing][Hannu]] just gave me a good idea in [[http://archives.postgresql.org/pgsql-hackers/2010-11/msg00236.php][this email]] on [[http://archives.postgresql.org/pgsql-hackers/][-hackers]], proposing that
[[https://github.com/dimitri/pg_basebackup][pg_basebackup]] should get the =xlog= files again and again in a loop for the
whole duration of the *base backup*. That's now done in the aforementioned
tool, whose options got a little more useful now:

<src>
Usage: pg_basebackup.py [-v] [-f] [-j jobs] "dsn" dest

Options:
  -h, --help            show this help message and exit
  --version             show version and quit
  -x, --pg_xlog         backup the pg_xlog files
  -v, --verbose         be verbose and about processing progress
  -d, --debug           show debug information, including SQL queries
  -f, --force           remove destination directory if it exists
  -j JOBS, --jobs=JOBS  how many helper jobs to launch
  -D DELAY, --delay=DELAY
                        pg_xlog subprocess loop delay, see -x
  -S, --slave           auxilliary process
  --stdin               get list of files to backup from stdin
</src>

Yeah, as implementing the =xlog= idea required having some kind of
parallelism, I built on it and the script now has a =--jobs= option for you to
setup how many processes to launch in parallel, all fetching some =base
backup= files in its own standard (=libpq=) [[http://www.postgresql.org/][PostgreSQL]] connection, in
compressed chunks of =8 MB= (so that's not =8 MB= chunks sent over).

The =xlog= loop will fetch any =WAL= file whose =ctime= changed again,
wholesale. It's easier this way, and tools to get optimized behavior already
do exist, either [[http://skytools.projects.postgresql.org/doc/walmgr.html][walmgr]] or [[http://www.postgresql.org/docs/9.0/interactive/warm-standby.html#STREAMING-REPLICATION][walreceiver]].

The script is still a little [[http://python.org/][python]] self-contained short file, it just went
from about =100= lines of code to about =400= lines. There's no external
dependency, all it needs is provided by a standard python installation. The
problem with that is that it's using =select.poll()= that I think is not
available on windows. Supporting every system or adding to the dependencies,
I've been choosing what's easier for me.

<src lang="python">
    import select
    p = select.poll()
    p.register(sys.stdin, select.POLLIN)
</src>

If you get to try it, please report about it, you should know or easily
discover my *email*!

* 20101021-13:45 Introducing Extensions

After reading [[http://database-explorer.blogspot.com/2010/10/extensions-in-91.html][Simon's blog post]], I can't help but try to give some details
about what it is exactly that I'm working on. As he said, there are several
aspects to *extensions* in [[http://www.postgresql.org/][PostgreSQL]], it all begins here:
[[http://www.postgresql.org/docs/9.0/interactive/extend.html][Chapter 35. Extending SQL]].

It's possible, and mostly simple enough, to add your own code or behavior to
PostgreSQL, so that it will use your code and your semantics while solving
user queries. That's highly useful and it's easy to understand how so when
you look at some projects like [[http://postgis.refractions.net/][PostGIS]], [[http://pgfoundry.org/projects/ip4r/][ip4r]] (index searches of =ip= in a
=range=, not limited to =CIDR= notation), or our own *Key Value Store*, [[http://www.postgresql.org/docs/9.0/interactive/hstore.html][hstore]].

** So, what's in an *Extension*?

An *extension* in its simple form is a =SQL= *script* that you load on your
database, but manage separately. Meaning you don't want the script to be
part of your backups. Often, that kind of script will create new datatypes
and operators, support functions, user functions and index support, and then
it would include some =C= code that ships in a *shared library object*.

As far as PostgreSQL is concerned, at least in the current version of my
patch, the extension is first a *meta* information file that allows to
register it. We currently call that the =control= file. Then, it's an =SQL=
script that is *executed* by the server when you =create= the *extension*.

If it so happens that the =SQL= script depends on some *shared library objects*
file, this has to be present at the right place (=MODULE_PATHNAME=) for the
*extension* to be successfully created, but that's always been the case.

The problem with current releases of PostgreSQL, that the *extension* patch is
solving, is the =pg_dump= and =pg_restore= support. We said it, you don't want
the =SQL= script to be part of your dump, because it's not maintained in your
database, but in some code repository out there. What you want is to be able
to install the *extension* again at the file system level then =pg_restore= your
database — that depends on it being there.

And that's exactly what the *extension* patch provides. By now having a =SQL=
object called an =extension=, and maintained in the new =pg_extension= catalog,
we have an =Oid= to refer to. Which we do by recording a dependency between
any object created by the script and the *extension* =Oid=, so that =pg_dump= can
be instructed to skip those.

** Examples?

So, let's have a look at what you can do if you play with a patched
development server version, or if you play directly from the =git= repository
at
[[http://git.postgresql.org/gitweb?p=postgresql-extension.git;a=shortlog;h=refs/heads/extension]]

<src>
dim ~ createdb exts
dim ~ psql exts
psql (9.1devel)
Type "help" for help.

dim=# \dx+
                                                        List of extensions
        Name        | | |                               Description                               
--------------------+-+-+-------------------------------------------------------------------------
 adminpack          | | | Administrative functions for PostgreSQL
 auto_username      | | | functions for tracking who changed a table
 autoinc            | | | functions for autoincrementing fields
 btree_gin          | | | GIN support for common types BTree operators
 btree_gist         | | | GiST support for common types BTree operators
 chkpass            | | | Store crypt()ed passwords
 citext             | | | case-insensitive character string type
 cube               | | | data type for representing multidimensional cubes
 dblink             | | | connect to other PostgreSQL databases from within a database
 dict_int           | | | example of an add-on dictionary template for full-text search
 dict_xsyn          | | | example of an add-on dictionary template for full-text search
 earthdistance      | | | calculating great circle distances on the surface of the Earth
 fuzzystrmatch      | | | determine similarities and distance between strings
 hstore             | | | storing sets of key/value pairs
 int_aggregate      | | | integer aggregator and an enumerator (obsolete)
 intarray           | | | one-dimensional arrays of integers: functions, operators, index support
 isn                | | | data types for the international product numbering standards
 lo                 | | | managing Large Objects
 ltree              | | | data type for hierarchical tree-like structure
 moddatetime        | | | functions for tracking last modification time
 pageinspect        | | | inspect the contents of database pages at a low level
 pg_buffercache     | | | examine the shared buffer cache in real time
 pg_freespacemap    | | | examine the free space map (FSM)
 pg_stat_statements | | | tracking execution statistics of all SQL statements executed
 pg_trgm            | | | determine the similarity of text, with indexing support
 pgcrypto           | | | cryptographic functions
 pgrowlocks         | | | show row locking information for a specified table
 pgstattuple        | | | obtain tuple-level statistics
 prefix             | | | Prefix Match Indexing
 refint             | | | functions for implementing referential integrity
 seg                | | | data type for representing line segments, or floating point intervals
 tablefunc          | | | various functions that return tables, including crosstab(text sql)
 test_parser        | | | example of a custom parser for full-text search
 timetravel         | | | functions for implementing time travel
 tsearch2           | | | backwards-compatible text search functionality (pre-8.3)
 unaccent           | | | text search dictionary that removes accents
(36 rows)
</src>

Ok I've edited the output in a visible way, to leave the *Version* and *Custom
Variable Classes* column out. It's taking lots of screen place and it's not
that useful here. Maybe the *classes* one will even get dropped out of the
patch before reaching =9.1=, we'll see.

Let's pick an extension there and install it in our new database:

<src>
exts=# create extension pg_trgm;
NOTICE:  Installing extension 'pg_trgm' from '/Users/dim/pgsql/exts/share/contrib/pg_trgm.sql', with user data
CREATE EXTENSION
exts=# \dx
                                           List of extensions
  Name   |  |  |                       Description                       
---------+--+--+---------------------------------------------------------
 pg_trgm |  |  | determine the similarity of text, with indexing support
(1 row)
</src>

See, that was easy enough. Same thing, the extra columns have been
removed. So, what's in this extension, will you ask me, what are those
objects that you would normally (that is, before the patch) find in your
=pg_dump= backup script?

<src>
exts=# select * from pg_extension_objects('pg_trgm');
    class     | classid | objid |                                                                objdesc                                                                 
--------------+---------+-------+----------------------------------------------------------------------------------------------------------------------------------------
 pg_extension |    3996 | 18498 | extension pg_trgm
 pg_proc      |    1255 | 18499 | function set_limit(real)
 pg_proc      |    1255 | 18500 | function show_limit()
 pg_proc      |    1255 | 18501 | function show_trgm(text)
 pg_proc      |    1255 | 18502 | function similarity(text,text)
 pg_proc      |    1255 | 18503 | function similarity_op(text,text)
 pg_operator  |    2617 | 18504 | operator %(text,text)
 pg_type      |    1247 | 18505 | type gtrgm
 pg_proc      |    1255 | 18506 | function gtrgm_in(cstring)
 pg_proc      |    1255 | 18507 | function gtrgm_out(gtrgm)
 pg_type      |    1247 | 18508 | type gtrgm[]
 pg_proc      |    1255 | 18509 | function gtrgm_consistent(internal,text,integer,oid,internal)
 pg_proc      |    1255 | 18510 | function gtrgm_compress(internal)
 pg_proc      |    1255 | 18511 | function gtrgm_decompress(internal)
 pg_proc      |    1255 | 18512 | function gtrgm_penalty(internal,internal,internal)
 pg_proc      |    1255 | 18513 | function gtrgm_picksplit(internal,internal)
 pg_proc      |    1255 | 18514 | function gtrgm_union(bytea,internal)
 pg_proc      |    1255 | 18515 | function gtrgm_same(gtrgm,gtrgm,internal)
 pg_opfamily  |    2753 | 18516 | operator family gist_trgm_ops for access method gist
 pg_opclass   |    2616 | 18517 | operator class gist_trgm_ops for access method gist
 pg_amop      |    2602 | 18518 | operator 1 %(text,text) of operator family gist_trgm_ops for access method gist
 pg_amproc    |    2603 | 18519 | function 1 gtrgm_consistent(internal,text,integer,oid,internal) of operator family gist_trgm_ops for access method gist
 pg_amproc    |    2603 | 18520 | function 2 gtrgm_union(bytea,internal) of operator family gist_trgm_ops for access method gist
 pg_amproc    |    2603 | 18521 | function 3 gtrgm_compress(internal) of operator family gist_trgm_ops for access method gist
 pg_amproc    |    2603 | 18522 | function 4 gtrgm_decompress(internal) of operator family gist_trgm_ops for access method gist
 pg_amproc    |    2603 | 18523 | function 5 gtrgm_penalty(internal,internal,internal) of operator family gist_trgm_ops for access method gist
 pg_amproc    |    2603 | 18524 | function 6 gtrgm_picksplit(internal,internal) of operator family gist_trgm_ops for access method gist
 pg_amproc    |    2603 | 18525 | function 7 gtrgm_same(gtrgm,gtrgm,internal) of operator family gist_trgm_ops for access method gist
 pg_proc      |    1255 | 18526 | function gin_extract_trgm(text,internal)
 pg_proc      |    1255 | 18527 | function gin_extract_trgm(text,internal,smallint,internal,internal)
 pg_proc      |    1255 | 18528 | function gin_trgm_consistent(internal,smallint,text,integer,internal,internal)
 pg_opfamily  |    2753 | 18529 | operator family gin_trgm_ops for access method gin
 pg_opclass   |    2616 | 18530 | operator class gin_trgm_ops for access method gin
 pg_amop      |    2602 | 18531 | operator 1 %(text,text) of operator family gin_trgm_ops for access method gin
 pg_amproc    |    2603 | 18532 | function 1 btint4cmp(integer,integer) of operator family gin_trgm_ops for access method gin
 pg_amproc    |    2603 | 18533 | function 2 gin_extract_trgm(text,internal) of operator family gin_trgm_ops for access method gin
 pg_amproc    |    2603 | 18534 | function 3 gin_extract_trgm(text,internal,smallint,internal,internal) of operator family gin_trgm_ops for access method gin
 pg_amproc    |    2603 | 18535 | function 4 gin_trgm_consistent(internal,smallint,text,integer,internal,internal) of operator family gin_trgm_ops for access method gin
(38 rows)
</src>

This function main intended users are the *extension* authors themselves, so
that it's easy for them to figure out which system identifier (the =objid=
column) has been attributed to some =SQL= objects from their install
script. With this knowledge, you can prepare some *upgrade* scripts. But
that's for another patch altogether, so we'll get back to the matter in
another blog entry.

So we chose [[http://www.postgresql.org/docs/9.0/interactive/pgtrgm.html][trgm]] as an example, let's follow the documentation and create a
test table and a custom index in there, just so that the extension is put to
good use. Then let's try to =DROP= our extension, because we're testing the
infrastructure, right?

<src>
exts=# create table test(id bigint, name text);
CREATE TABLE
exts=# CREATE INDEX idx_test_name ON test USING gist (name gist_trgm_ops);
CREATE INDEX
exts=# drop extension pg_trgm;
ERROR:  cannot drop extension pg_trgm because other objects depend on it
DETAIL:  index idx_test_name depends on operator class gist_trgm_ops for access method gist
HINT:  Use DROP ... CASCADE to drop the dependent objects too.
</src>

Of course PostgreSQL is smart enough here — the *extension* patch had nothing
special to do to achieve that, apart from recording the dependencies. Next,
as we didn't =drop extension pg_trgm cascade;=, it's still in the database. So
let's see what a =pg_dump= will look like. As it's quite a lot of text to
paste, let's see the =pg_restore= catalog instead. And that's a feature that
needs to be known some more, too.

<src>
dim ~ pg_dump -Fc exts | pg_restore -l | grep -v '^;'
1812; 1262 18497 DATABASE - exts dim
1; 3996 18498 EXTENSION - pg_trgm 
1813; 0 0 COMMENT - EXTENSION pg_trgm 
6; 2615 2200 SCHEMA - public dim
1814; 0 0 COMMENT - SCHEMA public dim
1815; 0 0 ACL - public dim
320; 2612 11602 PROCEDURAL LANGUAGE - plpgsql dim
1521; 1259 18543 TABLE public test dim
1809; 0 18543 TABLE DATA public test dim
1808; 1259 18549 INDEX public idx_test_name dim
</src>

As you see, the only SQL object that got into the backup are an =EXTENSION=
and its =COMMENT=. Nothing like the types or the functions that the =pg_trgm=
script creates.

** What does it means to extension authors?

In order to be an *extension*, you have to prepare a *control* file where to
give the necessary information to register your script. This file must be
named =extension.control= if the script is named =extension.sql=, at least at
the moment. This file can benefit from some variable expansion too, like
does the current =extension.sql.in=, in that if you provide an
=extension.control.in= file the term =VERSION= will be expanded to whatever
=$(VERSION)= is set to in your =Makefile=.

If you never wrote a =C= coded *extension* for PostgreSQL, this might look
complex and irrelevant. Baseline is that you need a =Makefile= so that you can
benefit easily from the PostgreSQL infrastructure work and have the =make
install= operation place your files at the right place, including the new
=control= file.

** That's it for today, folks

A next blog entry will detail what happens with extensions providing *user
data*, and the =CREATE EXTENSION name WITH NO DATA;= variant. Stay tuned!

* 20101015-11:30 Extensions: writing a patch for PostgreSQL

These days, thanks to my [[http://2ndquadrant.com/][community oriented job]], I'm working full time on a
[[http://www.postgresql.org/][PostgreSQL]] patch to terminate basic support for [[http://www.postgresql.org/docs/9/static/extend.html][extending SQL]]. First thing I
want to share is that patching the *backend code* is not as hard as one would
think. Second one is that [[http://git-scm.com/][git]] really is helping.

*“Not as hard as one would think*, are you kidding me?”, I hear some
say. Well, that's true. It's =C= code in there, but with a very good layer of
abstractions so that you're not dealing with subtle problems that much. Of
course it happens that you have to, and managing the memory isn't an
option. That said, =palloc()= and the *memory contexts* implementation makes
that as easy as *in lots of cases, you don't have to think about it*.

PostgreSQL is very well known for its reliability, and that's not something
that just happened. All the source code is organized in a way that makes it
possible, so your main task is to write code that looks as much as possible
like the existing surrounding code. And we all know how to *copy paste*,
right?

So, my current work on the *extensions* is to make it so that if you install
[[http://www.postgresql.org/docs/9.0/interactive/hstore.html][hstore]] in your database (to pick an example), your backup won't contain any
*hstore* specific objects (types, functions, operators, index support objects,
etc) but rather a single line that tells PostgreSQL to install *hstore* again.

<src lang="sql">
CREATE EXTENSION hstore;
</src>

The feature already works in [[http://git.postgresql.org/gitweb?p=postgresql-extension.git;a=shortlog;h=refs/heads/extension][my git branch]] and I'm extracting infrastructure
work in there to ease review. That's when =git= helps a lot. What I've done is
create a new branch from the master one, then [[http://www.kernel.org/pub/software/scm/git/docs/git-cherry-pick.html][cherry pick]] the patches of
interest. Well sometime you have to resort to helper tools. I've been told
after the fact that using =git cherry-pick -n= would have allowed the
following to be much simpler:

<src>
dim ~/dev/PostgreSQL/postgresql-extension git cherry-pick 3f291b4f82598309368610431cf2a18d7b7a7950
error: could not apply 3f291b4... Implement dependency tracking for CREATE EXTENSION, and DROP EXTENSION ... CASCADE.
hint: after resolving the conflicts, mark the corrected paths
hint: with 'git add <paths>' or 'git rm <paths>'
hint: and commit the result with 'git commit -c 3f291b4'
dim ~/dev/PostgreSQL/postgresql-extension git status \
| awk '/modified/ && ! /both/ && ! /genfile/ {print $3}
       /deleted/ {print $5}
       /both/    {print $4}' \
| xargs echo git reset -- \
| sh
Unstaged changes after reset:
M	src/backend/catalog/dependency.c
M	src/backend/catalog/heap.c
M	src/backend/catalog/pg_aggregate.c
M	src/backend/catalog/pg_conversion.c
M	src/backend/catalog/pg_namespace.c
M	src/backend/catalog/pg_operator.c
M	src/backend/catalog/pg_proc.c
M	src/backend/catalog/pg_type.c
M	src/backend/commands/extension.c
M	src/backend/commands/foreigncmds.c
M	src/backend/commands/opclasscmds.c
M	src/backend/commands/proclang.c
M	src/backend/commands/tsearchcmds.c
M	src/backend/nodes/copyfuncs.c
M	src/backend/nodes/equalfuncs.c
M	src/backend/parser/gram.y
M	src/include/catalog/dependency.h
M	src/include/commands/extension.h
M	src/include/nodes/parsenodes.h
</src>

That's what I did to prepare a side branch containing only changes to a part
of my current work. I had to filter the diff so much only because I'm
commiting in rather big steps, rather than very little chunks at a time. In
this case that means I had a single patch with several *units* of changes and
I wanted to extract only one. Well, it happens that even in such a case, =git=
is helping!

There's more to say about the *extension* related feature of course, but
that'll do it for this article. I'd just end up with the following nice
*diffstat* of 4 days of work:

<src>
dim ~/dev/PostgreSQL/postgresql-extension git --no-pager diff master..|wc -l
    3897
dim ~/dev/PostgreSQL/postgresql-extension git --no-pager diff master..|diffstat
 doc/src/sgml/extend.sgml               |   46 ++
 doc/src/sgml/ref/allfiles.sgml         |    2 
 doc/src/sgml/ref/create_extension.sgml |   95 ++++
 doc/src/sgml/ref/drop_extension.sgml   |  115 +++++
 doc/src/sgml/reference.sgml            |    2 
 src/backend/access/transam/xlog.c      |   95 ----
 src/backend/catalog/Makefile           |    1 
 src/backend/catalog/dependency.c       |   25 +
 src/backend/catalog/heap.c             |    9 
 src/backend/catalog/objectaddress.c    |   14 
 src/backend/catalog/pg_aggregate.c     |    7 
 src/backend/catalog/pg_conversion.c    |    7 
 src/backend/catalog/pg_namespace.c     |   13 
 src/backend/catalog/pg_operator.c      |    7 
 src/backend/catalog/pg_proc.c          |    7 
 src/backend/catalog/pg_type.c          |    8 
 src/backend/commands/Makefile          |    3 
 src/backend/commands/comment.c         |    6 
 src/backend/commands/extension.c       |  688 +++++++++++++++++++++++++++++++++
 src/backend/commands/foreigncmds.c     |   19 
 src/backend/commands/functioncmds.c    |    7 
 src/backend/commands/opclasscmds.c     |   13 
 src/backend/commands/proclang.c        |    7 
 src/backend/commands/tsearchcmds.c     |   25 +
 src/backend/nodes/copyfuncs.c          |   22 +
 src/backend/nodes/equalfuncs.c         |   18 
 src/backend/parser/gram.y              |   51 ++
 src/backend/tcop/utility.c             |   27 +
 src/backend/utils/adt/genfile.c        |  193 +++++++++
 src/backend/utils/init/postinit.c      |    3 
 src/backend/utils/misc/Makefile        |    2 
 src/backend/utils/misc/cfparser.c      |  113 +++++
 src/backend/utils/misc/guc-file.l      |   26 -
 src/backend/utils/misc/guc.c           |  160 ++++++-
 src/bin/pg_dump/common.c               |    6 
 src/bin/pg_dump/pg_dump.c              |  520 ++++++++++++++++++++++--
 src/bin/pg_dump/pg_dump.h              |   10 
 src/bin/pg_dump/pg_dump_sort.c         |    7 
 src/bin/psql/command.c                 |    3 
 src/bin/psql/describe.c                |   45 ++
 src/bin/psql/describe.h                |    3 
 src/bin/psql/help.c                    |    1 
 src/include/catalog/dependency.h       |    1 
 src/include/catalog/indexing.h         |    6 
 src/include/catalog/pg_extension.h     |   61 ++
 src/include/catalog/pg_proc.h          |   13 
 src/include/catalog/toasting.h         |    1 
 src/include/commands/extension.h       |   54 ++
 src/include/nodes/nodes.h              |    2 
 src/include/nodes/parsenodes.h         |   20 
 src/include/parser/kwlist.h            |    1 
 src/include/utils/builtins.h           |    4 
 src/include/utils/cfparser.h           |   18 
 src/include/utils/guc.h                |   11 
 src/makefiles/pgxs.mk                  |   21 -
 55 files changed, 2456 insertions(+), 188 deletions(-)
</src>


* 20101008-10:00 Date puzzle for starters

The [[http://www.postgresql.org/][PostgreSQL]] =IRC= channel is a good place to be, for all the very good help
you can get there, because people are always wanting to remain helpful,
because of the off-topics discussions sometime, or to get to talk with
community core members. And to start up your day too.

This morning's question started simple : “how can I check if today is the
"first sunday fo the month". or "the second tuesday of the month" etc?”

And the first version of the answer, quite simple it is too:

<src lang="sql">
dim=#   with begin(d) as (select date_trunc('month', 'today'::date)::date) 
dim-# select d + 7 - extract(dow from d)::int as sunday from begin;
   sunday   
------------
 2010-10-03
(1 row)
</src>

So you just have to compare the result of the function with ='today'::date=
and there you go. The problem is that the question could be read in the
other way round, like, what is today in *first* or *second* *day name* of this
month *format*? Once more, [[http://blog.rhodiumtoad.org.uk/][RhodiumToad]] to the rescue:

<src lang="sql">
select to_char(current_date,
               '"' || ((ARRAY['First','Second','Third','Fourth','Fifth'])
                             [(extract(day from current_date)::integer - 1)/7 + 1]
                      ) 
                   || '" Day');
     to_char      
------------------
 Second Friday   
(1 row)
</src>

That's a straight answer to the question, read that way!

But the part that I found nice to play with was my first reading of the
question, as I don't get to lose my ideas that easily, you see… so what
about writing a function to return the date of any *nth* occurrence of a given
*day of week* in a *given month*, defaulting to this very month?

<src lang="sql">
create or replace function get_nth_dow_of_month
 (
  nth int,
  dow int,
  begin date default current_date
 )
 returns date
 language sql
 strict
 as
$$
with month(d) as (
  select generate_series(date_trunc('month', $3), 
                         date_trunc('month', $3) + interval '1 month - 1 day', 
                         interval '1 day')::date
), 
     repeat as (
  select d, extract(dow from d) as dow, (d - date_trunc('month', $3)::date) / 7 as repeat
    from month
) 
select d
  from repeat
 where dow = $2 and repeat = $1;
$$;

dim=# select get_nth_dow_of_month(0, 0);
 get_nth_dow_of_month 
----------------------
 2010-10-03
(1 row)

dim=# select get_nth_dow_of_month(1, 4, '2010-09-12');
 get_nth_dow_of_month 
----------------------
 2010-09-09
(1 row)
</src>

So you see we just got the first Sunday of this month =(0, 0)= and the second
Thursday =(1, 4)= of the previous one. Any date within a month is a good way
to tell which month you want to work in, as the function's written, abusing
=date_trunc= like it does.

Now the way the function is written is unfinished. You want to fix it in one
of two ways. Either stop using =generate_series= to only output one row at a
time, or fix the =API= so that you can ask for more than a *nth dow* at a
time. Of course, that was a starter for me, not a problem I need to solve
directly, and that was a good excuse for a blog entry, so I won't fix
it. That's left as an exercise to our interested readers!

* 20101007-17:15 Resuming work on Extensions, first little step

Yeah I'm back on working on my part of the extension thing in [[http://www.postgresql.org/][PostgreSQL]].

First step is a little one, but as it has public consequences, I figured I'd
talk about it already. I've just refreshed my =git= repository to follow the
new =master= one, and you can see that here
[[http://git.postgresql.org/gitweb?p=postgresql-extension.git;a=commitdiff;h=9a88e9de246218e93c04b6b97e1ef61d97925430]].

It's been easier than I feared, mainly:

<src>
$ git --no-pager diff master..extension
$ git --no-pager format-patch master..extension
$ cp 0001-First-stab-at-writing-pg_execute_from_file-function.patch ..
$ git checkout master
$ git pull -f pgmaster
$ git reset --hard pgmaster/master
$ git checkout extension
$ git reset --hard master
$ git am -s ../0001-First-stab-at-writing-pg_execute_from_file-function.edit.patch 
$ git status
$ git log --short | head
$ git log -n2 --oneline
$ git push -f
</src>

So that's still more steps that one want to call dead simple, but still. The
=format-patch= command is to save my work away (all patches that are in the
*extension* branch but not in the *master* — well that was only one of them
here). Then, as the master repository =URL= didn't change, I can simply =pull=
the changes in. Of course I had a nice message *warning: no common commits*.

Once pulled, I trashed my local copy and replaced it with the new official
one, that's =git reset --hard pgmaster/master=, then in the *extension* branch I
could trash it and have it linked to the local =master= again.

Of course, the =git am= method wouldn't apply my patch as-is, there was some
underlying changes in the source files, the identification tag changed from
=$PostgreSQL$= to, e.g., =src/backend/utils/adt/genfile.c=, and I had to cope
with that. Maybe there's some tool (=git am -3= ?) to do it automatically, I
just copy edited the =.patch= file.

Lastly, it's all about checking the result and publishing the result. This
last line is =git push -f= and is when I just trashed and replaced my
[[http://git.postgresql.org/gitweb?p=postgresql-extension.git;a=summary][postgresql-extension]] community repository. I don't think anybody was
following it, but should it be the case, you will have to *reinit* your copy.

More blog posts to come about extensions, as I arranged to have some real
time to devote on the topic. At least I was able to arrange things so that I
can work on the subject for real, and the first thing I did, the very night
before it was meant to begin, is catch a *tonsillitis*. Lost about a week, not
the project! Stay tuned!

* 20100926-21:00 Regexp performances and Finite Automata

The major reason why I dislike [[http://www.perl.org/][perl]] so much, and [[http://www.ruby-lang.org][ruby]] too, and the thing I'd
want different in the [[http://www.gnu.org/software/emacs/manual/elisp.html][Emacs Lisp]] =API= so far is how they set developers mind
into using [[http://www.regular-expressions.info/][regexp]]. You know the quote, don't you?

<quote>
Some people, when confronted with a problem, think “I know, I'll use regular
expressions.” Now they have two problems.
</quote>

That said, some situations require the use of *regexp* — or are so much
simpler to solve using them than the maintenance hell you're building here
ain't that big a drag. The given expressiveness is hard to match with any
other solution, to the point I sometime use them in my code (well I use [[http://www.emacswiki.org/emacs/rx][rx]]
to lower the burden sometime, just see this example).

<src lang="emacs-lisp">
(rx bol (zero-or-more blank) (one-or-more digit) ":")
"^[[:blank:]]*[[:digit:]]+:"
</src>

The thing you might want to know about *regexp* is that computing them is an
heavy task usually involving *parsing* their representation, *compiling* it to
some executable code, and then *executing* generated code. It's been showed in
the past (as soon as 1968) that a *regexp* is just another way to write a
finite automata, at least as soon as you don't need *backtracking*. The
writing of this article is my reaction to reading
[[http://swtch.com/~rsc/regexp/regexp1.html][Regular Expression Matching Can Be Simple And Fast]] (but is slow in Java,
Perl, PHP, Python, Ruby, ...), a very interesting article — see the
benchmarks in there.

The bulk of it is that we find mainly two categories of *regexp* engine in the
wild, those that are using [[http://en.wikipedia.org/wiki/Nondeterministic_finite_state_machine][NFA]] and [[http://en.wikipedia.org/wiki/Deterministic_finite_automaton][DFA]] intermediate representation
techniques, and the others. Our beloved [[http://www.postgresql.org/][PostgreSQL]] sure offers the feature,
it's the =~= and =~*= [[http://www.postgresql.org/docs/9.0/interactive/functions-matching.html][operators]]. The implementation here is based on
[[http://www.arglist.com/regex/][Henry Spencer]]'s work, which the aforementioned article says

<quote>
became very widely used, eventually serving as the basis for the slow
regular expression implementations mentioned earlier: Perl, PCRE, Python,
and so on.
</quote>

Having a look at the actual implementation shows that indeed, current
PostgreSQL code for *regexp* matching uses intermediate representations of
them as =NFA= and =DFA=. The code is quite complex, even more than I though it
would be, and I didn't have the time it would take to check it against the
proposed one from the *simple and fast* article.

<src>
postgresql/src/backend/regex
  -rw-r--r--   1 dim  staff   4362 Sep 25 20:59 COPYRIGHT
  -rw-r--r--   1 dim  staff    614 Sep 25 20:59 Makefile
  -rw-r--r--   1 dim  staff  28217 Sep 25 20:59 re_syntax.n
  -rw-r--r--   1 dim  staff  16589 Sep 25 20:59 regc_color.c
  -rw-r--r--   1 dim  staff   3464 Sep 25 20:59 regc_cvec.c
  -rw-r--r--   1 dim  staff  25036 Sep 25 20:59 regc_lex.c
  -rw-r--r--   1 dim  staff  16845 Sep 25 20:59 regc_locale.c
  -rw-r--r--   1 dim  staff  35917 Sep 25 20:59 regc_nfa.c
  -rw-r--r--   1 dim  staff  50714 Sep 25 20:59 regcomp.c
  -rw-r--r--   1 dim  staff  17368 Sep 25 20:59 rege_dfa.c
  -rw-r--r--   1 dim  staff   3627 Sep 25 20:59 regerror.c
  -rw-r--r--   1 dim  staff  27664 Sep 25 20:59 regexec.c
  -rw-r--r--   1 dim  staff   2122 Sep 25 20:59 regfree.c
</src>

So all in all, I'll continue avoiding *regexp* as much as I currently do, and
will maintain my tendency to using [[http://www.gnu.org/manual/gawk/gawk.html][awk]] when I need them on files (it allows
to refine the searching without resorting to more and more pipes in the
command line). And as far as resorting to using *regexp* in PostgreSQL is
concerned, it seems that the code here is already about topnotch. Once more.

* 20100912-21:35 Window Functions example remix

The drawback of hosting a static only website is, obviously, the lack of
comments. What happens actually, though, is that I receive very few comments
by direct mail. As I don't get another *spam* source to cleanup, I'm left
unconvinced that's such a drawback. I still miss the low probability of
seeing blog readers exchange directly, but I think a =tapoueh.org= mailing
list would be my answer, here...

Anyway, [[http://people.planetpostgresql.org/dfetter/][David Fetter]] took the time to send me a comment by mail with a
cleaned up rewrite of the previous entry =SQL=, here's it for your pleasure!

<src lang="sql">
WITH t AS (
    SELECT
        o, w,
        CASE WHEN
            LAG(w) OVER(w) IS DISTINCT FROM w AND
            ROW_NUMBER() OVER (w) > 1 /* Eliminate first change */
        THEN 1
        END AS change
    FROM (
        VALUES
            (1, 5),
            (2, 10),
            (3, 7),
            (4, 7),
            (5, 7)
    ) AS data(o, w)
    WINDOW w AS (ORDER BY o) /* Factor out WINDOW */
)
SELECT SUM(change) FROM t;
</src>

As you can see ***David*** chose to filter the first change in the subquery rather
than hacking it away with a simple =-1= at the outer level. I'm still
wondering which way is cleaner (that depends on how you look at the
problem), but I think I know which one is simpler! Thanks ***David*** for this
blog entry!

* 20100909-16:35 Window Functions example

So, when =8.4= came out there was all those comments about how getting
[[http://www.postgresql.org/docs/8.4/interactive/tutorial-window.html][window functions]] was an awesome addition. Now, it seems that a lot of people
seeking for help in [[http://wiki.postgresql.org/index.php?title=IRC][#postgresql]] just don't know what kind of problem this
feature helps solving. I've already been using them in some cases here in
this blog, for getting some nice overview about
[[http://tapoueh.org/articles/blog/_Partitioning:_relation_size_per_%E2%80%9Cgroup%E2%80%9D.html][Partitioning: relation size per “group”]].

Now, another example use case rose on =IRC= today. I'll quote directly our user here:

<quote>
  hey there, how can i count the number of (value) changes in one column?

  example: a table with a column *weight*. let's say we have 5 rows, having
  the following values for weight: =5, 10, 7, 7, 7=. the number of changes of
  weight would be 2 here (from 5 to 10 and 10 to 7). any idea how I could do
  that in SQL using PGSQL 8.4.4? GROUP BY or count(distinct weight)
  obviously does not work. thx in advance
</quote>

Now, several of us began talking about *window functions* and about the fact
that you need some other column to identify the ordering of those weights,
obviously, because that's the only way to define what a change is in this
context. Let's have a first try at it.

<src lang="sql">
=# select o, w, 
          case when lag(w) over(order by o) is distinct from w then 1 end as change
     from (values (1, 5), (2, 10), (3, 7), (4, 7), (5, 7)) as data(o, w);
 o | w  | change 
---+----+--------
 1 |  5 |      1
 2 | 10 |      1
 3 |  7 |      1
 4 |  7 |       
 5 |  7 |       
(5 rows)
</src>

Not too bad, but of course we are seeing a false change on the first line,
as for any *window* of rows you define the previous one, given by =lag()
over()=, will be =NULL=. The easiest way to accommodate is the following:

<src lang="sql">
=# select sum(change) -1 as changes 
     from (select case when lag(w) over(order by o) is distinct from w
                       then 1
                   end as change
             from (values (1, 5),
                          (2, 10),
                          (3, 7),
                          (4, 7),
                          (5, 7)) as t(o, w)) as x;
 changes 
---------
       2
(1 row)
</src>

So don't be shy and go read about [[http://www.postgresql.org/docs/8.4/interactive/sql-expressions.html#SYNTAX-WINDOW-FUNCTIONS][window functions in SQL expressions]] and
[[http://www.postgresql.org/docs/8.4/interactive/queries-table-expressions.html#QUERIES-WINDOW][window function processing]] in the query table expressions. That's a very
nice tool to have and my guess is that you will soon enough realize the only
reason why you could think you don't have a need for them is that you didn't
know it existed, and what you can do with it. *Sharpen your saw!* :)

* 20100906-18:05 Synchronous Replication

#20100906-18:05

Although the new asynchronous replication facility that ships with 9.0 ain't
released to the wide public yet, our hackers hero are already working on the
synchronous version of it. A part of the facility is rather easy to design,
we want something comparable to [[http://www.drbd.org/][DRBD]] flexibility, but specific to our
database world.  So *synchronous* would either mean *recv*, *fsync* or *apply*,
depending on what you need the *standby* to have already done when the master
acknowledges the =COMMIT=. Let's call that the *service level*.

The part of the design that's not so easy is more interesting. Do we need to
register standbys and have the *service level* setup per standby? Can we get
some more flexibility and have the *service level* set on a per-transaction
basis? The idea here would be that the application knows which transactions
are meant to be extra-safe and which are not, the same way that you can set
=synchronous_commit to off= when dealing with web sessions, for example.

*Why choosing?* I hear you ask. Well, it's all about having more data safety,
and a typical setup would contain an asynchronous reporting server and a
local *failover* synchronous server. Then add a remote one, too. So even if we
pick the transaction based facility, we still want to be able to choose at
setup time which server to failover to. Than means we don't want that much
flexibility now, we want to know where the data is safe, we don't want to
have to guess.

Some way to solve that is to be able to setup a slave as being the failover
one, or say, the =sync= one. Now, the detail that ruins it all is that we need
a *timeout* to handle worst cases when a given slave loses its connectivity
(or power, say). Now, the slave ain't in *sync* any more and some people will
require that the service is still available (*timeout* but =COMMIT=) and some
will require that the service is down: don't accept a new transaction if you
can't make its data safe to the slave too.

The answer would be to have the master arbitrate between what the
transaction wants and what the slave is setup to provide, and what it's able
to provide at the time of the transaction. Given a transaction with a
*service level* of *apply* and a slave setup for being *async*, the =COMMIT= does
not have to wait, because there's no known slave able to offer the needed
level. Or the =COMMIT= can not happen, for the very same reason.

Then I think it all flows quite naturally from there, and while arbitrating
the master could record which slave is currently offering what *service
level*. And offering the information in a system view too, of course.

The big question that's not answered in this proposal is how to setup that
being unable to reach the wanted *service level* is an error or a
warning?

That too would need to be for the master to arbitrate based on a per standby
and a per transaction setting, and in the general case it could be a *quorum*
setup: each slave is given a *weight* and each transaction a *quorum* to
reach. The master sums up the weights of the standby that ack the
transaction at the needed *service level* and the =COMMIT= happens as soon as
the quorum is reached, or is canceled as soon as the *timeout* is reached,
whichever comes first.

Such a model allows for very flexible setups, where each standby has a
*weight* and offers a given *service level*, and each transaction waits until a
*quorum* is reached. Giving the right weights to your standbys (like, powers
of two) allow you to set the quorum in a way that only one given standby is
able to acknowledge the most important transactions. But that's flexible
enough you can change it at any time, it's just a *weight* that allows a *sum*
to be made, so my guess would be it ends up in the *feedback loop* between the
standby and its master.

The most appealing part of this proposal is that it doesn't look complex to
implement, and should allow for highly flexible setups. Of course, the devil
is in the details, and we're talking about latencies in the distributed
system here. That's also being discussed on the [[http://archives.postgresql.org/pgsql-hackers/][mailing list]].

* 20100830-11:00 Happy Numbers

#20100830-11:00

After discovering the excellent [[http://gwene.org/][Gwene]] service, which allows you to subscribe
to *newsgroups* to read =RSS= content (*blogs*, *planets*, *commits*, etc), I came to
read this nice article about [[http://programmingpraxis.com/2010/07/23/happy-numbers/][Happy Numbers]]. That's a little problem that
fits well an interview style question, so I first solved it yesterday
evening in [[static/happy-numbers.el][Emacs Lisp]] as that's the language I use the most those days.

<quote>
  A happy number is defined by the following process. Starting with any
  positive integer, replace the number by the sum of the squares of its
  digits, and repeat the process until the number equals 1 (where it will
  stay), or it loops endlessly in a cycle which does not include 1. Those
  numbers for which this process ends in 1 are happy numbers, while those
  that do not end in 1 are unhappy numbers (or sad numbers).
</quote>

Now, what about implementing the same in pure =SQL=, for more fun? Now that's
interesting! After all, we didn't get =WITH RECURSIVE= for tree traversal
only, [[http://archives.postgresql.org/message-id/e08cc0400911042333o5361b21cu2c9438f82b1e55ce@mail.gmail.com][did we]]?

Unfortunately, we need a little helper function first, if only to ease the
reading of the recursive query. I didn't try to inline it, but here it goes:

<src lang="sql">
create or replace function digits(x bigint)
  returns setof int
  language sql
as $$
  select substring($1::text from i for 1)::int
    from generate_series(1, length($1::text)) as t(i)
$$;
</src>

That was easy: it will output one row per digit of the input number — and
rather than resorting to powers of ten and divisions and remainders, we do
use plain old text representation and =substring=. Now, to the real
problem. If you're read what is an happy number and already did read the
fine manual about [[http://www.postgresql.org/docs/8.4/interactive/queries-with.html][Recursive Query Evaluation]], it should be quite easy to
read the following:

<src lang="sql">
with recursive happy(n, seen) as (
    select 7::bigint, '{}'::bigint[]
  union all           
    select sum(d*d), h.seen || sum(d*d)
      from (select n, digits(n) as d, seen
              from happy
           ) as h     
  group by h.n, h.seen
    having not seen @> array[sum(d*d)]
)                     
  select * from happy;
  n  |       seen       
-----+------------------
   7 | {}
  49 | {49}
  97 | {49,97}
 130 | {49,97,130}
  10 | {49,97,130,10}
   1 | {49,97,130,10,1}
(6 rows)

Time: 1.238 ms
</src>

That shows how it works for some *happy* number, and it's easy to test for a
non-happy one, like for example =17=. The query won't cycle thanks to the =seen=
array and the =having= filter, so the only difference between an *happy* and a
*sad* number will be that in the former case the last line output by the
recursive query will have <code>n = 1</code>. Let's expand this knowledge
into a proper function (because we want to be able to have the number we
test for happiness as an argument):

<src lang="sql">
create or replace function happy(x bigint)
  returns boolean
  language sql
as $$
with recursive happy(n, seen) as (
    select $1, '{}'::bigint[]
  union all 
    select sum(d*d), h.seen || sum(d*d)
      from (select n, digits(n) as d, seen
              from happy
           ) as h
  group by h.n, h.seen
    having not seen @> array[sum(d*d)]
)
  select n = 1 as happy
    from happy
order by array_length(seen, 1) desc nulls last
   limit 1
$$;
</src>

We need the =desc nulls last= trick in the =order by= because the =array_length()=
of any dimension of an empty array is =NULL=, and we certainly don't want to
return all and any number as unhappy on the grounds that the query result
contains a line =input, {}=. Let's now play the same tricks as in the puzzle
article:

<src lang="sql">
=# select array_agg(x) as happy from generate_series(1, 50) as t(x) where happy(x);
              happy               
----------------------------------
 {1,7,10,13,19,23,28,31,32,44,49}
(1 row)

Time: 24.527 ms

=# explain analyze select x from generate_series(1, 10000) as t(x) where happy(x);
                      QUERY PLAN                                                        
----------------------------------------------------------------------------------------
 Function Scan on generate_series t  (cost=0.00..265.00 rows=333 width=4)
                          (actual time=2.938..3651.019 rows=1442 loops=1)
   Filter: happy((x)::bigint)
 Total runtime: 3651.534 ms
(3 rows)

Time: 3652.178 ms
</src>

(Yes, I tricked the =EXPLAIN ANALYZE= output so that it fits on the page width
here). For what it's worth, finding the first =10000= happy numbers in *Emacs
Lisp* on the same laptop takes =2830 ms=, also running a recursive version of
the code.

* 20100826-17:45 Playing with bit strings

#20100826-17:45

The idea of the day ain't directly from me, I'm just helping with a very
thin subpart of the problem. The problem, I can't say much about, let's just
assume you want to reduce the storage of =MD5= in your database, so you want
to abuse [[http://www.postgresql.org/docs/8.4/interactive/datatype-bit.html][bit strings]]. A solution to use them works fine, but the datatype is
still missing some facilities, for example going from and to hexadecimal
representation in text.

<src lang="sql">
create or replace function hex_to_varbit(h text)
 returns varbit
 language sql
as $$
  select ('X' || $1)::varbit;
$$;

create or replace function varbit_to_hex(b varbit)
 returns text
 language sql
as $$
  select array_to_string(array_agg(to_hex((b << (32*o))::bit(32)::bigint)), '')
    from (select b, generate_series(0, n-1) as o
            from (select $1, octet_length($1)/4) as t(b, n)) as x
$$;
</src>

To understand the magic in the second function, let's walk through the tests
one could do when wanting to grasp how things work in the =bitstring= world
(using also some reading of the fine documentation, too).

<src>
=# select ('101011001011100110010110'::varbit << 0)::bit(8);
   bit    
----------
 10101100
(1 row)

=# select ('101011001011100110010110'::varbit << 8)::bit(8);
   bit    
----------
 10111001
(1 row)

=# select ('101011001011100110010110'::varbit << 16)::bit(8);
   bit    
----------
 10010110
(1 row)

=# select * from *TEMP VERSION OF THE FUNCTION FOR TESTING*
 o |                b                 |    x     
---+----------------------------------+----------
 0 | 10101100101111010001100011011011 | acbd18db
 1 | 01001100110000101111100001011100 | 4cc2f85c
 2 | 11101101111011110110010101001111 | edef654f
 3 | 11001100110001001010010011011000 | ccc4a4d8
(4 rows)
</src>

What do we get from that, will you ask? Let's see a little example:

<src>
=# select hex_to_varbit(md5('foo'));
                                                          hex_to_varbit                                                           
----------------------------------------------------------------------------------------------------------------------------------
 10101100101111010001100011011011010011001100001011111000010111001110110111101111011001010100111111001100110001001010010011011000
(1 row)

=# select md5('foo'), varbit_to_hex(hex_to_varbit(md5('foo')));
               md5                |          varbit_to_hex           
----------------------------------+----------------------------------
 acbd18db4cc2f85cedef654fccc4a4d8 | acbd18db4cc2f85cedef654fccc4a4d8
(1 row)
</src>

Storing =varbits= rather than the =text= form of the =MD5= allows us to go from
=6510 MB= down to =4976 MB= on a sample table containing 100 millions
rows. We're targeting more that that, so that's a great win down here!

In case you wonder, querying the main index on =varbit= rather than the one on
=text= for a single result row, the cost of doing the conversion with
=varbit_to_hex= seems to be around =28 µs=. We can afford it.

Hope this helps!

* 20100809-14:45 Editing constants in constraints

#20100809-14:30

We're using constants in some constraints here, for example in cases where
several servers are replicating to the same *federating* one: each origin
server has his own schema, and all is replicated nicely on the central host,
thanks to [[http://wiki.postgresql.org/wiki/Londiste_Tutorial#Federated_database][Londiste]], as you might have guessed already.

For bare-metal recovery scripts, I'm working on how to change those
constants in the constraints, so that =pg_dump -s= plus some schema tweaking
would kick-start a server. Here's a =PLpgSQL= snippet to do just that:

<src lang="sql">
  FOR rec IN EXECUTE
$s$
SELECT schemaname, tablename, conname, attnames, def
  FROM (
   SELECT n.nspname, c.relname, r.conname, 
          (select array_accum(attname)
             from pg_attribute 
            where attrelid = c.oid and r.conkey @> array[attnum]) as attnames, 
          pg_catalog.pg_get_constraintdef(r.oid, true)
   FROM pg_catalog.pg_constraint r 
        JOIN pg_class c on c.oid = r.conrelid 
        JOIN pg_namespace n ON n.oid = c.relnamespace
   WHERE r.contype = 'c'
ORDER BY 1, 2, 3
       ) as cons(schemaname, tablename, conname, attnames, def)
WHERE attnames @> array['server']::name[]
$s$
  LOOP
    rec.def := replace(rec.def, 'server = ' || old_id,
                                'server = ' || new_id);

    sql := 'ALTER TABLE ' || rec.schemaname || '.' || rec.tablename
        || ' DROP CONSTRAINT ' || rec.conname;
    RAISE NOTICE '%', sql;
    RETURN NEXT;
    EXECUTE sql;

    sql := 'ALTER TABLE ' || rec.schemaname || '.' || rec.tablename
        || ' ADD ' || rec.def;
    RAISE NOTICE '%', sql;
    RETURN NEXT;
    EXECUTE sql;

  END LOOP;
</src>

This relies on the fact that our constraints are on the column =server=. Why
would this be any better than a =sed= one-liner, would you ask me? I'm fed up
with having pseudo-parsing scripts and taking the risk that the simple
command will change data I didn't want to edit. I want context aware tools,
pretty please, to *feel* safe.

Otherwise I'd might have gone with <code>pg_dump -s| sed -e 's:\(server =\)
17:\1 18:'</code> but this one-liner already contains too much useless magic
for my taste (the space before =17= ain't in the group match to allow for
having =\1 18= in the right hand side. And this isn't yet parametrized, and
there I'll need to talk to the database, as that's were I store the servers
name and their id (a =bigserial= — yes, the constraints are all generated from
scripts). I don't want to write an *SQL parser* and I don't want to play
loose, so the =PLpgSQL= approach is what I'm thinking as the best tool
here. Opinionated answers get to my mailbox!

* 20100806-13:00 debian packaging PostgreSQL extensions

#20100806-13:00

In trying to help an extension *debian packaging* effort, I've once again
proposed to handle it. That's because I now begin to know how to do it, as
you can see in my [[http://qa.debian.org/developer.php?login=dim%40tapoueh.org][package overview]] page at *debian QA* facility. There's a
reason why I proposed myself here, it's that yet another tool of mine is now
to be found in *debian*, and should greatly help *extension packaging*
there. You can already check for the [[http://packages.debian.org/sid/postgresql-server-dev-all][postgresql-server-dev-all]] package page
if you're that impatient!

Back? Ok, so I used to have two main gripes against debian support for
[[http://www.postgresql.org/][PostgreSQL]]. The first one, which is now feeling alone, is that both project
[[http://wiki.postgresql.org/wiki/PostgreSQL_Release_Support_Policy][release support policy]] aren't compatible enough for debian stable to include
all currently supported stable PostgreSQL major version. That's very bad
that debian stable will only propose one major version, knowing that the
support for several of them is in there.

The problem is two fold: first, debian stable has to maintain any
distributed package. There's no *deprecation policy* allowing for droping the
ball. So the other side of this coin is that debian developers must take on
themselves maintaining included software for as long as stable is not
renamed =oldstable=. And it so happens that there's no debian developer that
feels like maintaining *end of lined* PostgreSQL releases without help from
[[http://www.postgresql.org/community/contributors/][PostgreSQL Core Team]]. Or, say, without official statement that they would
help.

Now, why I don't like this situation is because I'm pretty sure there's very
few software development group offering as long and reliable maintenance
policy as PostgreSQL is doing, but debian will still happily distribute
*unknown-maintenance-policy* pieces of code in its stable repositories. So the
*uncertainty* excuse is rather poor. And highly frustrating.

<quote>
  ***Note:*** you have to admit that the debian stable management model copes very
  well with all the debian included software. You can't release stable with
  a new PostgreSQL major version unless each and every package depending on
  PostgreSQL will actually work with the newer version, and the debian
  scripts will care for upgrading the cluster. Where it's not working good
  is when you're using debian for a PostgreSQL server for a proprietary
  application, which happens quite frequently too.
</quote>

The consequence of this fact leads to my second main gripe against debian
support for PostgreSQL: the extensions. It so happens that the PostgreSQL
extensions are developped for supporting several major versions from the
same source code. So typically, all you need to do is recompile the
extension against the new major version, and there you go.

Now, say debian new stable is coming with [[http://packages.debian.org/squeeze/postgresql-8.4][8.4]] rather than [[http://packages.debian.org/lenny/postgresql-8.3][8.3]] as it used
to. You should be able to just build the extensions (like [[http://packages.debian.org/squeeze/postgresql-8.4-prefix][prefix]]), without
changing the source package, nor droping =postgresql-8.3-prefix= from the
distribution on the grounds that =8.3= ain't in debian stable anymore.

I've been ranting a lot about this state of facts, and I finally provided a
patch to the [[http://packages.debian.org/sid/postgresql-common][postgresql-common]] debian packaging, which made it into version
=110=: welcome [[http://packages.debian.org/sid/postgresql-server-dev-all][pg_buildext]]. An exemple of how to use it can be found in the
git branch for [[http://github.com/dimitri/prefix][prefix]], it shows up in [[http://github.com/dimitri/prefix/blob/master/debian/pgversions][debian/pgversions]] and [[http://github.com/dimitri/prefix/blob/master/debian/rules][debian/rules]]
files. 

As you can see, the =pg_buildext= tool allows you to list the PostgreSQL major
versions the extension you're packaging supports, and only those that are
both in your list and in the current debian supported major version list
will get built. =pg_buildext= will do a =VPATH= build of your extension, so it's
capable of building the same extension for multiple major versions of
PostgreSQL. Here's how it looks:

<src lang="Makefile">
	# build all supported version
	pg_buildext build $(SRCDIR) $(TARGET) "$(CFLAGS)"

	# then install each of them
	for v in `pg_buildext supported-versions $(SRCDIR)`; do \
		dh_install -ppostgresql-$$v-prefix ;\
	done
</src>

And the files are to be found in those places:

<src>
dim ~/dev/prefix cat debian/postgresql-8.3-prefix.install 
debian/prefix-8.3/prefix.so usr/lib/postgresql/8.3/lib
debian/prefix-8.3/prefix.sql usr/share/postgresql/8.3/contrib

dim ~/dev/prefix cat debian/postgresql-8.4-prefix.install                                                                         
debian/prefix-8.4/prefix.so usr/lib/postgresql/8.4/lib
debian/prefix-8.4/prefix.sql usr/share/postgresql/8.4/contrib
</src>

So you still need to maintain [[http://github.com/dimitri/prefix/blob/master/debian/pgversions][debian/pgversions]] and the
=postgresql-X.Y-extension.*= files, but then a change in debian support for
PostgreSQL major versions will be handled automatically (there's a facility
to trigger automatic rebuild when necessary).

All this ranting to explain that pretty soon, the extenion's packages that I
maintain will no longer have to be patched when dropping a previously
supported major version of PostgreSQL. I'm breathing a little better, so
thanks a lot [[http://www.piware.de/category/debian/][Martin]]!

* 20100805-11:00 Querying the Catalog to plan an upgrade

#20100805-11:00

Some user on =IRC= was reading the releases notes in order to plan for a minor
upgrade of his =8.3.3= installation, and was puzzled about potential needs for
rebuilding =GIST= indexes. That's from the [[http://www.postgresql.org/docs/8.3/static/release-8-3-5.html][8.3.5 release notes]], and from the
[[http://www.postgresql.org/docs/8.3/static/release-8-3-8.html][8.3.8 notes]] you see that you need to consider *hash* indexes on *interval*
columns too. Now the question is, how to find out if any such beasts are in
use in your database?

It happens that [[http://www.postgresql.org/][PostgreSQL]] is letting you know those things by querying its
[[http://www.postgresql.org/docs/8.4/static/catalogs.html][system catalogs]]. That might look hairy at first, but it's very worth getting
used to those system tables. You could compare that to introspection and
reflexive facilities of some programming languages, except much more useful,
because you're reaching all the system at once. But, well, here it goes:

<src lang="sql">
SELECT schemaname, tablename, relname, amname, indexdef
  FROM pg_indexes i 
       JOIN pg_class c ON i.indexname = c.relname and c.relkind = 'i' 
       JOIN pg_am am ON c.relam = am.oid
 WHERE amname = 'gist';
</src>

Now you could replace the =WHERE= clause with =WHERE amname IN ('gist', 'hash')=
to check both conditions at once. What about pursuing the restriction on the
*hash* indexes rebuild to schedule, as they should only get done to indexes on
=interval= columns. Well let's try it:

<src lang="sql">
SELECT schemaname, tablename, relname as indexname, amname, indclass
  FROM pg_indexes i 
       JOIN pg_class c on i.indexname = c.relname and c.relkind = 'i' 
       JOIN pg_am am on c.relam = am.oid 
       JOIN pg_index x on x.indexrelid = c.oid 
 WHERE amname in ('btree', 'gist') 
       and schemaname not in ('pg_catalog', 'information_schema');
</src>

We're not there yet, because as you notice, the catalogs are somewhat
optimized and not always in a normal form. That's good for the system's
performance, but it makes querying a bit uneasy. What we want is to get from
the =indclass= column if there's any of them (it's an =oidvector=) that applies
to an =interval= data type. There's a subtlety here as the index could store
=interval= data even if the column is not of an =interval= type itself, so we
have to find both cases.

Well the *subtlety* applies after you know what an [[http://www.postgresql.org/docs/8.4/static/xindex.html][operator class]] is: *“An
operator class defines how a particular data type can be used with an
index”* is what the [[http://www.postgresql.org/docs/8.4/static/sql-createopclass.html][CREATE OPERATOR CLASS]] manual page teaches us. What we
need to know here is that an index will talk to an operator class to get to
the data type, either the *column* data type or the index *storage* one.

<src lang="sql">
SELECT schemaname, tablename, relname as indexname, amname, indclass, opcname, typname
  FROM pg_indexes i 
       JOIN pg_class c on i.indexname = c.relname and c.relkind = 'i' 
       JOIN pg_am am on c.relam = am.oid 
       JOIN pg_index x on x.indexrelid = c.oid 
       JOIN pg_opclass o 
         on string_to_array(x.indclass::text, ' ')::oid[] @> array[o.oid]::oid[]
       JOIN pg_type t on o.opckeytype = t.oid
WHERE amname = 'hash' and t.typname = 'interval'

UNION ALL

SELECT schemaname, tablename, relname as indexname, amname, indclass, opcname, typname
  FROM pg_indexes i 
       JOIN pg_class c on i.indexname = c.relname and c.relkind = 'i' 
       JOIN pg_am am on c.relam = am.oid 
       JOIN pg_index x on x.indexrelid = c.oid 
       JOIN pg_opclass o 
         on string_to_array(x.indclass::text, ' ')::oid[] @> array[o.oid]::oid[]
       JOIN pg_type t on o.opcintype = t.oid
WHERE amname = 'hash' and t.typname = 'interval';
</src>

Most certainly this query will return no row for you, as *hash* indexes are
not widely used, mainly because they are not crash tolerant. For seeing some
results you could remove the =amname= restriction of course, that would show
the query is working, but don't forget to add the restriction back to plan
for the upgrade!

But hey, why walking the extra mile here, would you ask me? After all, in
the second query we would already have had the information we needed should
we added the =indexdef= column, albeit in a human reader friendly way: the
*resultset* would then contain the =CREATE INDEX= command you need to issue to
build the index from scratch. That would be enough for checking only the
catalog, but the extra mile allows you to produce a =SQL= script to build the
indexes that need your attention post upgrade. That last step is left as an
exercise for the reader, though.

* 20100803-13:30 Database Virtual Machines

#20100803-13:30

Today I'm being told once again about [[http://www.sqlite.org/][SQLite]] as an embedded database
software. That one ain't a *database server* but a *software library* that you
can use straight into your main program. I'm yet to use it, but it looks
like [[http://www.sqlite.org/lang.html][its SQL support]] is good enough for simple things — and that covers
*loads* of things. I guess read-only cache and configuration storage would be
the obvious ones, because it seems that [[http://www.sqlite.org/whentouse.html][SQLite use cases]] aren't including
[[http://www.sqlite.org/lockingv3.html][mixed concurrency]], that is workloads with concurrent readers and writers.

The part that got my full attention is
[[http://www.sqlite.org/vdbe.html][The Virtual Database Engine of SQLite]], as this blog title would imply. It
seems to be the same idea as what [[http://monetdb.cwi.nl/][MonetDB]] calls their
[[http://monetdb.cwi.nl/MonetDB/Documentation/MAL-Synopsis.html][MonetDB Assembly Language]], and I've been trying to summarize some idea about
it in my [[http://tapoueh.org/char10.html#sec11][Next Generation PostgreSQL]] article.

The main thing is how to further optimize [[http://www.postgresql.org/][PostgreSQL]] given what we have. It
seems that among the major road blocks in the performance work is how we get
the data from disk and to the client. We're still spending so many time in
the =CPU= that the disk bandwidth are not always saturated, and that's a
problem. Further thoughts on the [[http://tapoueh.org/char10.html#sec11][full length article]], but that's just about
a one page section now!

* 20100726-17:00 Partitioning: relation size per “group”

#20100726-17:00

This time, we are trying to figure out where is the bulk of the data on
disk. The trick is that we're using [[http://www.postgresql.org/docs/current/static/ddl-partitioning.html][DDL partitioning]], but we want a “nice”
view of size per *partition set*. Meaning that if you have for example a
parent table =foo= with partitions =foo_201006= and =foo_201007=, you would want
to see a single category =foo= containing the accumulated size of all the
partitions underneath =foo=.

Here we go:

<src lang="sql">
select groupe, pg_size_pretty(sum(bytes)::bigint) as size, sum(bytes)
  from (
select relkind as k, nspname, relname, tablename, bytes,
         case when relkind = 'r' and relname ~ '[0-9]{6}$'
              then substring(relname from 1 for length(relname)-7)

	      when relkind = 'i' and  tablename ~ '[0-9]{6}$'
              then substring(tablename from 1 for length(tablename)-7)

              else 'core' 
          end as groupe
  from (
  select nspname, relname,
         case when relkind = 'i'
              then (select relname
                      from pg_index x 
                           join pg_class xc on x.indrelid = xc.oid
                           join pg_namespace xn on xc.relnamespace = xn.oid
                     where x.indexrelid = c.oid
                    )
              else null
           end as tablename,
         pg_size_pretty(pg_relation_size(c.oid)) as relation,
         pg_total_relation_size(c.oid) as bytes,
	 relkind
    from pg_class c join pg_namespace n on c.relnamespace = n.oid 
   where c.relkind in ('r', 'i') 
         and nspname in ('public', 'archive')
         and pg_total_relation_size(c.oid) > 32 * 1024
order by 5 desc
       ) as s
       ) as t
group by 1
order by 3 desc;
</src>

Note that by simply removing those last two lines here, you will get a
detailed view of the *indexes* and *tables* that are taking the most volume on
disk at your place.

Now, what about using [[http://www.postgresql.org/docs/8.4/static/functions-window.html][window functions]] here so that we get some better
detailed view of historic changes on each partition? With some evolution
figure in percentage from the previous partition of the same year,
accumulated size per partition and per year, yearly sum, you name it. Here's
another one you might want to try, ready for some tuning (schema name, table
name, etc):

<src lang="sql">
WITH s AS (
  select relname, 
         pg_relation_size(c.oid) as size,
         pg_total_relation_size(c.oid) as tsize,
         substring(substring(relname from '[0-9]{6}$') for 4)::bigint as year
    from pg_class c 
         join pg_namespace n on n.oid = c.relnamespace 
   where c.relkind = 'r'
     -- and n.nspname = 'public'
     -- and c.relname ~ 'stats' 
     and substring(substring(relname from '[0-9]{6}$') for 4)::bigint >= 2008
order by relname
),
     sy AS (
  select relname, 
         size,
	 tsize,
         year,
         (sum(size) over w_year)::bigint as ysize,
         (sum(size) over w_month)::bigint as cumul,
	 (lag(size) over (order by relname))::bigint as previous 
    from s
  window w_year  as (partition by year),
         w_month as (partition by year order by relname)
),
     syp AS (
  select relname, 
         size, 
	 tsize,
	 rank() over (partition by year order by size desc) as rank,
         case when ysize = 0 then ysize 
	      else round(size / ysize::numeric * 100, 2) end as yp, 
         case when previous = 0 then previous
	      else round((size / previous::numeric - 1.0) * 100, 2) end as evol, 
         cumul, 
         year, 
         ysize
    from sy
)
  SELECT relname, 
         pg_size_pretty(size) as size,
         pg_size_pretty(tsize) as "+indexes",
         evol, yp as "% annuel", rank,
         pg_size_pretty(cumul) as cumul, year,
         pg_size_pretty(ysize) as "yearly sum", 
         pg_size_pretty((sum(size) over())::bigint) as total
    FROM syp
ORDER BY relname;
</src>

Hope you'll find it useful, I certainly do!

* 20100722-09:30 Emacs and PostgreSQL

#20100722-09:30

Those are my two all times favorite Open Source Software. Or [[http://www.gnu.org/philosophy/free-sw.html][Free Software]]
in the [[http://www.gnu.org/][GNU]] sense of the world, as both the *BSD* and the *GPL* are labeled free
there. Even if I prefer the [[http://www.debian.org/social_contract][The Debian Free Software Guidelines]] as a global
definition and the [[http://sam.zoy.org/wtfpl/][WTFPL]] license. But that's a digression.

I think that [[http://www.gnu.org/software/emacs/][Emacs]] and [[http://www.postgresql.org/][PostgreSQL]] do share a lot in common. I'd begin with
the documentation, which quality is amazing for both projects. Then of
course the extensibility with [[http://www.gnu.org/software/emacs/emacs-lisp-intro/html_node/Preface.html#Preface][Emacs Lisp]] on the one hand and
[[http://www.postgresql.org/docs/8.4/static/extend.html][catalog-driven operations]] on the other hand. Whether you're extending Emacs
or PostgreSQL you'll find that it's pretty easy to tweak the system *while
it's running*. The other comparison points are less important, like the fact
the both the systems get about the same uptime on my laptop (currently *13
days, 23 hours, 57 minutes, 10 seconds*).

So of course I'm using *Emacs* to edit *PostgreSQL* =.sql= files, including stored
procedures. And it so happens that [[http://archives.postgresql.org/pgsql-hackers/2010-07/msg01067.php][line numbering in plpgsql]] is not as
straightforward as one would naively think, to the point that we'd like to
have better tool support there. So I've extended Emacs [[http://www.gnu.org/software/emacs/manual/html_node/emacs/Minor-Modes.html][linum-mode minor mode]]
to also display the line numbers as computed per PostgreSQL, and here's what
it looks like:

	    [[../images/emacs-pgsql-line-numbers.png]]

Now, here's also the source code, [[static/dim-pgsql.el][dim-pgsql.el]]. Hope you'll enjoy!

* 20100719-16:30 Background writers

#20100719-16:30

There's currently a thread on [[http://archives.postgresql.org/pgsql-hackers/][hackers]] about [[http://archives.postgresql.org/pgsql-hackers/2010-07/msg00493.php][bg worker: overview]] and a series
of 6 patches. Thanks a lot ***Markus***! This is all about generalizing a concept
already in use in the *autovacuum* process, where you have an independent
subsystem that require having an autonomous *daemon* running and able to start
its own *workers*.

I've been advocating about generalizing this concept for awhile already, in
order to have *postmaster* able to communicate to subsystems when to shut down
and start and reload, etc. Some external processes are only external because
there's no need to include them *by default* in to the database engine, not
because there's no sense to having them in there.

So even if ***Markus*** work is mainly about generalizing *autovacuum* so that he
has a *coordinator* to ask for helper backends to handle broadcasting of
*writesets* for [[http://postgres-r.org/][Postgres-R]], it still could be a very good first step towards
something more general. What I'd like to see the generalization handle are
things like [[http://wiki.postgresql.org/wiki/PGQ_Tutorial][PGQ]], or the *pgagent scheduler*. In some cases, [[http://pgbouncer.projects.postgresql.org/doc/usage.html][pgbouncer]] too.

What we're missing there is an *API* for everybody to be able to extend
PostgreSQL with its own background processes and workers. What would such a
beast look like? I have some preliminary thoughts about this in my
[[char10.html#sec16][Next Generation PostgreSQL]] article, but that's still early thoughts. The
main idea is to steal as much as sensible from
[[http://www.erlang.org/doc/man/supervisor.html][Erlang Generic Supervisor Behaviour]], and maybe up to its
[[http://www.erlang.org/doc/design_principles/fsm.html][Generic Finite State Machines]] *behavior*. In the *Erlang* world, a *behavior* is a
generic process.

The *FSM* approach would allow for any user daemon to provide an initial state
and register functions that would do some processing then change the
state. My feeling is that if those functions are exposed at the SQL level,
then you can *talk* to the daemon from anywhere (the Erlang ideas include a
globally —cluster wide— unique name). Of course the goal would be to
provide an easy way for the *FSM* functions to have a backend connected to the
target database handle the work for it, or be able to connect itself. Then
we'd need something else here, a way to produce events based on the clock. I
guess relying on =SIGALRM= is a possibility.

I'm not sure about how yet, but I think getting back in consultancy after
having opened [[http://2ndQuadrant.com][2ndQuadrant]] [[http://2ndQuadrant.fr][France]] has some influence on how I think about all
that. My guess is that those blog posts are a first step on a nice journey!

* 20100713-14:15 Logs analysis

#20100713-14:15

Nowadays to analyze logs and provide insights, the more common tool to use
is [[http://pgfouine.projects.postgresql.org/][pgfouine]], which does an excellent job. But there has been some
improvements in logs capabilities that we're not benefiting from yet, and
I'm thinking about the =CSV= log format.

So the idea would be to turn *pgfouine* into a set of =SQL= queries against the
logs themselves once imported into the database. Wait. What about having our
next PostgreSQL version, which is meant (I believe) to include CSV support
in *SQL/MED*, to directly expose its logs as a system view?

A good thing would be to expose that as a ddl-partitioned table following
the log rotation scheme as setup in =postgresql.conf=, or maybe given in some
sort of a setup, in order to support =logrotate= users. At least some
facilities to do that would be welcome, and I'm not sure plain *SQL/MED* is
that when it comes to *source* partitioning.

Then all that remains to be done is a set of =SQL= queries and some static or
dynamic application to derive reports from there.

This is yet again an idea I have in mind but don't have currently time to
explore myself, so I talk about it here in the hope that others will share
the interest. Of course, now that I work at [[http://2ndQuadrant.com][2ndQuadrant]], you can make it so
that we consider the idea in more details, up to implementing and
contributing it!

* 20100708-11:15 Using indexes as column store?

#20100708-11:15
#%20Using%20indexes%20as%20column%20store%3F

There's a big trend nowadays about using column storage as opposed to what
PostgreSQL is doing, which would be row storage. The difference is that if
you have the same column value in a lot of rows, you could get to a point
where you have this value only once in the underlying storage file. That
means high compression. Then you tweak the *executor* to be able to load this
value only once, not once per row, and you win another huge source of data
traffic (often enough, from disk).

Well, it occurs to me that maybe we could have column oriented storage
support without adding any new storage facility into PostgreSQL itself, just
using in new ways what we already have now. Column oriented storage looks
somewhat like an index, where any given value is meant to appear only
once. And you have *links* to know where to find the full row associated in
the main storage.

There's a work in progress to allow for PostgreSQL to use indexes on their
own, without having to get to the main storage for checking the
visibility. That's known as the [[http://www.postgresql.org/docs/8.4/static/storage-vm.html][Visibility Map]], which is still only a hint
in released versions. The goal is to turn that into a crash-safe trustworthy
source in the future, so that we get *covering indexes*. That means we can use
an index and skip getting to the full row in main storage and get the
visibility information there.

Now, once we have that, we could consider using the indexes in more
queries. It could be a win to get the column values from the index when
possible and if you don't *output* more columns from the *heap*, return the
values from there. Scanning the index only once per value, not once per row.

There's a little more though on the point in the [[char10.html#sec10][Next Generation PostgreSQL]]
article I've been referencing already, should you be interested.


* 20100706-10:50 MVCC in the Cloud

#20100706-10:50
#%20MVCC%20in%20the%20Cloud

At [[http://char10.org/][CHAR(10)]] ***Markus*** had a talk about
[[http://char10.org/talk-schedule-details#talk13][Using MVCC for Clustered Database Systems]] and explained how [[http://postgres-r.org/][Postgres-R]] does
it. The scope of his project is to maintain a set of database servers in the
same state, eventually.

Now, what does it mean to get "In the Cloud"? Well there are more than one
answer I'm sure, mine would insist on including this "Elasticity" bit. What
I mean here is that it'd be great to be able to add or lose nodes and stay
*online*. Granted, that what's *Postgres-R* is providing. Does that make it
ready for the "Cloud"? Well it happens so that I don't think so.

Once you have elasticity, you also want *scalability*. That could mean lots of
thing, and *Postgres-R* already provides a great deal of it, at the connect
and reads level: you can do your business *unlimited* on any node, the others
will eventually (*eagerly*) catch-up, and you can do your =select= on any node
too, reading from the same data set. Eventually.

What's still missing here is the hard sell, *write scalability*. This is the
idea that you don't want to sustain the same *write load* on all the members
of the "Cloud cluster". It happens that I have some idea about how to go on
this, and this time I've been trying to write them down. You might be
interested into the [[http://tapoueh.org/char10.html#sec3][MVCC in the Cloud]] part of my [[http://tapoueh.org/char10.html][Next Generation PostgreSQL]]
notes.

My opinion is that if you want to distribute the data, this is a problem
that falls in the category of finding the data on disk. This problem is
already solved in the executor, it knows which operating system level file
to open and where to seek inside that in order to find a row value for a
given relation. So it should be possible to teach it that some relation's
storage ain't local, to get the data it needs to communicate to another
PostgreSQL instance. 

I would call that a *remote tablespace*. It allows for distributing both the
data and their processing, which could happen in parallel. Of course that
means there's now some latency concerns, and that some *JOIN* will get slow if
you need to retrieve the data from the network each time. For that what I'm
thinking about is the possibility to manage a local copy of a remote
tablespace, which would be a *mirror tablespace*. But that's for another blog
post.

Oh, if that makes you think a lot of [[http://wiki.postgresql.org/wiki/SQL/MED][SQL/MED]], that would mean I did a good
enough job at explaining the idea. The main difference though would be to
ensure transaction boundaries over the local and remote data: it's one
single distributed database we're talking about here.

* 20100705-09:30 Back from CHAR(10)

#20100705-09:30
#%20Back%20from%20CHAR

It surely does not feel like a full month and some more went by since we
were enjoying [[http://www.pgcon.org/2010/][PGCon 2010]], but in fact it was already the time for
[[http://char10.org/talk-schedule][CHAR(10)]]. The venue was most excellent, as Oxford is a very beautiful
city. Also, the college was like a city in the city, and having the
accomodation all in there really smoothed it all.

On a more technical viewpoint, the [[http://char10.org/talk-schedule][range of topics]] we talked about and the
even broader one in the *"Hall Track"* make my mind full of ideas, again. So
I'm preparing a quite lengthy article to summarise or present all those
ideas, and I think a post series should cover the points in there. When
trying to label things, it appears that my current obsessions are mainly
about *PostgreSQL in the Cloud* and *Further Optimising PostgreSQL*, so that's
what I'll be talking about those next days.

Meanwhile I'm going to search for existing solutions on how to use the
[[http://en.wikipedia.org/wiki/Paxos_algorithm][Paxos algorithm]] to generate a reliable distributed sequence, using [[http://libpaxos.sourceforge.net/][libpaxos]]
for example. The goal would be to see if it's feasible to have a way to
offer some global =XID= from a network of servers in a distributed fashion,
ideally in such a way that new members can join in at any point, and of
course that losing a member does not cause downtime for the online ones. It
sounds like this problem has been extensively researched and is solved,
either by the *Global Communication Systems* or the underlying
algorithms. Given the current buy-in lack of our community for =GCS= my guess
is that bypassing them would be a pretty good move, even if that mean
implementing a limited form of =GCS= ourselves.

* 20100527-14:26 Back from PgCon2010

#20100527-14:26
#%20Back%20from%20PgCon2010

This year's edition has been the [[http://www.pgcon.org/2010/][best pgcon]] ever for me. Granted, it's only
my third time, but still :) As [[http://blog.endpoint.com/2010/05/pgcon-hall-track.html][Josh said]] the *"Hall Track"* in particular was
very good, and the [[http://wiki.postgresql.org/wiki/PgCon_2010_Developer_Meeting][Dev Meeting]] has been very effective!

** Extensions

This time I prepared some [[http://wiki.postgresql.org/wiki/Image:Pgcon2010-dev-extensions.pdf][slides to present the extension design]] and I tried
hard to make it so that we get to agree on a plan, even recognizing it's not
solving all of our problems from the get go. I had been talking about the
concept and design with lots of people already, and continued to do so while in
Ottawa on Monday evening and through all Tuesday. So Wednesday, I felt
prepared. It proved to be a good thing, as I edited the slides with ideas from
several people I had the chance to expose my ideas to! Thanks *Greg Stark* and
*Heikki Linnakangas* for the part we talked about at the meeting, and a lot more
people for the things we'll have to solve later (Hi *Stefan*!).

So the current idea for **extensions** is for the *backend* support to start with a
file in <code>`pg_config --sharedir`/extensions/foo/control</code> containing
the *foo* extension's *metadata*. From that we know if we can install an extension
and how. Here's an example:

<src>
name = foo
version = 1.0
custom_variable_classes = 'foo'
depends  = bar (>= 1.1), baz
conflicts = bla (< 0.8)
</src>

The other files should be =install.sql=, =uninstall.sql= and =foo.conf=. The only
command the user will have to type in order for using the extension in his
database will then be:

<src lang="sql">
  INSTALL EXTENSION foo;
</src>

For that to work all that needs to happen is for me to write the code. I'll
keep you informed as soon as I get a change to resume my activities on the
[[http://git.postgresql.org/gitweb?p=postgresql-extension.git;a=shortlog;h=refs/heads/extension][git branch]] I'm using. You can already find my first attempt at a
=pg_execute_from_file()= function [[http://git.postgresql.org/gitweb?p=postgresql-extension.git;a=commitdiff;h=6eed4eca0179cbdeb737b9783084e9f03fcb7470][there]].

Building atop that backend support we already have two gentlemen competing on
features to offer to [[http://justatheory.com/computers/databases/postgresql/pgan-bikeshedding.html][distribute]] and [[http://petereisentraut.blogspot.com/2010/05/postgresql-package-management.html][package]] extensions! That will complete the
work just fine, thanks guys.

** Hot Standby

Heikki's talk about [[http://www.pgcon.org/2010/schedule/events/264.en.html][Built-in replication in PostgreSQL 9.0]] left me with lots of
thinking. In particular it seems we need two projects out of core to complete
what =9.0= has to offer, namely something very simple to prepare a base backup
and something more involved to manage a pool of standbys.

*** pg_basebackup

The idea I had listening to the talk was that it might be possible to ask the
server, in a single SQL query, for the list of all the files it's using. After
all, there's those =pg_ls_files()= and =pg_read_file()= functions, we could put
them to good use. I couldn't get the idea out of my head, so I had to write
some code and see it running: [[http://github.com/dimitri/pg_basebackup][pg_basebackup]] is there at =github=, grab a copy!

What it does is very simple, in about 100 lines of self-contained python code
it get all the files from a running server through a normal PostgreSQL
connection. That was my first [[http://www.postgresql.org/docs/8.4/interactive/queries-with.html][recursive query]]. I had to create a new function
to get the file contents as the existing one returns text, and I want =bytea=
here, of course.

Note that the code depends on the =bytea= representation in use, so it's only
working with =9.0= as of now. Can be changed easily though, send a patch or just
ask me to do it!

Lastly, note that even if =pg_basebackup= will compress each chunk it sends over
the =libpq= connection, it won't be your fastest option around. Its only
advantage there is its simplicity. Get the code, run it with 2 arguments: a
connection string and a destination directory. There you are.

*** wal proxy, wal relay

The other thing that we'll miss in =9.0= is the ability to both manage more than
a couple of *standby* servers and to manage failover gracefully. Here the idea
would be to have a proxy server acting as both a *walreceiver* and a
*walsender*. Its role would be to both *archive* the WAL and *relay* them to the real
standbys.

Then in case of master's failure, we could instruct this *proxy* to be fed from
the elected new master (manual procedure), the other standbys not being
affected. Well apart than apparently changing the *timeline* (which will happen
as soon as you promote a standby to master) while streaming is not meant to be
supported. So the *proxy* would also disconnect all the *slaves* and have them
reconnect.

If we need such a finesse, we could have the =restore_command= on the *standbys*
prepared so that it'll connect to the *proxy's archive*. Now on failover, the
*standbys* are disconnected from the stream, get a =WAL= file with a new *timeline*
from the *archive*, replay it, and reconnect.

That means that for a full =HA= scenario you could get on with three
servers. You're back to two servers at failover time and need to rebuild the
crashed master as a standby, running a base backup again.

If you've followed the idea, I hope you liked it! I still have to motivate some
volunteers so that some work gets done here, as I'm probably not the one to ask
to as far as coding this is concerned, if you want it out before =9.1= kicks in!

** Queuing

We also had a nice *Hall Tack* session with *Jan Wieck*, *Marko Kreen* and *Jim Nasby*
about how to get a single general (enough) queueing solution for PostgreSQL. It
happens that the Slony queueing ideas made their way into =PGQ= and that we'd
want to add some more capabilities to this one.

What we talked about was adding more interfaces (event producers, event format
translating at both ends of the pipe) and optimising how many events from the
past we keep in the queue for the subscribers, in a cascading environment.

It seems that the basic architecture of the queue is what =PGQ 3= provides
already, so it could even be not that much of a hassle to get something working
out of the ideas exchanged.

Of course, one of those ideas has been discussed at the [[http://wiki.postgresql.org/wiki/PgCon_2010_Developer_Meeting][Dev Meeting]], it's about
deriving the transaction commit order from the place which already has the
information rather than *reconstructing* it after the fact. We'll see how it
goes, but it started pretty well with a design mail thread.

** Other talks 

I went to some other talks too, of course, unfortunately with an attention span
far from constant. Between the social events (you should read that as *beer
drinking evenings*) and the hall tracks, more than once my brain were less
present than my body in the talks. I won't risk into commenting them here, but
overall it was very good: in about each talk, new ideas popped into my
head. And I love that.

** Conclusion: I'm addicted.

The social aspect of the conference has been very good too. Once more, a warm
welcome from the people that are central to the project, and who are so easily
available for a chat about any aspect of it! Or just for sharing a drink.

Meeting our users is very important too, and [[http://www.pgcon.org/2010/][pgcon]] allows for that also. I've
met some people I'm used to talk to via =IRC=, and it was good fun sharing a beer
over there.

All in all, I'm very happy I made it to Ottawa despite the volcano activity,
there's so much happening over there! Thanks to all the people who made it
possible by either organizing the conference or attending to it! See you next
year, I'm addicted...

* 20100427-12:01 Import fixed width data with pgloader

#20100427-12:01

So, following previous blog entries about importing *fixed width* data, from
[[http://www.postgresonline.com/journal/index.php?/archives/157-Import-fixed-width-data-into-PostgreSQL-with-just-PSQL.html][Postgres Online Journal]] and [[http://people.planetpostgresql.org/dfetter/index.php?/archives/58-psql,-Paste,-Perl-Pefficiency!.html][David (perl) Fetter]], I couldn't resist following
the meme and showing how to achieve the same thing with [[http://pgloader.projects.postgresql.org/#toc9][pgloader]].

I can't say how much I dislike such things as the following, and I can't
help thinking that non IT people are right looking at us like this when
encountering such prose.

<src lang="perl">
  map {s/\D*(\d+)-(\d+).*/$a.="A".(1+$2-$1). " "/e} split(/\n/,<<'EOT');
</src>

So, the *pgloader* way. First you need to have setup a database, I called it
=pgloader= here. Then you need the same =CREATE TABLE= as on the original
article, here is it for completeness:

<src lang="sql">
CREATE TABLE places(usps char(2) NOT NULL,
    fips char(2) NOT NULL, 
    fips_code char(5),
    loc_name varchar(64));
</src>

Now the data file I've taken here:
[[http://www.census.gov/tiger/tms/gazetteer/places2k.txt]].

Then we translate the file description into *pgloader* setup:

<src lang="conf">
[pgsql]
host = localhost
port = 5432
base = pgloader
user = dim
pass = None

log_file            = /tmp/pgloader.log
log_min_messages    = DEBUG
client_min_messages = WARNING

client_encoding = 'latin1'
lc_messages         = C
pg_option_standard_conforming_strings = on

[fixed]
table           = places
format          = fixed
filename        = places2k.txt
columns         = *
fixed_specs     = usps:0:2, fips:2:2, fips_code:4:5, loc_name:9:64, p:73:9, h:82:9, land:91:14, water:105:14, ldm:119:14, wtm:131:14, lat:143:10, long:153:11
</src>

We're ready to import the data now:

<src>
dim ~/PostgreSQL/examples pgloader -vsTc pgloader.conf 
pgloader     INFO     Logger initialized
pgloader     WARNING  path entry '/usr/share/python-support/pgloader/reformat' does not exists, ignored
pgloader     INFO     Reformat path is []
pgloader     INFO     Will consider following sections:
pgloader     INFO       fixed
pgloader     INFO     Will load 1 section at a time
fixed        INFO     columns = *, got [('usps', 1), ('fips', 2), ('fips_code', 3), ('loc_name', 4)]
fixed        INFO     Loading threads: 1
fixed        INFO     closing current database connection
fixed        INFO     fixed processing
fixed        INFO     TRUNCATE TABLE places;
pgloader     INFO     All threads are started, wait for them to terminate
fixed        INFO     COPY 1: 10000 rows copied in 5.769s
fixed        INFO     COPY 2: 10000 rows copied in 5.904s
fixed        INFO     COPY 3: 5375 rows copied in 3.187s
fixed        INFO     No data were rejected
fixed        INFO      25375 rows copied in 3 commits took 14.907 seconds
fixed        INFO     No database error occured
fixed        INFO     closing current database connection
fixed        INFO     releasing fixed semaphore
fixed        INFO     Announce it's over

Table name        |    duration |    size |  copy rows |     errors 
====================================================================
fixed             |     14.901s |       - |      25375 |          0
</src>

Note the =-T= option is for =TRUNCATE=, which you only need when you want to
redo the loading, I've come to always mention it in interactive usage. The
=-v= option is for some more *verbosity* and the =-s= for the *summary* at end of
operations.

With the =pgloader.conf= and =places2k.txt= in the current directory, and an
empty table, just typing in =pgloader= at the prompt would have done the job.

Oh, the =pg_option_standard_conforming_strings= bit is from the [[http://github.com/dimitri/pgloader][git HEAD]], the
current released version has no support for setting any PostgreSQL knob
yet. Still, it's not necessary here, so you can forget about it.

You will also notice that *pgloader* didn't trim the data for you, which ain't
funny for the *places* column. That's a drawback of the fixed width format
that you can work on two ways here, either by means of <src
lang="sql">UPDATE places SET loc_name = trim(loc_name) ;</src> or a custom
reformat module for *pgloader*. I guess the latter solution is overkill, but
it allows for *pipe* style processing of the data and a single database write.

Send me a mail if you want me to show here how to setup such a reformatting
module in a next blog entry!

* 20100406-09:10 pgloader activity report

#20100406-09:10

Yes. This [[http://pgloader.projects.postgresql.org/][pgloader]] project is still maintained and somewhat
active. Development happens when I receive a complaint, either about a bug
in existing code or a feature in yet-to-write code. If you have a bug to
report, just send me an email!

If you're following the development of it, the sources just moved from =CVS=
at [[http://cvs.pgfoundry.org/cgi-bin/cvsweb.cgi/pgloader/pgloader/][pgfoundry]] to [[http://github.com/dimitri/pgloader]]. I will still put the
releases at [[http://pgfoundry.org/projects/pgloader][pgfoundry]], and the existing binary packages maintenance should
continue. See also the [[http://pgloader.projects.postgresql.org/dev/pgloader.1.html][development version documentation]], which contains not
yet released stuff.

This time it's about new features, the goal being to open *pgloader* usage
without describing all the file format related details into the
=pgloader.conf= file. This time around, [[http://database-explorer.blogspot.com/][Simon]] is giving feedback and told me
he would appreciate that pgloader would work more like the competition.

We're getting there with some new options. The first one is that rather than
only =Sections=, now your can give a =filename= as an argument. *pgloader* will
then create a configuration section for you, considering the file format to
be =CSV=, setting <code>columns = *</code>. The default *field separator* is =|=,
so you have also the =-f, --field-separator= option to set that from the
command line.

As if that wasn't enough, *pgloader* now supports any [[http://www.postgresql.org/][PostgreSQL]] option either
in the configuration file (prefix the real name with =pg_option_=) or on the
command line, via the =-o, --pg-options= switch, that you can use more than
once. Command line setting will take precedence over any other setup, of
course. Consider for example <code>-o standard_conforming_strings=on</code>.

While at it, some more options can now be set on the command line, including
=-t, --section-threads= and =-m, --max-parallel-sections= on the one hand and
=-r, --reject-log= and =-j, --reject-data= on the other hand. Those two last
must contain a =%s= place holder which will get replaced by the *section* name,
or the =filename= if you skipped setting up a *section* for it.

Your *pgloader* usage is now more command line friendly than ever!

* 20100317-13:35 Finding orphaned sequences

#20100317-13:35
#%20Finding%20orphaned%20sequences

This time we're having a database where *sequences* were used, but not
systematically as a *default value* of a given column. It's mainly an historic
bad idea, but you know the usual excuse with bad ideas and bad code: the
first 6 months it's experimental, after that it's historic.

Still, here's a query for =8.4= that will allow you to list those *sequences*
you have that are not used as a default value in any of your tables:

<src lang="sql">
WITH seqs AS (
  SELECT n.nspname, relname as seqname
    FROM pg_class c
         JOIN pg_namespace n on n.oid = c.relnamespace
   WHERE relkind = 'S'
),
     attached_seqs AS (
  SELECT n.nspname, 
         c.relname as tablename,
         (regexp_matches(pg_get_expr(d.adbin, d.adrelid), '''([^'']+)'''))[1] as seqname
    FROM pg_class c
         JOIN pg_namespace n on n.oid = c.relnamespace
         JOIN pg_attribute a on a.attrelid = c.oid
         JOIN pg_attrdef d on d.adrelid = a.attrelid
                            and d.adnum = a.attnum
                            and a.atthasdef
  WHERE relkind = 'r' and a.attnum > 0
        and pg_get_expr(d.adbin, d.adrelid) ~ '^nextval'
)

 SELECT nspname, seqname, tablename
   FROM seqs s
        LEFT JOIN attached_seqs a USING(nspname, seqname)
  WHERE a.tablename IS NULL;
</src>

I hope you don't need the query...

* 20100223-17:30 Getting out of SQL_ASCII, part 2

#20100223-17:30
#%20Getting%20out%20of%20SQL_ASCII%2C%20part%202

So, if you followed the previous blog entry, now you have a new database
containing all the *static* tables encoded in =UTF-8= rather than
=SQL_ASCII=. Because if it was not yet the case, you now severely distrust
this non-encoding.

Now is the time to have a look at properly encoding the *live* data, those
stored in tables that continue to receive write traffic. The idea is to use
the =UPDATE= facilities of PostgreSQL to tweak the data, and too fix the
applications so as not to continue inserting badly encoded strings in there.

** Finding non UTF-8 data

First you want to find out the badly encoded data. You can do that with this
helper function that [[http://blog.rhodiumtoad.org.uk/][RhodiumToad]] gave me on IRC. I had a version from the
archives before that, but the *regexp* was hard to maintain and quote into a
=PL= function. This is avoided by two means, first one is to have a separate
pure =SQL= function for the *regexp* checking (so that you can index it should
you need to) and the other one is to apply the regexp to =hex= encoded
data. Here we go:

<src lang="sql">
create or replace function public.utf8hex_valid(str text) 
 returns boolean
 language sql immutable
as $f$
   select $1 ~ $r$(?x)
                  ^(?:(?:[0-7][0-9a-f])
                     |(?:(?:c[2-9a-f]|d[0-9a-f])
                        |e0[ab][0-9a-f]
                        |ed[89][0-9a-f]
                        |(?:(?:e[1-9abcef])
                           |f0[9ab][0-9a-f]
                           |f[1-3][89ab][0-9a-f]
                           |f48[0-9a-f]
                          )[89ab][0-9a-f]
                       )[89ab][0-9a-f]
                    )*$
                $r$;
$f$;
</src>

Now some little scripting around it in order to skip intense manual and
boring work (and see, some more catalog queries). Don't forget we will have
to work on a per-column basis here...

<src lang="sql">
create or replace function public.check_encoding_utf8
 (
   IN schemaname text,
   IN tablename  text,
  OUT relname    text,
  OUT attname    text,
  OUT count      bigint
 )
 returns setof record
 language plpgsql
as $f$
DECLARE
  v_sql text;
BEGIN
  FOR relname, attname
   IN SELECT c.relname, a.attname 
        FROM pg_attribute a 
             JOIN pg_class c on a.attrelid = c.oid
             JOIN pg_namespace s on s.oid = c.relnamespace 
	     JOIN pg_roles r on r.oid = c.relowner
       WHERE s.nspname = schemaname
         AND atttypid IN (25, 1043) -- text, varchar
         AND relkind = 'r'          -- ordinary table
         AND r.rolname = 'some_specific_role'
	 AND CASE WHEN tablename IS NOT NULL
	     	  THEN c.relname ~ tablename
		  ELSE true
	      END
  LOOP
    v_sql := 'SELECT count(*) '
          || '  FROM ONLY '|| schemaname || '.' || relname 
          || ' WHERE NOT public.utf8hex_valid(encode(textsend(' 
          || attname
          || '), ''hex''))';

    -- RAISE NOTICE 'Checking: %.%', relname, attname;
    -- RAISE NOTICE 'SQL: %', v_sql;
    EXECUTE v_sql INTO count;
    RETURN NEXT;
  END LOOP;
END;
$f$; 
</src>

Note that the =tablename= is compared using the =~= operator, so that's *regexp*
matching there too. Also note that I wanted only to check those tables that
are owned by a specific role, your case may vary.

The way I used this function was like this:

<src lang="sql">
create table leon.check_utf8 as
 select * 
   from public.check_encoding_utf8();
</src>

Then you need to take action on those lines in =leon.check_utf8= table which
have a =count > 0=. Rince and repeat, but you may soon realise building the
table over and over again is costly.

** Cleaning up the data

Up for some more helper tools? Unless you really want to manually fix this
huge amount of columns where some data ain't =UTF-8= compatible... here's some
more:

<src lang="sql">
create or replace function leon.nettoyeur
 (
  IN  action      text,
  IN  encoding    text,
  IN  tablename   text,
  IN  columname   text,

  OUT orig        text,
  OUT utf8        text
 )
 returns setof record
 language plpgsql
as $f$
DECLARE
  p_convert text;
BEGIN
  IF encoding IS NULL
  THEN
    p_convert := 'translate(' 
              || columname || ', ' 
              || $$'\211\203\202'$$ 
              || ', '
              || $$'   '$$
	      || ') ';
  ELSE
    -- in 8.2, write convert using, in 8.3, the other expression
    -- p_convert := 'convert(' || columname || ' using ' || conversion || ') ';
    p_convert := 'convert(textsend(' || columname || '), '''|| encoding ||''', ''utf-8'' ) ';
  END IF;

  IF action = 'select'
  THEN
    FOR orig, utf8
     IN EXECUTE 'SELECT ' || columname || ', '
         || p_convert
         || '  FROM ONLY ' || tablename
         || ' WHERE not public.utf8hex_valid('
         || 'encode(textsend('|| columname ||'), ''hex''))'
    LOOP
      RETURN NEXT;
    END LOOP;

  ELSIF action = 'update'
  THEN
    EXECUTE 'UPDATE ONLY ' || tablename 
         || ' SET ' || columname || ' = ' || p_convert
         || ' WHERE not public.utf8hex_valid('
         || 'encode(textsend('|| columname ||'), ''hex''))';

    FOR orig, utf8 
     IN SELECT * 
          FROM leon.nettoyeur('select', encoding, tablename, columname)
    LOOP
      RETURN NEXT;
    END LOOP;

  ELSE
    RAISE EXCEPTION 'Léon, Nettoyeur, veut de l''action.';

  END IF;
END;
$f$;
</src>

As you can see, this function allows to check the conversion process from a
given supposed encoding before to actually convert the data in place. This
is very useful as even when you're pretty sure the non-utf8 data is =latin1=,
sometime you find it's =windows-1252= or such. So double check before telling
=leon.nettoyeur()= to update your precious data!

Also, there's a facility to use =translate()= when none of the encoding match
your expectations. This is a skeleton just replacing invalid characters with
a =space=, tweak it at will!

** Conclusion

Enjoy your clean database now, even if it still accepts new data that will
probably not pass the checks, so we still have to be careful about that and
re-clean every day until the migration is effective. Or maybe add a =CHECK=
clause that will reject badly encoded data...

In fact here we're using [[http://wiki.postgresql.org/wiki/Londiste_Tutorial][Londiste]] to replicate the *live* data from the old to
the new server, and that means the replication will break each time there's
new data written in non-utf8, as the new server is running =8.4=, which by
design ain't very forgiving. Our plan is to clean-up as we go (remove table
from the *subscriber*, fix it, add it again) and migrate as soon as possible!

Bonus points to those of you getting the convoluted reference :)

* 20100218-11:37 Getting out of SQL_ASCII, part 1

#20100218-11:37
#%20Getting%20out%20of%20SQL_ASCII%2C%20part%201

It happens that you have to manage databases *designed* by your predecessor,
and it even happens that the team used to not have a *DBA*. Those *histerical
raisins* can lead to having a =SQL_ASCII= database. The horror!

What =SQL_ASCII= means, if you're not already familiar with the consequences
of such a choice, is that all the =text= and =varchar= data that you put in the
database is accepted as-is. No checks. At all. It's pretty nice when you're
lazy enough to not dealing with *strange* errors in your application, but if
you think that t's a smart move, please go read
[[http://www.joelonsoftware.com/articles/Unicode.html][The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets (No Excuses!)]]
by [[http://www.joelonsoftware.com/][Joel Spolsky]] now. I said now, I'm waiting for you to get back here. Yes,
I'll wait.

The problem of course is not being able to read the data you just stored,
which is seldom the use case anywhere you use a database solution such as
[[http://www.postgresql.org/][PostgreSQL]].

Now, it happens too that it's high time to get off of =SQL_ASCII=, the
infamous. In our case we're lucky enough in that the data are all in fact
=latin1= or about that, and this comes from the fact that all the applications
connecting to the database are sharing some common code and setup. Then we
have some tables that can be tagged *archives* and some other *live*. This blog
post will only deal with the former category.

For those tables that are not receiving changes anymore, we will migrate
them by using a simple but time hungry method: =COPY OUT|recode|COPY IN=. I've
tried to use =iconv= for recoding our data, but it failed to do so in lots of
cases, so I've switched to using the [[http://www.gnu.org/software/recode/recode.html][GNU recode]] tool, which works just fine.

The fact that it takes so much time doing the conversion is not really a
problem here, as you can do it *offline*, while the applications are still
using the =SQL_ASCII= database. So, here's the program's help:

<src>
recode.sh [-npdf0TI] [-U user ] -s schema [-m mintable] pattern
        -d    debug
        -n    dry run, only print table names and expected files
        -s    schema
        -m    mintable, to skip already processed once
        -U    connect to PostgreSQL as user
        -f    force table loading even when export files do exist
        -0    only (re)load tables with zero-sized copy files
        -T    Truncate the tables before COPYing recoded data
        -I    Temporarily drop the indexes of the table while COPYing
   pattern    ^table_name_, e.g.
</src>

The =-I= option is neat enough to create the indexes in parallel, but with no
upper limit on the number of index creation launched. In our case it worked
well, so I didn't have to bother.

Take a look at the [[static/recode.sh][recode.sh]] script, and don't hesitate editing it for your
purpose. It's missing some obvious options to get useful in the large, such
as the =recode= *request* which is currently hardcoded to =l1..utf8=. If there's
any demand about it, I'll setup a [[http://github.com/dimitri][GitHub]] project for the little script.

We'll get back to the subject of this entry in *part 2*, dealing with how to
recode your data in the database itself, thanks to some insane regexp based
queries and helper functions. And thanks to a great deal of IRC based
helping, too.

* 20100216-16:23 Resetting sequences. All of them, please!

#20100216-16:23
#%20Reseting%20sequences%2E%20All%20of%20them%2C%20please%21

So, after restoring a production dump with intermediate filtering, none of
our sequences were set to the right value. I could have tried to review the
process of filtering the dump here, but it's a *one-shot* action and you know
what that sometimes mean. With some pressure you don't script enough of it
and you just crawl more and more.

Still, I think how I solved it is worthy of a blog entry. Not that it's
about a super unusual *clever* trick, quite the contrary, because questions
involving this trick are often encountered on the support =IRC=. 

The idea is to query the catalog for all sequences, and produce from there
the =SQL= command you will have to issue for each of them. Once you have this
query, it's quite easy to arrange from the =psql= prompt as if you had dynamic
scripting capabilities. Of course in =9.0= you will have *inline anonymous* =DO=
blocks.

<src>
#> \o /tmp/sequences.sql
#> \t
Showing only tuples.
#> YOUR QUERY HERE
#> \o
#> \t
Tuples only is off.
</src>

Once you have the =/tmp/sequences.sql= file, you can ask =psql= to execute its
command as you're used to, that's using =\i= in an explicit transaction block.

Now, the interresting part if you got here attracted by the blog entry title
is in fact the query itself. A nice way to start is to =\set ECHO_HIDDEN= then
describe some table, you now have a catalog example query to work with. Then
you tweak it somehow and get this:

<src lang="sql">
  SELECT 'select ' 
          || trim(trailing ')' 
             from replace(pg_get_expr(d.adbin, d.adrelid),
                          'nextval', 'setval'))
          || ', (select max( ' || a.attname || ') from only '
          || nspname || '.' || relname || '));' 
    FROM pg_class c 
         JOIN pg_namespace n on n.oid = c.relnamespace 
         JOIN pg_attribute a on a.attrelid = c.oid
         JOIN pg_attrdef d on d.adrelid = a.attrelid 
                            and d.adnum = a.attnum
                            and a.atthasdef 
  WHERE relkind = 'r' and a.attnum > 0 
        and pg_get_expr(d.adbin, d.adrelid) ~ '^nextval';
</src>

Coming next, a =recode= based script in order to get from =SQL_ASCII= to =UTF-8=,
and some strange looking queries too.

<src lang="sh">
recode.sh [-npdf0TI] [-U user ] -s schema [-m mintable] pattern
</src>

Stay tuned!

<include file="blog.dim.2009.muse">
